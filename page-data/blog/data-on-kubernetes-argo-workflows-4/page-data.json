{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/data-on-kubernetes-argo-workflows-4/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Container-native workflow engine for Kubernetes üîÆ</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üêô Introduction</h2>\n<p>Welcome to the fourth part of the \"Data on Kubernetes\" series!</p>\n<p>In this part, we'll explore <a href=\"https://argo-workflows.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflows</a>‚Ää-‚Ääan open-source, container-native workflow engine designed specifically for orchestrating parallel jobs on Kubernetes.</p>\n<p>Argo Workflows seamlessly integrates with Kubernetes services like volumes, secrets, and RBAC, making it a powerful tool for managing complex data workflows.</p>\n<h2 id=\"introduction-to-argo-workflows\" style=\"position:relative;\"><a href=\"#introduction-to-argo-workflows\" aria-label=\"introduction to argo workflows permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Introduction to Argo Workflows</h2>\n<p>In our <a href=\"https://seifrajhi.github.io/blog/data-on-kubernetes-airflow-on-aws-3/\" target=\"_blank\" rel=\"noopener noreferrer\">previous blog post</a>, we explored the world of workflow orchestrators, with a specific focus on Apache Airflow.</p>\n<p>Now, let's check another tool: Argo Workflows.</p>\n<p>As an open-source, container-native workflow engine, Argo Workflows is purpose-built for orchestrating parallel jobs within Kubernetes environments.</p>\n<p>Let's take a closer look at why Argo Workflows stands out:</p>\n<h3 id=\"1-what-is-argo-workflows-\" style=\"position:relative;\"><a href=\"#1-what-is-argo-workflows-\" aria-label=\"1 what is argo workflows  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>1. What is Argo Workflows? ü§î</h3>\n<p>Argo Workflows empowers you to define workflows where each step corresponds to a container. This flexibility allows for the creation of complex data processing pipelines. Think of it as assembling a series of building blocks, where each block represents a specific task or operation.</p>\n<p>Additionally, Argo Workflows is implemented as a Kubernetes Custom Resource Definition (CRD), seamlessly integrating into the Kubernetes ecosystem without disrupting the existing setup.</p>\n<h3 id=\"2-main-features-and-benefits\" style=\"position:relative;\"><a href=\"#2-main-features-and-benefits\" aria-label=\"2 main features and benefits permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>2. Main Features and Benefits</h3>\n<p><strong>Container-Native Approach:</strong></p>\n<p>Argo Workflows fully embraces the containerization paradigm. Each step in your workflow runs within its own container, ensuring consistency and compatibility across different tasks. Whether you're dealing with data transformations, model training, or any other computational task, Argo Workflows has you covered.</p>\n<p><strong>DAG Modeling (Directed Acyclic Graphs):</strong></p>\n<p>Imagine your workflow as a flowchart with interconnected boxes. Argo Workflows allows you to model multi-step workflows using <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target=\"_blank\" rel=\"noopener noreferrer\">directed acyclic graphs (DAGs)</a>. These graphs capture dependencies between tasks, ensuring that steps execute in the correct order. It's like choreographing a dance where each move follows a logical sequence.</p>\n<p><strong>Efficient Compute Handling:</strong></p>\n<p>Argo Workflows shines when it comes to compute-intensive workloads. Whether you're crunching numbers, analyzing data, or training machine learning models, Argo efficiently manages the computational resources needed for each step.</p>\n<h3 id=\"3-integration-with-the-kubernetes-ecosystem\" style=\"position:relative;\"><a href=\"#3-integration-with-the-kubernetes-ecosystem\" aria-label=\"3 integration with the kubernetes ecosystem permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>3. Integration with the Kubernetes Ecosystem</h3>\n<p>Argo Workflows seamlessly integrates with other Kubernetes services:</p>\n<ul>\n<li><strong>Volumes:</strong> Need to read or write data? Argo Workflows plays well with <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes volumes</a>, allowing you to handle data storage efficiently.</li>\n<li><strong>Secrets:</strong> Security matters! Argo Workflows integrates with <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes secrets</a>, ensuring that sensitive information remains protected.</li>\n<li><strong>RBAC (Role-Based Access Control):</strong> Argo Workflows respects your access policies. It works harmoniously with <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes RBAC</a>, allowing you to control who can orchestrate workflows and who can't.</li>\n</ul>\n<h3 id=\"4-why-choose-argo-workflows\" style=\"position:relative;\"><a href=\"#4-why-choose-argo-workflows\" aria-label=\"4 why choose argo workflows permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>4. Why Choose Argo Workflows</h3>\n<p>Argo Workflows stands out for several reasons:</p>\n<ul>\n<li><strong>Kubernetes Integration:</strong> Argo Workflows seamlessly integrates with Kubernetes, simplifying usage and deployment.</li>\n<li><strong>Scalability and Complexity Handling:</strong> It efficiently manages complex workflows, even those with thousands of steps.</li>\n<li><strong>Flexible Workflow Patterns:</strong> Argo supports various workflow structures, adapting to your needs.</li>\n<li><strong>Active Open Source Community:</strong> Benefit from a thriving community that actively contributes to Argo's development.</li>\n</ul>\n<h2 id=\"exploring-argo-workflows-on-amazon-eks\" style=\"position:relative;\"><a href=\"#exploring-argo-workflows-on-amazon-eks\" aria-label=\"exploring argo workflows on amazon eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Exploring Argo Workflows on Amazon EKS</h2>\n<p>In this section, we'll dive into creating and deploying a data processing platform on <a href=\"https://aws.amazon.com/eks/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Kubernetes Service (Amazon EKS)</a>.</p>\n<p>The solution includes essential Kubernetes add-ons: <a href=\"https://argo-workflows.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflows</a>, <a href=\"https://argoproj.github.io/argo-events/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Events</a>, <a href=\"https://github.com/kubeflow/spark-operator\" target=\"_blank\" rel=\"noopener noreferrer\">Spark Operator</a> for managing Spark jobs, <a href=\"https://fluentbit.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Fluent Bit</a> for logging, and <a href=\"https://prometheus.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Prometheus</a> for metrics.</p>\n<h3 id=\"Ô∏è-setup-overview\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-setup-overview\" aria-label=\"Ô∏è setup overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Setup Overview</h3>\n<p><strong>Cluster Setup:</strong> We'll set up an Amazon EKS cluster with a managed node group. We'll install Argo Workflows and Argo Events in their own dedicated namespaces (<code class=\"language-text\">argo-workflows</code> and <code class=\"language-text\">argo-events</code>).</p>\n<p><strong>Event-Driven Workflow:</strong> An <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SQS queue</a> will receive user requests. In the Argo Events namespace, an SQS event source object will fetch messages from this external queue. When a new message arrives, a sensor in Argo Workflow will trigger the specified workflow.</p>\n<p><strong>Spark Job Execution:</strong> The triggered workflow will create a Spark application using Spark Operator in the <code class=\"language-text\">data-ops</code> namespace. This application consists of one Spark driver pod and two executor pods.</p>\n<p>The diagram illustrates how Argo Events sources drive the execution of Spark jobs.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 117.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsTAAALEwEAmpwYAAADOUlEQVR42qVVa08aQRTllzfppyZNtCIqgqJQUdO0tZ/6iC9EQAGtysobUXdBnrvuLshT4fTOLFL6iI+4yWV27pw595w7A5jw2NPvo9O9RZviKY/pAR7+tDpdOPwinPt5dDsdY+0lhDeNBr6EslgLZtButV6i0GBUrlVkL0TEk2notfoLCPlnD+sJBRZfETOBCraTMs+9yHIgWYDLm4RjU0AoU/xj7VmE7GnTgSiqDlHKI3N2jmutznPPttwbSPAkKjD7SrAHZR6WPRn+tMwb0n8O4f2BnEoq3IEsVn0puLYELAfOkShoD14d02OnVtN15PN5ZLPn9K497ZSZoNHoDcqzi/w5UoU1pGJ6r4q1SIUD/sbfx6MKe3e32IqI+HQg4WPoEtvC5eMK2anJWg2KXv8nZDphScohnUohlYhz6//DyhRq7QY9smbazShwCB24410sxQYRpXniDrb9Msa/RTH2NYo3a2GMf4/BcagR9o5jDDyN8Vu4Tuq4kmsweTIqFuJ9LKcBd2okaO4isNkjYnzjDO82s7Ds5rEoNAfY/gBr7HVG2xCrOkw7ZxoWYj0sM0ByJAjsIhWOozp97UqY8kpYOL7hOXcSI1i2l7CnbUhVrlDDIilcoSpsYRg0n/+pY3a/gtlglcYyrETsOKphJUPrRGZgjb2skEGYVmE/bsEZaWFxGE2y1sLcAV0XfxGTOyImti7ovYC5QxVOgWEaBo7hhTapr0NiPdyIlTG1r3DQwkmDBwM5yYKNlI39SOLVkgevV/2wEDFTzIotDvE3HD9DhcWKBtN6tAQbsX84h2GFLDipIiOcCRQx7StgyndFPczBulfm/WTKnKcdagvZzfSxmgUVKuGyrBoKrWEN76Nd2Mkis2cPX5M1nao3eR+nidC8fYF56h9TNUdXxxZSMOm5HBxUF7NUTDQIS5jw5jEXlqnpZW6JEdrDVdqsw04bmXX7wTWNMlxCg4qoXC0jsQUZXsEsuRHvFb7dFMlWARY/XQ8eRZh3cpjcLVBc8V/sabJqoTlbZ7lJEsGxlDd7CUM5id1DudZEPKcgkZNpZFFFXJJ5Lj7MyYOcjJhUHb7/xspI088a+7v9Bc1z22duIPu3AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"diagram\" title=\"\" src=\"/static/77747e1703c0180ae5de6102493873dc/c5bb3/diagram.png\" srcset=\"/static/77747e1703c0180ae5de6102493873dc/04472/diagram.png 170w,\n/static/77747e1703c0180ae5de6102493873dc/9f933/diagram.png 340w,\n/static/77747e1703c0180ae5de6102493873dc/c5bb3/diagram.png 680w,\n/static/77747e1703c0180ae5de6102493873dc/dc616/diagram.png 933w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<blockquote>\n<p><strong>Prerequisites:</strong> Ensure you have <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS CLI</a>, <a href=\"https://kubernetes.io/docs/tasks/tools/\" target=\"_blank\" rel=\"noopener noreferrer\">kubectl</a>, <a href=\"https://learn.hashicorp.com/tutorials/terraform/install-cli\" target=\"_blank\" rel=\"noopener noreferrer\">Terraform</a>, and <a href=\"https://github.com/argoproj/argo-workflows/releases/latest\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflow CLI</a> installed.</p>\n</blockquote>\n<p>The diagram below shows the detailed technical design that will be set up in this demo:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 35.29411764705882%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABvElEQVR42m2Q224SURSG54F8Cl/BO9/AF/CiMcaYKhpjTKxeaNOpLdQDAbFUEYRKT4BAD6lBraUHRgYGZ0+nUJihwwCfA9Y7d/Lly/+vnXWxJMcdYLb7NLt4HjDKo9cfDL3scnLWG8+NVg+9eY7wbFx0/zj1/o3cdQZIZsclV7bJbn2j+FPH7PxdaHX7bFUcdtQh24rLmxWFx6EdpsK7zCePyB90KRzaHp6PXDb2LNSTHpKw4MPyKv4bl0jF/IwyDLGcIbFsHfnFOrGcRuJrh+CGwJ/WiBZMPpc6Y1ZKLbIljbUfHRT9HKnegmA0Tn76MuvJABUxwLJsRLvHI1+Y21eu8fBehJdrNULLZeLJXeTEAa/TCgurDT7GcxjzV1GP9zFMC6kqHKYTNZ48l5Gj2yjCpd00Ma0+T+eKTF5/xtTcJnJaZ+aTymziFzNJjdmUhpxqEEgqvI8XCWc09lQbqVy3mQwq3Fw45tarCodad3xD0XJ4sFjH97bO/YjnUNVDxRe+YJTDVe563Z3IbyYCCpnvTaSG6RDJGSwVDBa/GDROnfHCM7tPNG/wLie8Xoxn/0ewlNcJZXT2azZ/AE3d4+DdQprxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"setup\" title=\"\" src=\"/static/7ca26b2a4e959d7b89e601c5b9c98a7c/c5bb3/setup.png\" srcset=\"/static/7ca26b2a4e959d7b89e601c5b9c98a7c/04472/setup.png 170w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/9f933/setup.png 340w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/c5bb3/setup.png 680w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/b12f7/setup.png 1020w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/b5a09/setup.png 1360w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/29007/setup.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-implementation-steps\" style=\"position:relative;\"><a href=\"#-implementation-steps\" aria-label=\" implementation steps permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Implementation Steps</h3>\n<ol>\n<li>\n<p><strong>Clone the Repository:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/awslabs/data-on-eks.git\n<span class=\"token builtin class-name\">cd</span> data-on-eks/schedulers/terraform/argo-workflow</code></pre></div>\n</li>\n<li>\n<p><strong>Initialize and Apply Terraform:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">terraform init\nterraform apply <span class=\"token parameter variable\">-var</span> <span class=\"token assign-left variable\">region</span><span class=\"token operator\">=</span>eu-west-1 --auto-approve</code></pre></div>\n<p>This will create:</p>\n<ul>\n<li><strong>EKS and Networking Infrastructure:</strong>\n<ul>\n<li>VPC with Private and Public Subnets.</li>\n<li>Internet Gateway (Public) and NAT Gateway (Private).</li>\n<li>EKS Cluster Control Plane with managed node group.</li>\n</ul>\n</li>\n<li><strong>EKS Managed Add-ons:</strong>\n<ul>\n<li>VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_Driver.</li>\n</ul>\n</li>\n<li><strong>Additional Kubernetes Components:</strong>\n<ul>\n<li>Argo Workflows, Argo Events, AWS for FluentBit, Karpenter.</li>\n<li>Metrics Server, CoreDNS Autoscaler, Cluster Autoscaler.</li>\n<li>Kube Prometheus Stack, Spark Operator.</li>\n<li>Roles for Argo Workflows and Argo Events.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Create the SQS Queue:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">queue_url</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>aws sqs create-queue --queue-name demo-argo-workflows <span class=\"token parameter variable\">--region</span> eu-west-1 <span class=\"token parameter variable\">--output</span> text<span class=\"token variable\">)</span></span></code></pre></div>\n</li>\n<li>\n<p><strong>Retrieve the Queue ARN:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">sqs_queue_arn</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>aws sqs get-queue-attributes --queue-url $queue_url --attribute-names QueueArn <span class=\"token parameter variable\">--region</span> eu-west-1 <span class=\"token parameter variable\">--query</span> <span class=\"token string\">\"Attributes.QueueArn\"</span> <span class=\"token parameter variable\">--output</span> text<span class=\"token variable\">)</span></span></code></pre></div>\n<p>Update the SQS access policy:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">template</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">`</span><span class=\"token function\">cat</span> argo-events-manifests/sqs-accesspolicy.json <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token parameter variable\">-e</span> <span class=\"token string\">\"s|&lt;sqs_queue_arn>|<span class=\"token variable\">$sqs_queue_arn</span>|g;s|&lt;your_event_irsa_arn>|<span class=\"token variable\">$your_event_irsa_arn</span>|g\"</span><span class=\"token variable\">`</span></span>\naws sqs set-queue-attributes --queue-url <span class=\"token variable\">$queue_url</span> <span class=\"token parameter variable\">--attributes</span> <span class=\"token variable\">$template</span> <span class=\"token parameter variable\">--region</span> eu-west-1</code></pre></div>\n</li>\n<li>\n<p><strong>Access Argo Workflows Web UI:</strong></p>\n<p>Extract the bearer token:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">argo auth token</code></pre></div>\n<p>Get the load balancer URL:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> argo-workflows get <span class=\"token function\">service</span> argo-workflows-server <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">\"{.status.loadBalancer.ingress[*].hostname}{'<span class=\"token entity\" title=\"\\n\">\\n</span>'}\"</span></code></pre></div>\n<p>Open your browser, enter the URL, and paste the token (including <code class=\"language-text\">Bearer</code>) into the login prompt.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 48.8235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABRElEQVR42pWQi26CQBBF+f8va2NTWzVC5bEosBRcHgVkiRZvhxURrU1bkpuZnZl7MoxmbzzI/QHFrkZZy4tkQzX5L3U+7Xm5AntPYfoRrCCGSfLjBCIrkFc7GmpQdoa6N9YjjUBpUSm/NiXgOs5g08PhW1UUeYlaSuwPB7Rt2+tzlLc4Ho8Yfw39ZefXprqJdZQOwC5ykWNL0OTjrIo22CFVsaRYIqONOp1qlVpCAV906wrIQoElCzBdudDdgHIfOr0n8wUeZnM8LkxMFhbmtocnwxn6phf1QOMGSFpRbvixgrMwhRtuEApGWwS0LUeUrOnWHPowI5T3LvAMZX3ucEFARjeSw73qpqI+I11AA/DVsL8Br9WZQniJi6TgyIsIcbahWtD3boCzt9+AJ6jhv9OdAlg+J4U/zhLQ+QPwfAIx0n3gF2bb6WswfMI1AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"argo\" title=\"\" src=\"/static/3eb6034f8353d03f9b039f02915e645f/c5bb3/argo.png\" srcset=\"/static/3eb6034f8353d03f9b039f02915e645f/04472/argo.png 170w,\n/static/3eb6034f8353d03f9b039f02915e645f/9f933/argo.png 340w,\n/static/3eb6034f8353d03f9b039f02915e645f/c5bb3/argo.png 680w,\n/static/3eb6034f8353d03f9b039f02915e645f/b12f7/argo.png 1020w,\n/static/3eb6034f8353d03f9b039f02915e645f/b5a09/argo.png 1360w,\n/static/3eb6034f8353d03f9b039f02915e645f/64639/argo.png 1568w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n</li>\n<li>\n<p><strong>Create a Spark Job within an Argo Workflow:</strong></p>\n<p>Install the EventBus:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> argo-events-manifests/eventbus.yaml</code></pre></div>\n<p>Set up Amazon SQS as an event source:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">queue_name</span><span class=\"token operator\">=</span>demo-argo-workflows\n<span class=\"token assign-left variable\">region_sqs</span><span class=\"token operator\">=</span>eu-west-1\n\n<span class=\"token function\">cat</span> argo-events-manifests/eventsource-sqs.yaml <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token string\">\"s/&lt;region_sqs>/<span class=\"token variable\">$region_sqs</span>/g;s/&lt;queue_name>/<span class=\"token variable\">$queue_name</span>/g\"</span> <span class=\"token operator\">|</span> kubectl apply <span class=\"token parameter variable\">-f</span> -</code></pre></div>\n<p>Deploy the sensor:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> argo-events-manifests/sensor-rbac.yaml</code></pre></div>\n<p>Update <code class=\"language-text\">taxi-trip.sh</code> with your S3 bucket and region:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token keyword\">if</span> <span class=\"token punctuation\">[</span> <span class=\"token variable\">$#</span> <span class=\"token parameter variable\">-ne</span> <span class=\"token number\">2</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span> <span class=\"token keyword\">then</span>\n  <span class=\"token builtin class-name\">echo</span> <span class=\"token string\">\"Usage: <span class=\"token variable\">$0</span> &lt;s3_bucket> &lt;region>\"</span>\n  <span class=\"token builtin class-name\">exit</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">fi</span>\n\n<span class=\"token assign-left variable\">s3_bucket</span><span class=\"token operator\">=</span><span class=\"token string\">\"<span class=\"token variable\">$1</span>\"</span>\n<span class=\"token assign-left variable\">region</span><span class=\"token operator\">=</span><span class=\"token string\">\"<span class=\"token variable\">$2</span>\"</span>\n\n<span class=\"token assign-left variable\">INPUT_DATA_S3_PATH</span><span class=\"token operator\">=</span><span class=\"token string\">\"s3://<span class=\"token variable\">${s3_bucket}</span>/taxi-trip/input/\"</span>\n\n<span class=\"token function\">mkdir</span> input\n\naws s3 <span class=\"token function\">cp</span> pyspark-taxi-trip.py s3://<span class=\"token variable\">${s3_bucket}</span>/taxi-trip/scripts/ <span class=\"token parameter variable\">--region</span> <span class=\"token variable\">${region}</span>\n\n<span class=\"token function\">wget</span> https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet <span class=\"token parameter variable\">-O</span> <span class=\"token string\">\"input/yellow_tripdata_2022-0.parquet\"</span>\n\n<span class=\"token assign-left variable\">max</span><span class=\"token operator\">=</span><span class=\"token number\">5</span>\n<span class=\"token keyword\">for</span> <span class=\"token variable\"><span class=\"token punctuation\">((</span> i<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;=</span> $max<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>i <span class=\"token punctuation\">))</span></span>\n<span class=\"token keyword\">do</span>\n   <span class=\"token function\">cp</span> <span class=\"token parameter variable\">-rf</span> <span class=\"token string\">\"input/yellow_tripdata_2022-0.parquet\"</span> <span class=\"token string\">\"input/yellow_tripdata_2022-<span class=\"token variable\">${i}</span>.parquet\"</span>\n<span class=\"token keyword\">done</span>\n\naws s3 <span class=\"token function\">sync</span> <span class=\"token string\">\"input/\"</span> <span class=\"token variable\">${INPUT_DATA_S3_PATH}</span>\n\n<span class=\"token function\">rm</span> <span class=\"token parameter variable\">-rf</span> input</code></pre></div>\n<p>Execute the script:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">./taxi-trip.sh</code></pre></div>\n<p>Apply the sensor configuration:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> sensor-sqs-sparkjobs.yaml</code></pre></div>\n</li>\n<li>\n<p><strong>Test the Setup:</strong></p>\n<p>Send a message from SQS:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">aws sqs send-message --queue-url <span class=\"token variable\">$queue_url</span> --message-body <span class=\"token string\">'{\"message\": \"hello data on k8s\"}'</span> <span class=\"token parameter variable\">--region</span> <span class=\"token variable\">$region_sqs</span></code></pre></div>\n<p>Check the workflow status:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get workflows <span class=\"token parameter variable\">-n</span> argo-workflows</code></pre></div>\n<p>You can also see the SQS workflow status in the web UI:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 49.411764705882355%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABBElEQVR42q2SUU/DIBDH+fZG48MSXVzsnE7jx9EHE31wc7pFN0cDbWlLKVT/Hi3GF1+0I/nljgP+3HGwvf0D3N4/4ubuAaa2RI2qxXbWfM+DH9DG/MSJsjJI8hLscHAMLlNsuIAqK2yTDHGqIFUJpQ1EViBOVGs9/pBUnU0L3SJontPejM6z4UkEYx1q5yBp02LN8Uws3wVWXGK1FQHyeYj9wssmxpJgg6MhDJXgmoZu0Ji9cczXMebe/oHZawcbnY7xCcBRhr5kv/hEgv+FRWdTyu4jCOr+guPJFJo61FDJud5BhheX11B5EQRNf8FRNEFtLfzYiWB0foWU3s532v+nvoJfHgHiSwjOf4UAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"webui\" title=\"\" src=\"/static/a6886faf142fff5d7b6ed8fdb42d33f0/c5bb3/webui.png\" srcset=\"/static/a6886faf142fff5d7b6ed8fdb42d33f0/04472/webui.png 170w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/9f933/webui.png 340w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/c5bb3/webui.png 680w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/b12f7/webui.png 1020w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/b5a09/webui.png 1360w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/29007/webui.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+klEQVR42pWQbU+DMBSF+f8/S5PpPkw2NcB4aSkWWijFbcmS4SLH27gsM6kYPzzJbc/t6b0neMlLhEmG8DXBc5TiNH5gAvA5TVemm9rHeD7jeBqxSTgC2RpwqVE2LerOQtkdSt1DtBYlURFRrcDoztXCA1c91LDH42aLwD3i+gIJed1dyQhWGzwxgS19WtxoP/pki8q8YxGSIVMWuTQkEI1B4WEpBOJag/2iO9O3fofFmgy5HsBoMh9u4rTpsOQlIqnobL19zlTaPR7WKQLRDpSZnaUixIzuTJvh8L1y4XJrzB90s3pKK1e08t0qdhn23lz+Q3bJ8H4V4wsH4k7oxFSq1AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"webui2\" title=\"\" src=\"/static/52ac3f0c144f0d186e2d73126b7d153d/c5bb3/webui2.png\" srcset=\"/static/52ac3f0c144f0d186e2d73126b7d153d/04472/webui2.png 170w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/9f933/webui2.png 340w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/c5bb3/webui2.png 680w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/b12f7/webui2.png 1020w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/b5a09/webui2.png 1360w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/29007/webui2.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n</li>\n</ol>\n<h3 id=\"-key-takeaways\" style=\"position:relative;\"><a href=\"#-key-takeaways\" aria-label=\" key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Key Takeaways</h3>\n<p>In this post, we explored how Argo Workflows and Argo Events can help us better manage and scale Spark applications on Amazon EKS. These tools have several advantages, such as allowing us to create and automate complex pipelines using Argo Workflows, and triggering actions based on events using Argo Events.</p>\n<p><strong>Stay tuned for next blogs in this¬†seriesüéâ</strong></p>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://spacelift.io/blog/argo-workflows\" target=\"_blank\" rel=\"noopener noreferrer\">https://spacelift.io/blog/argo-workflows</a></li>\n<li><a href=\"https://codefresh.io/learn/argo-workflows\" target=\"_blank\" rel=\"noopener noreferrer\">https://codefresh.io/learn/argo-workflows</a></li>\n<li><a href=\"https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples\" target=\"_blank\" rel=\"noopener noreferrer\">https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples</a></li>\n<li><a href=\"https://argo-workflows.readthedocs.io/en/latest\" target=\"_blank\" rel=\"noopener noreferrer\">https://argo-workflows.readthedocs.io/en/latest</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events\" target=\"_blank\" rel=\"noopener noreferrer\">https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events</a></li>\n<li><a href=\"https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything\" target=\"_blank\" rel=\"noopener noreferrer\">https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything</a></li>\n<li><a href=\"https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/\" target=\"_blank\" rel=\"noopener noreferrer\">https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/</a></li>\n<li>\n<iframe width=\"100%\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/FBRMURQYbgw?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</li>\n</ul>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **Container-native workflow engine for Kubernetes üîÆ**\n\n## üêô Introduction\n\nWelcome to the fourth part of the \"Data on Kubernetes\" series!\n\nIn this part, we'll explore [Argo Workflows](https://argo-workflows.readthedocs.io/en/latest/)‚Ää-‚Ääan open-source, container-native workflow engine designed specifically for orchestrating parallel jobs on Kubernetes.\n\nArgo Workflows seamlessly integrates with Kubernetes services like volumes, secrets, and RBAC, making it a powerful tool for managing complex data workflows.\n\n## Introduction to Argo Workflows\n\nIn our [previous blog post](https://seifrajhi.github.io/blog/data-on-kubernetes-airflow-on-aws-3/), we explored the world of workflow orchestrators, with a specific focus on Apache Airflow.\n\nNow, let's check another tool: Argo Workflows.\n\nAs an open-source, container-native workflow engine, Argo Workflows is purpose-built for orchestrating parallel jobs within Kubernetes environments.\n\nLet's take a closer look at why Argo Workflows stands out:\n\n### 1. What is Argo Workflows? ü§î\n\nArgo Workflows empowers you to define workflows where each step corresponds to a container. This flexibility allows for the creation of complex data processing pipelines. Think of it as assembling a series of building blocks, where each block represents a specific task or operation.\n\nAdditionally, Argo Workflows is implemented as a Kubernetes Custom Resource Definition (CRD), seamlessly integrating into the Kubernetes ecosystem without disrupting the existing setup.\n\n### 2. Main Features and Benefits\n\n**Container-Native Approach:**\n\nArgo Workflows fully embraces the containerization paradigm. Each step in your workflow runs within its own container, ensuring consistency and compatibility across different tasks. Whether you're dealing with data transformations, model training, or any other computational task, Argo Workflows has you covered.\n\n**DAG Modeling (Directed Acyclic Graphs):**\n\nImagine your workflow as a flowchart with interconnected boxes. Argo Workflows allows you to model multi-step workflows using [directed acyclic graphs (DAGs)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). These graphs capture dependencies between tasks, ensuring that steps execute in the correct order. It's like choreographing a dance where each move follows a logical sequence.\n\n**Efficient Compute Handling:**\n\nArgo Workflows shines when it comes to compute-intensive workloads. Whether you're crunching numbers, analyzing data, or training machine learning models, Argo efficiently manages the computational resources needed for each step.\n\n### 3. Integration with the Kubernetes Ecosystem\n\nArgo Workflows seamlessly integrates with other Kubernetes services:\n\n- **Volumes:** Need to read or write data? Argo Workflows plays well with [Kubernetes volumes](https://kubernetes.io/docs/concepts/storage/volumes/), allowing you to handle data storage efficiently.\n- **Secrets:** Security matters! Argo Workflows integrates with [Kubernetes secrets](https://kubernetes.io/docs/concepts/configuration/secret/), ensuring that sensitive information remains protected.\n- **RBAC (Role-Based Access Control):** Argo Workflows respects your access policies. It works harmoniously with [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/), allowing you to control who can orchestrate workflows and who can't.\n\n### 4. Why Choose Argo Workflows\n\nArgo Workflows stands out for several reasons:\n\n- **Kubernetes Integration:** Argo Workflows seamlessly integrates with Kubernetes, simplifying usage and deployment.\n- **Scalability and Complexity Handling:** It efficiently manages complex workflows, even those with thousands of steps.\n- **Flexible Workflow Patterns:** Argo supports various workflow structures, adapting to your needs.\n- **Active Open Source Community:** Benefit from a thriving community that actively contributes to Argo's development.\n\n##  Exploring Argo Workflows on Amazon EKS\n\nIn this section, we'll dive into creating and deploying a data processing platform on [Amazon Elastic Kubernetes Service (Amazon EKS)](https://aws.amazon.com/eks/).\n\nThe solution includes essential Kubernetes add-ons: [Argo Workflows](https://argo-workflows.readthedocs.io/en/latest/), [Argo Events](https://argoproj.github.io/argo-events/), [Spark Operator](https://github.com/kubeflow/spark-operator) for managing Spark jobs, [Fluent Bit](https://fluentbit.io/) for logging, and [Prometheus](https://prometheus.io/) for metrics.\n\n### üõ†Ô∏è Setup Overview\n\n**Cluster Setup:** We'll set up an Amazon EKS cluster with a managed node group. We'll install Argo Workflows and Argo Events in their own dedicated namespaces (`argo-workflows` and `argo-events`).\n\n**Event-Driven Workflow:** An [Amazon SQS queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html) will receive user requests. In the Argo Events namespace, an SQS event source object will fetch messages from this external queue. When a new message arrives, a sensor in Argo Workflow will trigger the specified workflow.\n\n**Spark Job Execution:** The triggered workflow will create a Spark application using Spark Operator in the `data-ops` namespace. This application consists of one Spark driver pod and two executor pods.\n\nThe diagram illustrates how Argo Events sources drive the execution of Spark jobs.\n\n![diagram](./diagram.png)\n\n> **Prerequisites:** Ensure you have [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html), [kubectl](https://kubernetes.io/docs/tasks/tools/), [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli), and [Argo Workflow CLI](https://github.com/argoproj/argo-workflows/releases/latest) installed.\n\nThe diagram below shows the detailed technical design that will be set up in this demo:\n\n![setup](./setup.png)\n\n### üöÄ Implementation Steps\n\n1. **Clone the Repository:**\n\n    ```shell\n    git clone https://github.com/awslabs/data-on-eks.git\n    cd data-on-eks/schedulers/terraform/argo-workflow\n    ```\n\n2. **Initialize and Apply Terraform:**\n\n    ```shell\n    terraform init\n    terraform apply -var region=eu-west-1 --auto-approve\n    ```\n\n    This will create:\n    - **EKS and Networking Infrastructure:**\n        - VPC with Private and Public Subnets.\n        - Internet Gateway (Public) and NAT Gateway (Private).\n        - EKS Cluster Control Plane with managed node group.\n    - **EKS Managed Add-ons:**\n        - VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_Driver.\n    - **Additional Kubernetes Components:**\n        - Argo Workflows, Argo Events, AWS for FluentBit, Karpenter.\n        - Metrics Server, CoreDNS Autoscaler, Cluster Autoscaler.\n        - Kube Prometheus Stack, Spark Operator.\n        - Roles for Argo Workflows and Argo Events.\n\n3. **Create the SQS Queue:**\n\n    ```shell\n    queue_url=$(aws sqs create-queue --queue-name demo-argo-workflows --region eu-west-1 --output text)\n    ```\n\n4. **Retrieve the Queue ARN:**\n\n    ```shell\n    sqs_queue_arn=$(aws sqs get-queue-attributes --queue-url $queue_url --attribute-names QueueArn --region eu-west-1 --query \"Attributes.QueueArn\" --output text)\n    ```\n\n    Update the SQS access policy:\n\n    ```shell\n    template=`cat argo-events-manifests/sqs-accesspolicy.json | sed -e \"s|<sqs_queue_arn>|$sqs_queue_arn|g;s|<your_event_irsa_arn>|$your_event_irsa_arn|g\"`\n    aws sqs set-queue-attributes --queue-url $queue_url --attributes $template --region eu-west-1\n    ```\n\n5. **Access Argo Workflows Web UI:**\n\n    Extract the bearer token:\n\n    ```shell\n    argo auth token\n    ```\n\n    Get the load balancer URL:\n\n    ```shell\n    kubectl -n argo-workflows get service argo-workflows-server -o jsonpath=\"{.status.loadBalancer.ingress[*].hostname}{'\\n'}\"\n    ```\n\n    Open your browser, enter the URL, and paste the token (including `Bearer`) into the login prompt.\n\n    ![argo](./argo.png)\n\n6. **Create a Spark Job within an Argo Workflow:**\n\n    Install the EventBus:\n\n    ```shell\n    kubectl apply -f argo-events-manifests/eventbus.yaml\n    ```\n\n    Set up Amazon SQS as an event source:\n\n    ```shell\n    queue_name=demo-argo-workflows\n    region_sqs=eu-west-1\n\n    cat argo-events-manifests/eventsource-sqs.yaml | sed \"s/<region_sqs>/$region_sqs/g;s/<queue_name>/$queue_name/g\" | kubectl apply -f -\n    ```\n\n    Deploy the sensor:\n\n    ```shell\n    kubectl apply -f argo-events-manifests/sensor-rbac.yaml\n    ```\n\n    Update `taxi-trip.sh` with your S3 bucket and region:\n\n    ```shell\n    if [ $# -ne 2 ]; then\n      echo \"Usage: $0 <s3_bucket> <region>\"\n      exit 1\n    fi\n\n    s3_bucket=\"$1\"\n    region=\"$2\"\n\n    INPUT_DATA_S3_PATH=\"s3://${s3_bucket}/taxi-trip/input/\"\n\n    mkdir input\n\n    aws s3 cp pyspark-taxi-trip.py s3://${s3_bucket}/taxi-trip/scripts/ --region ${region}\n\n    wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet -O \"input/yellow_tripdata_2022-0.parquet\"\n\n    max=5\n    for (( i=1; i <= $max; ++i ))\n    do\n       cp -rf \"input/yellow_tripdata_2022-0.parquet\" \"input/yellow_tripdata_2022-${i}.parquet\"\n    done\n\n    aws s3 sync \"input/\" ${INPUT_DATA_S3_PATH}\n\n    rm -rf input\n    ```\n\n    Execute the script:\n\n    ```shell\n    ./taxi-trip.sh\n    ```\n\n    Apply the sensor configuration:\n\n    ```shell\n    kubectl apply -f sensor-sqs-sparkjobs.yaml\n    ```\n\n7. **Test the Setup:**\n\n    Send a message from SQS:\n\n    ```shell\n    aws sqs send-message --queue-url $queue_url --message-body '{\"message\": \"hello data on k8s\"}' --region $region_sqs\n    ```\n\n    Check the workflow status:\n\n    ```shell\n    kubectl get workflows -n argo-workflows\n    ```\n\n    You can also see the SQS workflow status in the web UI:\n\n    ![webui](./webui.png)\n\n    ![webui2](./webui2.png)\n\n### üìå Key Takeaways\n\nIn this post, we explored how Argo Workflows and Argo Events can help us better manage and scale Spark applications on Amazon EKS. These tools have several advantages, such as allowing us to create and automate complex pipelines using Argo Workflows, and triggering actions based on events using Argo Events.\n\n**Stay tuned for next blogs in this¬†seriesüéâ**\n\n**References:**\n\n- https://spacelift.io/blog/argo-workflows\n- https://codefresh.io/learn/argo-workflows\n- https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples\n- https://argo-workflows.readthedocs.io/en/latest\n- https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks\n- https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events\n- https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything\n- https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/\n- https://www.youtube.com/watch?v=FBRMURQYbgw\n\n**Thank You üñ§**\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1012},"frontmatter":{"id":"5a8d4391432a6b8e14894f32","path":"/blog/data-on-kubernetes-argo-workflows-4/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Data on Kubernetes: Part 4‚Ää-‚ÄäArgo Workflows: Simplify parallel jobs","keywords":["Kubernetes","Argo Workflows","Workflow Automation","Parallel Jobs","Container-native"],"excerpt":"Discover how Argo Workflows can simplify the management of parallel jobs in Kubernetes, enhancing efficiency and scalability.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACoklEQVR42j2SyW4TURBF/Td8BDs+ALFnhcSGDb8ACkhh2LABBRIiQAExJiaxE5xWBmewYzu2u9122+7B7jGOBxLCFEBIh3KDWFzVe7eeblXdeglnbx6nssLAMxn6ThwHnsXQs+nHnP0fo8D5m5Pzodum75r/udCq4zU1EhtbSd6uv8Ry6jidBlHkcNDr0OkaRKEdc7ajC+q0TBXfa2PZOqNhSBjZtCwNPzA5lrvXVEm4ovpx4FOt7aEJbEtHlVhRdzGaZUrlLfZKm+QKa2ztZrBMjaJwXSnUaleFX5fCDiejCM8QwaCl8ev0hNPvn6QbgyNJFPaz7OQVanqRILBQ9XGBHEXh9XpBhFRpIC8N5Akl//vXN476Pm5DBD0RPP12jCmVS5VtStVt6awSd2FaNQzpIl/aYFsKODLqWDDwTcLQIl9cp1DOMjrqcfrliKAtHobtGj9EcDQIOOy5cRz0PYHPl5Mhx/I4Eq98vy0iNp7EgVg0lHdRZNFQC9Tyq7xWFsgsKyRsfR/X1Ak7TYIxHCNG2G3GXH+81UC2PN6m/y8K13Mtvn7+RHruLVcvnCM1fY03k49J3EpOcSc1zeT7h9xNT8fnW4uPuDH/gLnUc+xKnq5ejg33GlXcegXX0HBl1EC5T7g9IzkNvdqUvHiY3EyytJMWLPF44RXv1pKsFFZIZhdJLaeo7uYY/4RuoxKLRmJR2LHpZmYJJ87w8+lZkorOlWdfeblqkTho6nyU8bLJNW5fnmD25iyeVsZRSxiZJWpPHmA2G6TfPGduZoqcksasq3Q35jEnz+PPXOSDssed+QOyOUP+oYxx2KmjLChcv3SNqYlpzGqRjvDa8jt2Z+7hmQ06tX0KG6uspxeoF3cI2+J7S2c/m2Fz8QW+fK2eVeMPq2jvUMTf9AQAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/1cbbd/dok4.png","srcSet":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/38de2/dok4.png 750w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/f79fe/dok4.png 1080w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/8bf4f/dok4.png 1366w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/1cbbd/dok4.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/45f0d/dok4.webp 750w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/3c63c/dok4.webp 1080w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/9ee7a/dok4.webp 1366w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/cfdd9/dok4.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.55875}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/carvel-kapp-kubernetes-management/","title":"Simplify Kubernetes Application Management with Carvel kapp","date":"2024-10-25 18:06:00"},"excerpt":"Take control of your Kubernetes resourcesüî• üìî Introduction Kubernetes applications involve juggling multiple resources, configurations, and‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/finops-mayfly-kubernetes-ephemeral-resources/","title":"FinOps Best Practices: Using Mayfly Kubernetes Operator for Ephemeral Resources","date":"2024-10-25 15:34:00"},"excerpt":"Optimizing Resources Spend with the Mayfly Kubernetes Operator üìñ Introduction In the world of cloud computing, managing costs can be a‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}