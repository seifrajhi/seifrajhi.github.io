{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/data-on-kubernetes-argo-workflows-4/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Container-native workflow engine for Kubernetes üîÆ</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üêô Introduction</h2>\n<p>Welcome to the fourth part of the \"Data on Kubernetes\" series!</p>\n<p>In this part, we'll explore <a href=\"https://argo-workflows.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflows</a>‚Ää-‚Ääan open-source, container-native workflow engine designed specifically for orchestrating parallel jobs on Kubernetes.</p>\n<p>Argo Workflows seamlessly integrates with Kubernetes services like volumes, secrets, and RBAC, making it a powerful tool for managing complex data workflows.</p>\n<h2 id=\"introduction-to-argo-workflows\" style=\"position:relative;\"><a href=\"#introduction-to-argo-workflows\" aria-label=\"introduction to argo workflows permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Introduction to Argo Workflows</h2>\n<p>In our <a href=\"https://seifrajhi.github.io/blog/data-on-kubernetes-airflow-on-aws-3/\" target=\"_blank\" rel=\"noopener noreferrer\">previous blog post</a>, we explored the world of workflow orchestrators, with a specific focus on Apache Airflow.</p>\n<p>Now, let's check another tool: Argo Workflows.</p>\n<p>As an open-source, container-native workflow engine, Argo Workflows is purpose-built for orchestrating parallel jobs within Kubernetes environments.</p>\n<p>Let's take a closer look at why Argo Workflows stands out:</p>\n<h3 id=\"1-what-is-argo-workflows-\" style=\"position:relative;\"><a href=\"#1-what-is-argo-workflows-\" aria-label=\"1 what is argo workflows  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>1. What is Argo Workflows? ü§î</h3>\n<p>Argo Workflows empowers you to define workflows where each step corresponds to a container. This flexibility allows for the creation of complex data processing pipelines. Think of it as assembling a series of building blocks, where each block represents a specific task or operation.</p>\n<p>Additionally, Argo Workflows is implemented as a Kubernetes Custom Resource Definition (CRD), seamlessly integrating into the Kubernetes ecosystem without disrupting the existing setup.</p>\n<h3 id=\"2-main-features-and-benefits\" style=\"position:relative;\"><a href=\"#2-main-features-and-benefits\" aria-label=\"2 main features and benefits permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>2. Main Features and Benefits</h3>\n<p><strong>Container-Native Approach:</strong></p>\n<p>Argo Workflows fully embraces the containerization paradigm. Each step in your workflow runs within its own container, ensuring consistency and compatibility across different tasks. Whether you're dealing with data transformations, model training, or any other computational task, Argo Workflows has you covered.</p>\n<p><strong>DAG Modeling (Directed Acyclic Graphs):</strong></p>\n<p>Imagine your workflow as a flowchart with interconnected boxes. Argo Workflows allows you to model multi-step workflows using <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target=\"_blank\" rel=\"noopener noreferrer\">directed acyclic graphs (DAGs)</a>. These graphs capture dependencies between tasks, ensuring that steps execute in the correct order. It's like choreographing a dance where each move follows a logical sequence.</p>\n<p><strong>Efficient Compute Handling:</strong></p>\n<p>Argo Workflows shines when it comes to compute-intensive workloads. Whether you're crunching numbers, analyzing data, or training machine learning models, Argo efficiently manages the computational resources needed for each step.</p>\n<h3 id=\"3-integration-with-the-kubernetes-ecosystem\" style=\"position:relative;\"><a href=\"#3-integration-with-the-kubernetes-ecosystem\" aria-label=\"3 integration with the kubernetes ecosystem permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>3. Integration with the Kubernetes Ecosystem</h3>\n<p>Argo Workflows seamlessly integrates with other Kubernetes services:</p>\n<ul>\n<li><strong>Volumes:</strong> Need to read or write data? Argo Workflows plays well with <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes volumes</a>, allowing you to handle data storage efficiently.</li>\n<li><strong>Secrets:</strong> Security matters! Argo Workflows integrates with <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes secrets</a>, ensuring that sensitive information remains protected.</li>\n<li><strong>RBAC (Role-Based Access Control):</strong> Argo Workflows respects your access policies. It works harmoniously with <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes RBAC</a>, allowing you to control who can orchestrate workflows and who can't.</li>\n</ul>\n<h3 id=\"4-why-choose-argo-workflows\" style=\"position:relative;\"><a href=\"#4-why-choose-argo-workflows\" aria-label=\"4 why choose argo workflows permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>4. Why Choose Argo Workflows</h3>\n<p>Argo Workflows stands out for several reasons:</p>\n<ul>\n<li><strong>Kubernetes Integration:</strong> Argo Workflows seamlessly integrates with Kubernetes, simplifying usage and deployment.</li>\n<li><strong>Scalability and Complexity Handling:</strong> It efficiently manages complex workflows, even those with thousands of steps.</li>\n<li><strong>Flexible Workflow Patterns:</strong> Argo supports various workflow structures, adapting to your needs.</li>\n<li><strong>Active Open Source Community:</strong> Benefit from a thriving community that actively contributes to Argo's development.</li>\n</ul>\n<h2 id=\"exploring-argo-workflows-on-amazon-eks\" style=\"position:relative;\"><a href=\"#exploring-argo-workflows-on-amazon-eks\" aria-label=\"exploring argo workflows on amazon eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Exploring Argo Workflows on Amazon EKS</h2>\n<p>In this section, we'll dive into creating and deploying a data processing platform on <a href=\"https://aws.amazon.com/eks/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Kubernetes Service (Amazon EKS)</a>.</p>\n<p>The solution includes essential Kubernetes add-ons: <a href=\"https://argo-workflows.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflows</a>, <a href=\"https://argoproj.github.io/argo-events/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Events</a>, <a href=\"https://github.com/kubeflow/spark-operator\" target=\"_blank\" rel=\"noopener noreferrer\">Spark Operator</a> for managing Spark jobs, <a href=\"https://fluentbit.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Fluent Bit</a> for logging, and <a href=\"https://prometheus.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Prometheus</a> for metrics.</p>\n<h3 id=\"Ô∏è-setup-overview\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-setup-overview\" aria-label=\"Ô∏è setup overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Setup Overview</h3>\n<p><strong>Cluster Setup:</strong> We'll set up an Amazon EKS cluster with a managed node group. We'll install Argo Workflows and Argo Events in their own dedicated namespaces (<code class=\"language-text\">argo-workflows</code> and <code class=\"language-text\">argo-events</code>).</p>\n<p><strong>Event-Driven Workflow:</strong> An <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SQS queue</a> will receive user requests. In the Argo Events namespace, an SQS event source object will fetch messages from this external queue. When a new message arrives, a sensor in Argo Workflow will trigger the specified workflow.</p>\n<p><strong>Spark Job Execution:</strong> The triggered workflow will create a Spark application using Spark Operator in the <code class=\"language-text\">data-ops</code> namespace. This application consists of one Spark driver pod and two executor pods.</p>\n<p>The diagram illustrates how Argo Events sources drive the execution of Spark jobs.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 117.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsTAAALEwEAmpwYAAADOUlEQVR42qVVa08aQRTllzfppyZNtCIqgqJQUdO0tZ/6iC9EQAGtysobUXdBnrvuLshT4fTOLFL6iI+4yWV27pw595w7A5jw2NPvo9O9RZviKY/pAR7+tDpdOPwinPt5dDsdY+0lhDeNBr6EslgLZtButV6i0GBUrlVkL0TEk2notfoLCPlnD+sJBRZfETOBCraTMs+9yHIgWYDLm4RjU0AoU/xj7VmE7GnTgSiqDlHKI3N2jmutznPPttwbSPAkKjD7SrAHZR6WPRn+tMwb0n8O4f2BnEoq3IEsVn0puLYELAfOkShoD14d02OnVtN15PN5ZLPn9K497ZSZoNHoDcqzi/w5UoU1pGJ6r4q1SIUD/sbfx6MKe3e32IqI+HQg4WPoEtvC5eMK2anJWg2KXv8nZDphScohnUohlYhz6//DyhRq7QY9smbazShwCB24410sxQYRpXniDrb9Msa/RTH2NYo3a2GMf4/BcagR9o5jDDyN8Vu4Tuq4kmsweTIqFuJ9LKcBd2okaO4isNkjYnzjDO82s7Ds5rEoNAfY/gBr7HVG2xCrOkw7ZxoWYj0sM0ByJAjsIhWOozp97UqY8kpYOL7hOXcSI1i2l7CnbUhVrlDDIilcoSpsYRg0n/+pY3a/gtlglcYyrETsOKphJUPrRGZgjb2skEGYVmE/bsEZaWFxGE2y1sLcAV0XfxGTOyImti7ovYC5QxVOgWEaBo7hhTapr0NiPdyIlTG1r3DQwkmDBwM5yYKNlI39SOLVkgevV/2wEDFTzIotDvE3HD9DhcWKBtN6tAQbsX84h2GFLDipIiOcCRQx7StgyndFPczBulfm/WTKnKcdagvZzfSxmgUVKuGyrBoKrWEN76Nd2Mkis2cPX5M1nao3eR+nidC8fYF56h9TNUdXxxZSMOm5HBxUF7NUTDQIS5jw5jEXlqnpZW6JEdrDVdqsw04bmXX7wTWNMlxCg4qoXC0jsQUZXsEsuRHvFb7dFMlWARY/XQ8eRZh3cpjcLVBc8V/sabJqoTlbZ7lJEsGxlDd7CUM5id1DudZEPKcgkZNpZFFFXJJ5Lj7MyYOcjJhUHb7/xspI088a+7v9Bc1z22duIPu3AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"diagram\" title=\"\" src=\"/static/77747e1703c0180ae5de6102493873dc/c5bb3/diagram.png\" srcset=\"/static/77747e1703c0180ae5de6102493873dc/04472/diagram.png 170w,\n/static/77747e1703c0180ae5de6102493873dc/9f933/diagram.png 340w,\n/static/77747e1703c0180ae5de6102493873dc/c5bb3/diagram.png 680w,\n/static/77747e1703c0180ae5de6102493873dc/dc616/diagram.png 933w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<blockquote>\n<p><strong>Prerequisites:</strong> Ensure you have <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS CLI</a>, <a href=\"https://kubernetes.io/docs/tasks/tools/\" target=\"_blank\" rel=\"noopener noreferrer\">kubectl</a>, <a href=\"https://learn.hashicorp.com/tutorials/terraform/install-cli\" target=\"_blank\" rel=\"noopener noreferrer\">Terraform</a>, and <a href=\"https://github.com/argoproj/argo-workflows/releases/latest\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflow CLI</a> installed.</p>\n</blockquote>\n<p>The diagram below shows the detailed technical design that will be set up in this demo:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 35.29411764705882%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABvElEQVR42m2Q224SURSG54F8Cl/BO9/AF/CiMcaYKhpjTKxeaNOpLdQDAbFUEYRKT4BAD6lBraUHRgYGZ0+nUJihwwCfA9Y7d/Lly/+vnXWxJMcdYLb7NLt4HjDKo9cfDL3scnLWG8+NVg+9eY7wbFx0/zj1/o3cdQZIZsclV7bJbn2j+FPH7PxdaHX7bFUcdtQh24rLmxWFx6EdpsK7zCePyB90KRzaHp6PXDb2LNSTHpKw4MPyKv4bl0jF/IwyDLGcIbFsHfnFOrGcRuJrh+CGwJ/WiBZMPpc6Y1ZKLbIljbUfHRT9HKnegmA0Tn76MuvJABUxwLJsRLvHI1+Y21eu8fBehJdrNULLZeLJXeTEAa/TCgurDT7GcxjzV1GP9zFMC6kqHKYTNZ48l5Gj2yjCpd00Ma0+T+eKTF5/xtTcJnJaZ+aTymziFzNJjdmUhpxqEEgqvI8XCWc09lQbqVy3mQwq3Fw45tarCodad3xD0XJ4sFjH97bO/YjnUNVDxRe+YJTDVe563Z3IbyYCCpnvTaSG6RDJGSwVDBa/GDROnfHCM7tPNG/wLie8Xoxn/0ewlNcJZXT2azZ/AE3d4+DdQprxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"setup\" title=\"\" src=\"/static/7ca26b2a4e959d7b89e601c5b9c98a7c/c5bb3/setup.png\" srcset=\"/static/7ca26b2a4e959d7b89e601c5b9c98a7c/04472/setup.png 170w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/9f933/setup.png 340w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/c5bb3/setup.png 680w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/b12f7/setup.png 1020w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/b5a09/setup.png 1360w,\n/static/7ca26b2a4e959d7b89e601c5b9c98a7c/29007/setup.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-implementation-steps\" style=\"position:relative;\"><a href=\"#-implementation-steps\" aria-label=\" implementation steps permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Implementation Steps</h3>\n<ol>\n<li>\n<p><strong>Clone the Repository:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/awslabs/data-on-eks.git\n<span class=\"token builtin class-name\">cd</span> data-on-eks/schedulers/terraform/argo-workflow</code></pre></div>\n</li>\n<li>\n<p><strong>Initialize and Apply Terraform:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">terraform init\nterraform apply <span class=\"token parameter variable\">-var</span> <span class=\"token assign-left variable\">region</span><span class=\"token operator\">=</span>eu-west-1 --auto-approve</code></pre></div>\n<p>This will create:</p>\n<ul>\n<li><strong>EKS and Networking Infrastructure:</strong>\n<ul>\n<li>VPC with Private and Public Subnets.</li>\n<li>Internet Gateway (Public) and NAT Gateway (Private).</li>\n<li>EKS Cluster Control Plane with managed node group.</li>\n</ul>\n</li>\n<li><strong>EKS Managed Add-ons:</strong>\n<ul>\n<li>VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_Driver.</li>\n</ul>\n</li>\n<li><strong>Additional Kubernetes Components:</strong>\n<ul>\n<li>Argo Workflows, Argo Events, AWS for FluentBit, Karpenter.</li>\n<li>Metrics Server, CoreDNS Autoscaler, Cluster Autoscaler.</li>\n<li>Kube Prometheus Stack, Spark Operator.</li>\n<li>Roles for Argo Workflows and Argo Events.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Create the SQS Queue:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">queue_url</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>aws sqs create-queue --queue-name demo-argo-workflows <span class=\"token parameter variable\">--region</span> eu-west-1 <span class=\"token parameter variable\">--output</span> text<span class=\"token variable\">)</span></span></code></pre></div>\n</li>\n<li>\n<p><strong>Retrieve the Queue ARN:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">sqs_queue_arn</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>aws sqs get-queue-attributes --queue-url $queue_url --attribute-names QueueArn <span class=\"token parameter variable\">--region</span> eu-west-1 <span class=\"token parameter variable\">--query</span> <span class=\"token string\">\"Attributes.QueueArn\"</span> <span class=\"token parameter variable\">--output</span> text<span class=\"token variable\">)</span></span></code></pre></div>\n<p>Update the SQS access policy:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">template</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">`</span><span class=\"token function\">cat</span> argo-events-manifests/sqs-accesspolicy.json <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token parameter variable\">-e</span> <span class=\"token string\">\"s|&lt;sqs_queue_arn>|<span class=\"token variable\">$sqs_queue_arn</span>|g;s|&lt;your_event_irsa_arn>|<span class=\"token variable\">$your_event_irsa_arn</span>|g\"</span><span class=\"token variable\">`</span></span>\naws sqs set-queue-attributes --queue-url <span class=\"token variable\">$queue_url</span> <span class=\"token parameter variable\">--attributes</span> <span class=\"token variable\">$template</span> <span class=\"token parameter variable\">--region</span> eu-west-1</code></pre></div>\n</li>\n<li>\n<p><strong>Access Argo Workflows Web UI:</strong></p>\n<p>Extract the bearer token:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">argo auth token</code></pre></div>\n<p>Get the load balancer URL:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> argo-workflows get <span class=\"token function\">service</span> argo-workflows-server <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">\"{.status.loadBalancer.ingress[*].hostname}{'<span class=\"token entity\" title=\"\\n\">\\n</span>'}\"</span></code></pre></div>\n<p>Open your browser, enter the URL, and paste the token (including <code class=\"language-text\">Bearer</code>) into the login prompt.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 48.8235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABRElEQVR42pWQi26CQBBF+f8va2NTWzVC5bEosBRcHgVkiRZvhxURrU1bkpuZnZl7MoxmbzzI/QHFrkZZy4tkQzX5L3U+7Xm5AntPYfoRrCCGSfLjBCIrkFc7GmpQdoa6N9YjjUBpUSm/NiXgOs5g08PhW1UUeYlaSuwPB7Rt2+tzlLc4Ho8Yfw39ZefXprqJdZQOwC5ykWNL0OTjrIo22CFVsaRYIqONOp1qlVpCAV906wrIQoElCzBdudDdgHIfOr0n8wUeZnM8LkxMFhbmtocnwxn6phf1QOMGSFpRbvixgrMwhRtuEApGWwS0LUeUrOnWHPowI5T3LvAMZX3ucEFARjeSw73qpqI+I11AA/DVsL8Br9WZQniJi6TgyIsIcbahWtD3boCzt9+AJ6jhv9OdAlg+J4U/zhLQ+QPwfAIx0n3gF2bb6WswfMI1AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"argo\" title=\"\" src=\"/static/3eb6034f8353d03f9b039f02915e645f/c5bb3/argo.png\" srcset=\"/static/3eb6034f8353d03f9b039f02915e645f/04472/argo.png 170w,\n/static/3eb6034f8353d03f9b039f02915e645f/9f933/argo.png 340w,\n/static/3eb6034f8353d03f9b039f02915e645f/c5bb3/argo.png 680w,\n/static/3eb6034f8353d03f9b039f02915e645f/b12f7/argo.png 1020w,\n/static/3eb6034f8353d03f9b039f02915e645f/b5a09/argo.png 1360w,\n/static/3eb6034f8353d03f9b039f02915e645f/64639/argo.png 1568w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n</li>\n<li>\n<p><strong>Create a Spark Job within an Argo Workflow:</strong></p>\n<p>Install the EventBus:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> argo-events-manifests/eventbus.yaml</code></pre></div>\n<p>Set up Amazon SQS as an event source:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">queue_name</span><span class=\"token operator\">=</span>demo-argo-workflows\n<span class=\"token assign-left variable\">region_sqs</span><span class=\"token operator\">=</span>eu-west-1\n\n<span class=\"token function\">cat</span> argo-events-manifests/eventsource-sqs.yaml <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token string\">\"s/&lt;region_sqs>/<span class=\"token variable\">$region_sqs</span>/g;s/&lt;queue_name>/<span class=\"token variable\">$queue_name</span>/g\"</span> <span class=\"token operator\">|</span> kubectl apply <span class=\"token parameter variable\">-f</span> -</code></pre></div>\n<p>Deploy the sensor:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> argo-events-manifests/sensor-rbac.yaml</code></pre></div>\n<p>Update <code class=\"language-text\">taxi-trip.sh</code> with your S3 bucket and region:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token keyword\">if</span> <span class=\"token punctuation\">[</span> <span class=\"token variable\">$#</span> <span class=\"token parameter variable\">-ne</span> <span class=\"token number\">2</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span> <span class=\"token keyword\">then</span>\n  <span class=\"token builtin class-name\">echo</span> <span class=\"token string\">\"Usage: <span class=\"token variable\">$0</span> &lt;s3_bucket> &lt;region>\"</span>\n  <span class=\"token builtin class-name\">exit</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">fi</span>\n\n<span class=\"token assign-left variable\">s3_bucket</span><span class=\"token operator\">=</span><span class=\"token string\">\"<span class=\"token variable\">$1</span>\"</span>\n<span class=\"token assign-left variable\">region</span><span class=\"token operator\">=</span><span class=\"token string\">\"<span class=\"token variable\">$2</span>\"</span>\n\n<span class=\"token assign-left variable\">INPUT_DATA_S3_PATH</span><span class=\"token operator\">=</span><span class=\"token string\">\"s3://<span class=\"token variable\">${s3_bucket}</span>/taxi-trip/input/\"</span>\n\n<span class=\"token function\">mkdir</span> input\n\naws s3 <span class=\"token function\">cp</span> pyspark-taxi-trip.py s3://<span class=\"token variable\">${s3_bucket}</span>/taxi-trip/scripts/ <span class=\"token parameter variable\">--region</span> <span class=\"token variable\">${region}</span>\n\n<span class=\"token function\">wget</span> https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet <span class=\"token parameter variable\">-O</span> <span class=\"token string\">\"input/yellow_tripdata_2022-0.parquet\"</span>\n\n<span class=\"token assign-left variable\">max</span><span class=\"token operator\">=</span><span class=\"token number\">5</span>\n<span class=\"token keyword\">for</span> <span class=\"token variable\"><span class=\"token punctuation\">((</span> i<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;=</span> $max<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>i <span class=\"token punctuation\">))</span></span>\n<span class=\"token keyword\">do</span>\n   <span class=\"token function\">cp</span> <span class=\"token parameter variable\">-rf</span> <span class=\"token string\">\"input/yellow_tripdata_2022-0.parquet\"</span> <span class=\"token string\">\"input/yellow_tripdata_2022-<span class=\"token variable\">${i}</span>.parquet\"</span>\n<span class=\"token keyword\">done</span>\n\naws s3 <span class=\"token function\">sync</span> <span class=\"token string\">\"input/\"</span> <span class=\"token variable\">${INPUT_DATA_S3_PATH}</span>\n\n<span class=\"token function\">rm</span> <span class=\"token parameter variable\">-rf</span> input</code></pre></div>\n<p>Execute the script:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">./taxi-trip.sh</code></pre></div>\n<p>Apply the sensor configuration:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> sensor-sqs-sparkjobs.yaml</code></pre></div>\n</li>\n<li>\n<p><strong>Test the Setup:</strong></p>\n<p>Send a message from SQS:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">aws sqs send-message --queue-url <span class=\"token variable\">$queue_url</span> --message-body <span class=\"token string\">'{\"message\": \"hello data on k8s\"}'</span> <span class=\"token parameter variable\">--region</span> <span class=\"token variable\">$region_sqs</span></code></pre></div>\n<p>Check the workflow status:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get workflows <span class=\"token parameter variable\">-n</span> argo-workflows</code></pre></div>\n<p>You can also see the SQS workflow status in the web UI:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 49.411764705882355%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABBElEQVR42q2SUU/DIBDH+fZG48MSXVzsnE7jx9EHE31wc7pFN0cDbWlLKVT/Hi3GF1+0I/nljgP+3HGwvf0D3N4/4ubuAaa2RI2qxXbWfM+DH9DG/MSJsjJI8hLscHAMLlNsuIAqK2yTDHGqIFUJpQ1EViBOVGs9/pBUnU0L3SJontPejM6z4UkEYx1q5yBp02LN8Uws3wVWXGK1FQHyeYj9wssmxpJgg6MhDJXgmoZu0Ji9cczXMebe/oHZawcbnY7xCcBRhr5kv/hEgv+FRWdTyu4jCOr+guPJFJo61FDJud5BhheX11B5EQRNf8FRNEFtLfzYiWB0foWU3s532v+nvoJfHgHiSwjOf4UAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"webui\" title=\"\" src=\"/static/a6886faf142fff5d7b6ed8fdb42d33f0/c5bb3/webui.png\" srcset=\"/static/a6886faf142fff5d7b6ed8fdb42d33f0/04472/webui.png 170w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/9f933/webui.png 340w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/c5bb3/webui.png 680w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/b12f7/webui.png 1020w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/b5a09/webui.png 1360w,\n/static/a6886faf142fff5d7b6ed8fdb42d33f0/29007/webui.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+klEQVR42pWQbU+DMBSF+f8/S5PpPkw2NcB4aSkWWijFbcmS4SLH27gsM6kYPzzJbc/t6b0neMlLhEmG8DXBc5TiNH5gAvA5TVemm9rHeD7jeBqxSTgC2RpwqVE2LerOQtkdSt1DtBYlURFRrcDoztXCA1c91LDH42aLwD3i+gIJed1dyQhWGzwxgS19WtxoP/pki8q8YxGSIVMWuTQkEI1B4WEpBOJag/2iO9O3fofFmgy5HsBoMh9u4rTpsOQlIqnobL19zlTaPR7WKQLRDpSZnaUixIzuTJvh8L1y4XJrzB90s3pKK1e08t0qdhn23lz+Q3bJ8H4V4wsH4k7oxFSq1AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"webui2\" title=\"\" src=\"/static/52ac3f0c144f0d186e2d73126b7d153d/c5bb3/webui2.png\" srcset=\"/static/52ac3f0c144f0d186e2d73126b7d153d/04472/webui2.png 170w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/9f933/webui2.png 340w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/c5bb3/webui2.png 680w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/b12f7/webui2.png 1020w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/b5a09/webui2.png 1360w,\n/static/52ac3f0c144f0d186e2d73126b7d153d/29007/webui2.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n</li>\n</ol>\n<h3 id=\"-key-takeaways\" style=\"position:relative;\"><a href=\"#-key-takeaways\" aria-label=\" key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Key Takeaways</h3>\n<p>In this post, we explored how Argo Workflows and Argo Events can help us better manage and scale Spark applications on Amazon EKS. These tools have several advantages, such as allowing us to create and automate complex pipelines using Argo Workflows, and triggering actions based on events using Argo Events.</p>\n<p><strong>Stay tuned for next blogs in this¬†seriesüéâ</strong></p>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://spacelift.io/blog/argo-workflows\" target=\"_blank\" rel=\"noopener noreferrer\">https://spacelift.io/blog/argo-workflows</a></li>\n<li><a href=\"https://codefresh.io/learn/argo-workflows\" target=\"_blank\" rel=\"noopener noreferrer\">https://codefresh.io/learn/argo-workflows</a></li>\n<li><a href=\"https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples\" target=\"_blank\" rel=\"noopener noreferrer\">https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples</a></li>\n<li><a href=\"https://argo-workflows.readthedocs.io/en/latest\" target=\"_blank\" rel=\"noopener noreferrer\">https://argo-workflows.readthedocs.io/en/latest</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events\" target=\"_blank\" rel=\"noopener noreferrer\">https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events</a></li>\n<li><a href=\"https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything\" target=\"_blank\" rel=\"noopener noreferrer\">https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything</a></li>\n<li><a href=\"https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/\" target=\"_blank\" rel=\"noopener noreferrer\">https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/</a></li>\n<li>\n<iframe width=\"100%\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/FBRMURQYbgw?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</li>\n</ul>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **Container-native workflow engine for Kubernetes üîÆ**\n\n## üêô Introduction\n\nWelcome to the fourth part of the \"Data on Kubernetes\" series!\n\nIn this part, we'll explore [Argo Workflows](https://argo-workflows.readthedocs.io/en/latest/)‚Ää-‚Ääan open-source, container-native workflow engine designed specifically for orchestrating parallel jobs on Kubernetes.\n\nArgo Workflows seamlessly integrates with Kubernetes services like volumes, secrets, and RBAC, making it a powerful tool for managing complex data workflows.\n\n## Introduction to Argo Workflows\n\nIn our [previous blog post](https://seifrajhi.github.io/blog/data-on-kubernetes-airflow-on-aws-3/), we explored the world of workflow orchestrators, with a specific focus on Apache Airflow.\n\nNow, let's check another tool: Argo Workflows.\n\nAs an open-source, container-native workflow engine, Argo Workflows is purpose-built for orchestrating parallel jobs within Kubernetes environments.\n\nLet's take a closer look at why Argo Workflows stands out:\n\n### 1. What is Argo Workflows? ü§î\n\nArgo Workflows empowers you to define workflows where each step corresponds to a container. This flexibility allows for the creation of complex data processing pipelines. Think of it as assembling a series of building blocks, where each block represents a specific task or operation.\n\nAdditionally, Argo Workflows is implemented as a Kubernetes Custom Resource Definition (CRD), seamlessly integrating into the Kubernetes ecosystem without disrupting the existing setup.\n\n### 2. Main Features and Benefits\n\n**Container-Native Approach:**\n\nArgo Workflows fully embraces the containerization paradigm. Each step in your workflow runs within its own container, ensuring consistency and compatibility across different tasks. Whether you're dealing with data transformations, model training, or any other computational task, Argo Workflows has you covered.\n\n**DAG Modeling (Directed Acyclic Graphs):**\n\nImagine your workflow as a flowchart with interconnected boxes. Argo Workflows allows you to model multi-step workflows using [directed acyclic graphs (DAGs)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). These graphs capture dependencies between tasks, ensuring that steps execute in the correct order. It's like choreographing a dance where each move follows a logical sequence.\n\n**Efficient Compute Handling:**\n\nArgo Workflows shines when it comes to compute-intensive workloads. Whether you're crunching numbers, analyzing data, or training machine learning models, Argo efficiently manages the computational resources needed for each step.\n\n### 3. Integration with the Kubernetes Ecosystem\n\nArgo Workflows seamlessly integrates with other Kubernetes services:\n\n- **Volumes:** Need to read or write data? Argo Workflows plays well with [Kubernetes volumes](https://kubernetes.io/docs/concepts/storage/volumes/), allowing you to handle data storage efficiently.\n- **Secrets:** Security matters! Argo Workflows integrates with [Kubernetes secrets](https://kubernetes.io/docs/concepts/configuration/secret/), ensuring that sensitive information remains protected.\n- **RBAC (Role-Based Access Control):** Argo Workflows respects your access policies. It works harmoniously with [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/), allowing you to control who can orchestrate workflows and who can't.\n\n### 4. Why Choose Argo Workflows\n\nArgo Workflows stands out for several reasons:\n\n- **Kubernetes Integration:** Argo Workflows seamlessly integrates with Kubernetes, simplifying usage and deployment.\n- **Scalability and Complexity Handling:** It efficiently manages complex workflows, even those with thousands of steps.\n- **Flexible Workflow Patterns:** Argo supports various workflow structures, adapting to your needs.\n- **Active Open Source Community:** Benefit from a thriving community that actively contributes to Argo's development.\n\n##  Exploring Argo Workflows on Amazon EKS\n\nIn this section, we'll dive into creating and deploying a data processing platform on [Amazon Elastic Kubernetes Service (Amazon EKS)](https://aws.amazon.com/eks/).\n\nThe solution includes essential Kubernetes add-ons: [Argo Workflows](https://argo-workflows.readthedocs.io/en/latest/), [Argo Events](https://argoproj.github.io/argo-events/), [Spark Operator](https://github.com/kubeflow/spark-operator) for managing Spark jobs, [Fluent Bit](https://fluentbit.io/) for logging, and [Prometheus](https://prometheus.io/) for metrics.\n\n### üõ†Ô∏è Setup Overview\n\n**Cluster Setup:** We'll set up an Amazon EKS cluster with a managed node group. We'll install Argo Workflows and Argo Events in their own dedicated namespaces (`argo-workflows` and `argo-events`).\n\n**Event-Driven Workflow:** An [Amazon SQS queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html) will receive user requests. In the Argo Events namespace, an SQS event source object will fetch messages from this external queue. When a new message arrives, a sensor in Argo Workflow will trigger the specified workflow.\n\n**Spark Job Execution:** The triggered workflow will create a Spark application using Spark Operator in the `data-ops` namespace. This application consists of one Spark driver pod and two executor pods.\n\nThe diagram illustrates how Argo Events sources drive the execution of Spark jobs.\n\n![diagram](./diagram.png)\n\n> **Prerequisites:** Ensure you have [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html), [kubectl](https://kubernetes.io/docs/tasks/tools/), [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli), and [Argo Workflow CLI](https://github.com/argoproj/argo-workflows/releases/latest) installed.\n\nThe diagram below shows the detailed technical design that will be set up in this demo:\n\n![setup](./setup.png)\n\n### üöÄ Implementation Steps\n\n1. **Clone the Repository:**\n\n    ```shell\n    git clone https://github.com/awslabs/data-on-eks.git\n    cd data-on-eks/schedulers/terraform/argo-workflow\n    ```\n\n2. **Initialize and Apply Terraform:**\n\n    ```shell\n    terraform init\n    terraform apply -var region=eu-west-1 --auto-approve\n    ```\n\n    This will create:\n    - **EKS and Networking Infrastructure:**\n        - VPC with Private and Public Subnets.\n        - Internet Gateway (Public) and NAT Gateway (Private).\n        - EKS Cluster Control Plane with managed node group.\n    - **EKS Managed Add-ons:**\n        - VPC_CNI, CoreDNS, Kube_Proxy, EBS_CSI_Driver.\n    - **Additional Kubernetes Components:**\n        - Argo Workflows, Argo Events, AWS for FluentBit, Karpenter.\n        - Metrics Server, CoreDNS Autoscaler, Cluster Autoscaler.\n        - Kube Prometheus Stack, Spark Operator.\n        - Roles for Argo Workflows and Argo Events.\n\n3. **Create the SQS Queue:**\n\n    ```shell\n    queue_url=$(aws sqs create-queue --queue-name demo-argo-workflows --region eu-west-1 --output text)\n    ```\n\n4. **Retrieve the Queue ARN:**\n\n    ```shell\n    sqs_queue_arn=$(aws sqs get-queue-attributes --queue-url $queue_url --attribute-names QueueArn --region eu-west-1 --query \"Attributes.QueueArn\" --output text)\n    ```\n\n    Update the SQS access policy:\n\n    ```shell\n    template=`cat argo-events-manifests/sqs-accesspolicy.json | sed -e \"s|<sqs_queue_arn>|$sqs_queue_arn|g;s|<your_event_irsa_arn>|$your_event_irsa_arn|g\"`\n    aws sqs set-queue-attributes --queue-url $queue_url --attributes $template --region eu-west-1\n    ```\n\n5. **Access Argo Workflows Web UI:**\n\n    Extract the bearer token:\n\n    ```shell\n    argo auth token\n    ```\n\n    Get the load balancer URL:\n\n    ```shell\n    kubectl -n argo-workflows get service argo-workflows-server -o jsonpath=\"{.status.loadBalancer.ingress[*].hostname}{'\\n'}\"\n    ```\n\n    Open your browser, enter the URL, and paste the token (including `Bearer`) into the login prompt.\n\n    ![argo](./argo.png)\n\n6. **Create a Spark Job within an Argo Workflow:**\n\n    Install the EventBus:\n\n    ```shell\n    kubectl apply -f argo-events-manifests/eventbus.yaml\n    ```\n\n    Set up Amazon SQS as an event source:\n\n    ```shell\n    queue_name=demo-argo-workflows\n    region_sqs=eu-west-1\n\n    cat argo-events-manifests/eventsource-sqs.yaml | sed \"s/<region_sqs>/$region_sqs/g;s/<queue_name>/$queue_name/g\" | kubectl apply -f -\n    ```\n\n    Deploy the sensor:\n\n    ```shell\n    kubectl apply -f argo-events-manifests/sensor-rbac.yaml\n    ```\n\n    Update `taxi-trip.sh` with your S3 bucket and region:\n\n    ```shell\n    if [ $# -ne 2 ]; then\n      echo \"Usage: $0 <s3_bucket> <region>\"\n      exit 1\n    fi\n\n    s3_bucket=\"$1\"\n    region=\"$2\"\n\n    INPUT_DATA_S3_PATH=\"s3://${s3_bucket}/taxi-trip/input/\"\n\n    mkdir input\n\n    aws s3 cp pyspark-taxi-trip.py s3://${s3_bucket}/taxi-trip/scripts/ --region ${region}\n\n    wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet -O \"input/yellow_tripdata_2022-0.parquet\"\n\n    max=5\n    for (( i=1; i <= $max; ++i ))\n    do\n       cp -rf \"input/yellow_tripdata_2022-0.parquet\" \"input/yellow_tripdata_2022-${i}.parquet\"\n    done\n\n    aws s3 sync \"input/\" ${INPUT_DATA_S3_PATH}\n\n    rm -rf input\n    ```\n\n    Execute the script:\n\n    ```shell\n    ./taxi-trip.sh\n    ```\n\n    Apply the sensor configuration:\n\n    ```shell\n    kubectl apply -f sensor-sqs-sparkjobs.yaml\n    ```\n\n7. **Test the Setup:**\n\n    Send a message from SQS:\n\n    ```shell\n    aws sqs send-message --queue-url $queue_url --message-body '{\"message\": \"hello data on k8s\"}' --region $region_sqs\n    ```\n\n    Check the workflow status:\n\n    ```shell\n    kubectl get workflows -n argo-workflows\n    ```\n\n    You can also see the SQS workflow status in the web UI:\n\n    ![webui](./webui.png)\n\n    ![webui2](./webui2.png)\n\n### üìå Key Takeaways\n\nIn this post, we explored how Argo Workflows and Argo Events can help us better manage and scale Spark applications on Amazon EKS. These tools have several advantages, such as allowing us to create and automate complex pipelines using Argo Workflows, and triggering actions based on events using Argo Events.\n\n**Stay tuned for next blogs in this¬†seriesüéâ**\n\n**References:**\n\n- https://spacelift.io/blog/argo-workflows\n- https://codefresh.io/learn/argo-workflows\n- https://codefresh.io/learn/argo-workflows/learn-argo-workflows-with-8-simple-examples\n- https://argo-workflows.readthedocs.io/en/latest\n- https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/argo-workflows-eks\n- https://aws.amazon.com/blogs/containers/dynamic-spark-scaling-on-amazon-eks-with-argo-workflows-and-events\n- https://mkdev.me/posts/argo-ecosystem-argo-cd-argo-workflows-argo-events-argo-rollouts-argo-everything\n- https://dok.community/blog/scheduled-scaling-with-dask-and-argo-workflows/\n- https://www.youtube.com/watch?v=FBRMURQYbgw\n\n**Thank You üñ§**\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1012},"frontmatter":{"id":"5a8d4391432a6b8e14894f32","path":"/blog/data-on-kubernetes-argo-workflows-4/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Data on Kubernetes: Part 4‚Ää-‚ÄäArgo Workflows: Simplify parallel jobs","keywords":["Kubernetes","Argo Workflows","Workflow Automation","Parallel Jobs","Container-native"],"excerpt":"Discover how Argo Workflows can simplify the management of parallel jobs in Kubernetes, enhancing efficiency and scalability.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACoklEQVR42j2SyW4TURBF/Td8BDs+ALFnhcSGDb8ACkhh2LABBRIiQAExJiaxE5xWBmewYzu2u9122+7B7jGOBxLCFEBIh3KDWFzVe7eeblXdeglnbx6nssLAMxn6ThwHnsXQs+nHnP0fo8D5m5Pzodum75r/udCq4zU1EhtbSd6uv8Ry6jidBlHkcNDr0OkaRKEdc7ajC+q0TBXfa2PZOqNhSBjZtCwNPzA5lrvXVEm4ovpx4FOt7aEJbEtHlVhRdzGaZUrlLfZKm+QKa2ztZrBMjaJwXSnUaleFX5fCDiejCM8QwaCl8ev0hNPvn6QbgyNJFPaz7OQVanqRILBQ9XGBHEXh9XpBhFRpIC8N5Akl//vXN476Pm5DBD0RPP12jCmVS5VtStVt6awSd2FaNQzpIl/aYFsKODLqWDDwTcLQIl9cp1DOMjrqcfrliKAtHobtGj9EcDQIOOy5cRz0PYHPl5Mhx/I4Eq98vy0iNp7EgVg0lHdRZNFQC9Tyq7xWFsgsKyRsfR/X1Ak7TYIxHCNG2G3GXH+81UC2PN6m/y8K13Mtvn7+RHruLVcvnCM1fY03k49J3EpOcSc1zeT7h9xNT8fnW4uPuDH/gLnUc+xKnq5ejg33GlXcegXX0HBl1EC5T7g9IzkNvdqUvHiY3EyytJMWLPF44RXv1pKsFFZIZhdJLaeo7uYY/4RuoxKLRmJR2LHpZmYJJ87w8+lZkorOlWdfeblqkTho6nyU8bLJNW5fnmD25iyeVsZRSxiZJWpPHmA2G6TfPGduZoqcksasq3Q35jEnz+PPXOSDssed+QOyOUP+oYxx2KmjLChcv3SNqYlpzGqRjvDa8jt2Z+7hmQ06tX0KG6uspxeoF3cI2+J7S2c/m2Fz8QW+fK2eVeMPq2jvUMTf9AQAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/1cbbd/dok4.png","srcSet":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/38de2/dok4.png 750w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/f79fe/dok4.png 1080w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/8bf4f/dok4.png 1366w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/1cbbd/dok4.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/3afa5d844f90e8fbfb49d3a8845f4ac3/45f0d/dok4.webp 750w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/3c63c/dok4.webp 1080w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/9ee7a/dok4.webp 1366w,\n/static/3afa5d844f90e8fbfb49d3a8845f4ac3/cfdd9/dok4.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.55875}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/carvel-kapp-kubernetes-management/","title":"Simplify Kubernetes Application Management with Carvel kapp","date":"2024-10-25 18:06:00"},"excerpt":"Take control of your Kubernetes resourcesüî• üìî Introduction Kubernetes applications involve juggling multiple resources, configurations, and‚Ä¶","html":"<blockquote>\n<p><strong>Take control of your Kubernetes resourcesüî•</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìî Introduction</h2>\n<p>Kubernetes applications involve juggling multiple resources, configurations, and dependencies. Carvel kapp offers a straightforward solution to this complexity. It treats related Kubernetes resources as logical \"applications,\" allowing you to deploy, update, and manage them as cohesive units. With kapp, you can safely apply changes, preview diffs, and prune obsolete resources‚Ää‚Äî‚Ääall while minimizing disruptions to running workloads.</p>\n<h2 id=\"kapp-overview\" style=\"position:relative;\"><a href=\"#kapp-overview\" aria-label=\"kapp overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>kapp Overview</h2>\n<p>Carvel kapp is a lightweight, command-line tool that simplifies the deployment and management of Kubernetes applications. Unlike some other tools, kapp does not require any server-side components, elevated privileges, or custom resources, making it highly portable and easy to use, even in RBAC-constrained clusters.</p>\n<p>At its core, kapp is designed to be explicit, providing visibility into the changes it will apply to your cluster before executing them. It calculates the differences between your desired configuration and the live cluster state, presenting a clear set of create, update, and delete operations for your approval.</p>\n<p>One of kapp's standout features is its dependency-aware resource ordering. It intelligently manages the deployment order of certain resources, ensuring that dependencies are respected. For instance, Custom Resource Definitions (CRDs) and Namespaces are installed before the resources that depend on them. Additionally, kapp allows you to declare your own dependency rules, such as requiring a Job to complete database migrations before updating a Deployment.</p>\n<h2 id=\"deploy-kubernetes-applications-with-kapp\" style=\"position:relative;\"><a href=\"#deploy-kubernetes-applications-with-kapp\" aria-label=\"deploy kubernetes applications with kapp permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deploy Kubernetes Applications with kapp</h2>\n<p>Before getting too deep, let's get some basic preparations out of the way:</p>\n<ol>\n<li><strong>Create a Kubernetes cluster.</strong></li>\n<li><strong>Install <a href=\"https://carvel.dev/ytt/\" target=\"_blank\" rel=\"noopener noreferrer\">ytt</a>, <a href=\"https://carvel.dev/kbld/\" target=\"_blank\" rel=\"noopener noreferrer\">kbld</a>, <a href=\"https://carvel.dev/kapp/\" target=\"_blank\" rel=\"noopener noreferrer\">kapp</a></strong> by following instructions in the <a href=\"https://carvel.dev/#install\" target=\"_blank\" rel=\"noopener noreferrer\">Install section on carvel.dev</a>.</li>\n</ol>\n<p>To get started with our example application, clone <a href=\"https://github.com/seifrajhi/kapp-k8s-demo.git\" target=\"_blank\" rel=\"noopener noreferrer\">kapp-k8s-demo</a> locally:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/kapp-k8s-demo.git\n<span class=\"token builtin class-name\">cd</span> kapp-k8s-demo</code></pre></div>\n<p>This directory contains a simple Go application that consists of main.go (an HTTP web server) and a Dockerfile¬†.\nMultiple step-* directories contain variations of application configuration that we will use in each step.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ <span class=\"token function\">ls</span> <span class=\"token parameter variable\">-l</span>\nDockerfile\nmain.go\nconfig-minimal</code></pre></div>\n<p>Typically, an application deployed to Kubernetes will include <strong>Deployment</strong> and <strong>Service</strong> resources in its configuration. In our example, <code class=\"language-text\">config-minimal/</code> directory contains <code class=\"language-text\">config.yml</code> which includes exactly that. (Note that the Docker image is already preset and the environment variable <code class=\"language-text\">HELLO_MSG</code> is hard coded. We'll get to those shortly.)</p>\n<p>Traditionally, you can use <code class=\"language-text\">kubectl apply -f config-minimal/config.yml</code> to deploy this application. However, <code class=\"language-text\">kubectl</code> does not indicate which resources are affected and how they are affected before applying changes, and does not yet have a robust prune functionality to converge a set of resources.</p>\n<p><strong>kapp</strong> addresses and improves on several <code class=\"language-text\">kubectl</code> limitations as it was designed from the start around the notion of a \"Kubernetes Application\" ‚Äî a set of resources with the same label:</p>\n<ul>\n<li><strong>Visibility and Confidence</strong>: <code class=\"language-text\">kapp</code> separates the change calculation phase (diff) from the change apply phase (apply) to give users visibility and confidence regarding what's about to change in the cluster.</li>\n<li><strong>Resource Tracking</strong>: <code class=\"language-text\">kapp</code> tracks and converges resources based on a unique generated label, freeing its users from worrying about cleaning up old deleted resources as the application is updated.</li>\n<li><strong>Order Management</strong>: <code class=\"language-text\">kapp</code> orders certain resources so that the Kubernetes API server can successfully process them (e.g., CRDs and namespaces before other resources).</li>\n<li><strong>Readiness Wait</strong>: <code class=\"language-text\">kapp</code> tries to wait for resources to become ready before considering the deploy a success.</li>\n</ul>\n<p>For more information, you can visit the <a href=\"https://carvel.dev/kapp/docs/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Carvel kapp documentation</a>.</p>\n<p>Let us deploy our application with kapp:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp deploy <span class=\"token parameter variable\">-a</span> simple-app <span class=\"token parameter variable\">-f</span> step-1-minimal/\nTarget cluster <span class=\"token punctuation\">[</span><span class=\"token string\">'https://192.168.99.111:8443'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>https://192.168.99.111:8443<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nChanges\nNamespace  Name        Kind        Conds.  Age  Op      Op st.  Wait to    Rs  Ri\ndefault    simple-app  Deployment  -       -    create  -       reconcile  -   -\n^          simple-app  Service     -       -    create  -       reconcile  -   -\nOp:      <span class=\"token number\">2</span> create, <span class=\"token number\">0</span> delete, <span class=\"token number\">0</span> update, <span class=\"token number\">0</span> noop\nWait to: <span class=\"token number\">2</span> reconcile, <span class=\"token number\">0</span> delete, <span class=\"token number\">0</span> noop\nContinue? <span class=\"token punctuation\">[</span>yN<span class=\"token punctuation\">]</span>: y\n<span class=\"token number\">8</span>:17:44PM: ---- applying <span class=\"token number\">2</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">0</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:44PM: create deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:44PM: create service/simple-app <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:44PM: ---- waiting on <span class=\"token number\">2</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">0</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:45PM: ok: reconcile service/simple-app <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM: ongoing: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:  ^ Waiting <span class=\"token keyword\">for</span> generation <span class=\"token number\">2</span> to be observed\n<span class=\"token number\">8</span>:17:45PM:    ok: waiting on replicaset/simple-app-7fbc6b7c9b <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:    ongoing: waiting on pod/simple-app-7fbc6b7c9b-g92t7 <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:     ^ Pending: ContainerCreating\n<span class=\"token number\">8</span>:17:45PM: ---- waiting on <span class=\"token number\">1</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">1</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:45PM: ongoing: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:  ^ Waiting <span class=\"token keyword\">for</span> <span class=\"token number\">1</span> unavailable replicas\n<span class=\"token number\">8</span>:17:45PM:    ok: waiting on replicaset/simple-app-7fbc6b7c9b <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:    ongoing: waiting on pod/simple-app-7fbc6b7c9b-g92t7 <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:     ^ Pending: ContainerCreating\n<span class=\"token number\">8</span>:17:49PM: ok: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:49PM: ---- applying complete <span class=\"token punctuation\">[</span><span class=\"token number\">2</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:49PM: ---- waiting complete <span class=\"token punctuation\">[</span><span class=\"token number\">2</span>/2 done<span class=\"token punctuation\">]</span> ----\nSucceeded</code></pre></div>\n<p>Our simple-app received a unique label kapp.k14s.io/app=1557433075084066000 for resource tracking:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp <span class=\"token function\">ls</span>\nTarget cluster <span class=\"token punctuation\">[</span><span class=\"token string\">'https://192.168.99.111:8443'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>https://192.168.99.111:8443<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nApps <span class=\"token keyword\">in</span> namespace <span class=\"token string\">'default'</span>\nName        Namespaces  Lcs   Lca\nsimple-app  default     <span class=\"token boolean\">true</span>  23s\n<span class=\"token number\">1</span> apps\nSucceeded</code></pre></div>\n<p>Using this label, kapp tracks and allows inspection of all Kubernetes resources created for sample-app:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp inspect <span class=\"token parameter variable\">-a</span> simple-app <span class=\"token parameter variable\">--tree</span>\nTarget cluster <span class=\"token string\">'https://192.168.99.111:8443'</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nResources <span class=\"token keyword\">in</span> app <span class=\"token string\">'simple-app'</span>\nNamespace  Name                              Kind        Owner    Conds.  Rs  Ri  Age\ndefault    simple-app                        Deployment  kapp     <span class=\"token number\">2</span>/2 t   ok  -   46s\ndefault       simple-app-7fbc6b7c9b          ReplicaSet  cluster  -       ok  -   46s\ndefault         simple-app-7fbc6b7c9b-g92t7  Pod         cluster  <span class=\"token number\">4</span>/4 t   ok  -   46s\ndefault    simple-app                        Service     kapp     -       ok  -   46s\ndefault       simple-app                     Endpoints   cluster  -       ok  -   46s\nRs: Reconcile state\nRi: Reconcile information\n<span class=\"token number\">5</span> resources\nSucceeded</code></pre></div>\n<p>Note that it even knows about resources it did not directly create (such as ReplicaSet and Endpoints).</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp logs <span class=\"token parameter variable\">-f</span> <span class=\"token parameter variable\">-a</span> simple-app\nTarget cluster <span class=\"token string\">'https://192.168.99.111:8443'</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># starting tailing 'simple-app-7fbc6b7c9b-g92t7 > simple-app' logs</span>\nsimple-app-7fbc6b7c9b-g92t7 <span class=\"token operator\">></span> simple-app <span class=\"token operator\">|</span> <span class=\"token number\">2020</span>/12/14 01:17:48 Server started</code></pre></div>\n<p>inspect and logs commands demonstrate why it's convenient to view resources in \"bulk\" (via a label). For example, logs command will tail any existing or new Pod that is part of simple-app application, even after we make changes and redeploy.</p>\n<h3 id=\"deploying-configuration-changes\" style=\"position:relative;\"><a href=\"#deploying-configuration-changes\" aria-label=\"deploying configuration changes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deploying configuration changes</h3>\n<p>Let's make a change to the application configuration to simulate a common occurrence in a development workflow. A simple observable change we can make is to change the value of the HELLO_MSG environment variable in <a href=\"https://github.com/carvel-dev/simple-app-on-kubernetes/blob/develop/config-step-1-minimal/config.yml\" target=\"_blank\" rel=\"noopener noreferrer\">config-step-1-minimal/config.yml</a>:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 626px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 22.941176470588236%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABHElEQVR42lWQ2U7CUBRF+f+PMMYYxSAYFJC0DdBaUJnEiNEAMggBZC60hduqyxswGh5W9n5aZwgIe4XotXHaDVat+rbbsi+bddz3puSNRf0Fu99F+D4bz2Mj/hFC7BEQkxFeTqUVPeX1/AgrHWOQjFALH/MhsxcPUQsd0tVVPj2B59r4a5cvscZbO1Ky2Rd60zHfeQOKJpSyULj56/NUgn7igrFyhWtqTLW4JMFIuaQZCTIzNDnAlaL/TQMTa0i+laTc1STqNksyH3oaxnOUZCGIWg6RrcXIVCMoxTOu704IGwco+RDWcoHvyVdsdpsGps6M4rxCxX7a49GpkhvcY3Sy6B2TwqTM7TBPqq6TbujobZNMzcBaWTvh7+k/5vVm70G3AfEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"alt text\" title=\"\" src=\"/static/0d16a6b8266dc46deaad6fd887b5b158/af590/change.png\" srcset=\"/static/0d16a6b8266dc46deaad6fd887b5b158/04472/change.png 170w,\n/static/0d16a6b8266dc46deaad6fd887b5b158/9f933/change.png 340w,\n/static/0d16a6b8266dc46deaad6fd887b5b158/af590/change.png 626w\" sizes=\"(max-width: 626px) 100vw, 626px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>and <strong>re-run kapp deploy command</strong>.</p>\n<p>The output should highlight several kapp features:</p>\n<p>kapp detected a single change to simple-app Deployment by comparing given local configuration against the live cluster copy</p>\n<p>kapp showed changes in a git-style diff via <code class=\"language-text\">--diff-changes</code> flag</p>\n<p>since simple-app Service was not changed in any way, it was not \"touched\" during the apply changes phase at all</p>\n<p>kapp waited for Pods associated with a Deployment to converge to their ready state before exiting successfully</p>\n<p>To double check that our change applied, go ahead and refresh your browser window with our deployed application.</p>\n<p>Given that kapp does not care where application configuration comes from, one can use it with any other tools that produce Kubernetes configuration, for example, Helm's template command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm template my-chart <span class=\"token parameter variable\">--values</span> values.yml <span class=\"token operator\">|</span> kapp deploy <span class=\"token parameter variable\">-a</span> my-app -f- <span class=\"token parameter variable\">--yes</span></code></pre></div>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<p><br><br></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time, „Å§„Å•„Åè üáµüá∏üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/finops-mayfly-kubernetes-ephemeral-resources/","title":"FinOps Best Practices: Using Mayfly Kubernetes Operator for Ephemeral Resources","date":"2024-10-25 15:34:00"},"excerpt":"Optimizing Resources Spend with the Mayfly Kubernetes Operator üìñ Introduction In the world of cloud computing, managing costs can be a‚Ä¶","html":"<blockquote>\n<p><strong>Optimizing Resources Spend with the Mayfly Kubernetes Operator</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìñ Introduction</h2>\n<p>In the world of cloud computing, managing costs can be a challenge. One way to optimize your cloud spend is to use ephemeral resources that automatically expire after a certain period of time. Kubernetes is a popular container orchestration platform that can help manage these resources.</p>\n<p>In this blog post, we will explore the <a href=\"https://github.com/NCCloud/mayfly\" target=\"_blank\" rel=\"noopener noreferrer\">Mayfly Kubernetes operator</a>, which enables you to create temporary resources on the cluster that will expire after a certain period of time. We will discuss how to use Mayfly to implement FinOps best practices and optimize your cloud spend.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFAv/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAEbc2RSwR//xAAbEAACAgMBAAAAAAAAAAAAAAABAwACBBETFP/aAAgBAQABBQI5DNtZcD0XnBUK6GcVz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABsQAAICAwEAAAAAAAAAAAAAAAABAiERMTIS/9oACAEBAAY/AsemyLUna0XJnCLjo4R//8QAGxABAAICAwAAAAAAAAAAAAAAAQAREEFRgcH/2gAIAQEAAT8hAxQ3LDQV9TgPWDUEFFGM/9oADAMBAAIAAwAAABA7P//EABYRAAMAAAAAAAAAAAAAAAAAAAEQYf/aAAgBAwEBPxAVf//EABcRAAMBAAAAAAAAAAAAAAAAAAABEUH/2gAIAQIBAT8QcxEP/8QAHBAAAgICAwAAAAAAAAAAAAAAAREAIUFRMZHx/9oACAEBAAE/EAvwbUB4axCYyEAIIkv1UXymg0Jm7E61A6YE0BiefP/Z'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"mayfly\" title=\"\" src=\"/static/39bcfe1514a7edad2ce14f2345af81c6/7bf67/mayfly.jpg\" srcset=\"/static/39bcfe1514a7edad2ce14f2345af81c6/651be/mayfly.jpg 170w,\n/static/39bcfe1514a7edad2ce14f2345af81c6/d30a3/mayfly.jpg 340w,\n/static/39bcfe1514a7edad2ce14f2345af81c6/7bf67/mayfly.jpg 680w,\n/static/39bcfe1514a7edad2ce14f2345af81c6/990cb/mayfly.jpg 1020w,\n/static/39bcfe1514a7edad2ce14f2345af81c6/eea4a/mayfly.jpg 1280w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-introduction-to-mayfly\" style=\"position:relative;\"><a href=\"#-introduction-to-mayfly\" aria-label=\" introduction to mayfly permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìÑ Introduction to Mayfly</h2>\n<p>The <strong>Mayfly Operator</strong> is a tool that allows you to manage resources on your cluster for a temporary time. It works by deleting the resources from the cluster based on a Mayfly expiration annotation that you set to specify how long the resource should remain active. This can be useful for creating temporary resources, managing temporary accesses, or simply keeping your cluster organized and tidy. The Mayfly Operator helps in resource management by automatically cleaning up resources after a specified period, reducing clutter and improving efficiency.</p>\n<p>The Mayfly Operator is a valuable tool for managing resources in a Kubernetes cluster. By setting a Mayfly expiration annotation, you can ensure that resources are automatically deleted after a specified period, helping to keep your cluster organized and efficient. This can be useful for managing temporary resources, such as test environments or short-lived jobs, and can also help to ensure that resources are not left running unnecessarily, reducing costs and improving security.</p>\n<h2 id=\"-configuration\" style=\"position:relative;\"><a href=\"#-configuration\" aria-label=\" configuration permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ† Configuration</h2>\n<p>Mayfly is an easy-to-use and configurable project that uses resource watches and schedulers to delete your resources at the appropriate time. It is simple to set up and customize. To specify which resources should be monitored and cleaned up, you can set the <code class=\"language-text\">RESOURCES</code> environment variable to a comma-separated list of <code class=\"language-text\">{ApiVersion};{Kind}</code> as text. This allows you to customize which resources are targeted for cleanup.</p>\n<p><strong>Example:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">RESOURCES</span><span class=\"token operator\">=</span><span class=\"token string\">\"v1;Secret,test.com/v1alpha;MyCRD\"</span></code></pre></div>\n<h2 id=\"-usage\" style=\"position:relative;\"><a href=\"#-usage\" aria-label=\" usage permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Usage</h2>\n<p>Once you have determined which resources you want Mayfly to monitor, you can set the <code class=\"language-text\">mayfly.cloud.namecheap.com/expire</code> annotation on those resources with a duration value. This will cause Mayfly to delete the resources once the specified duration has passed, based on the time of their creation. Keep in mind that the expiration will be calculated based on the creation time of the resource.</p>\n<p><strong>Example:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>pod\n    <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">mayfly.cloud.namecheap.com/expire</span><span class=\"token punctuation\">:</span> 30s\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> alpine\n            <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> alpine\n            <span class=\"token key atrule\">command</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> sleep\n                <span class=\"token punctuation\">-</span> infinity</code></pre></div>\n<h2 id=\"Ô∏è-deployment\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-deployment\" aria-label=\"Ô∏è deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ≥Ô∏è Deployment</h2>\n<p>The easiest and most recommended way to deploy the Mayfly operator to your Kubernetes cluster is by using the Helm chart. To do this, you will need to add our Helm repository and install it from there, providing the <code class=\"language-text\">RESOURCES</code> environment variable as needed. If you prefer, you can also compile the operator and install it using any method you choose.</p>\n<p><strong>Example:</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">helm repo <span class=\"token function\">add</span> nccloud https://nccloud.github.io/charts\nhelm <span class=\"token function\">install</span> mayfly nccloud/mayfly <span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">RESOURCES</span><span class=\"token operator\">=</span><span class=\"token string\">\"v1;Secret\"</span> <span class=\"token comment\"># For only secrets</span></code></pre></div>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Summary</h2>\n<p>In conclusion, the <strong>Mayfly Kubernetes Operator</strong> is a valuable tool for managing ephemeral resources in a Kubernetes cluster. It allows you to create temporary resources, accesses, or maintain a clean and organized cluster by automatically deleting resources after a specified period. Key points about the Mayfly Operator include:</p>\n<ul>\n<li>Mayfly enables you to create temporary resources on the cluster that will expire after a certain period of time.</li>\n<li>It helps in resource management by automatically cleaning up resources after a specified period, reducing clutter and improving efficiency.</li>\n<li>The Mayfly Operator is easy to set up and customize, with the ability to specify which resources should be monitored and cleaned up using the <code class=\"language-text\">RESOURCES</code> environment variable.</li>\n<li>To deploy the Mayfly Operator, you can use the Helm chart, which is the easiest and most recommended method.</li>\n</ul>\n<p>By using the Mayfly Kubernetes Operator, you can ensure that your Kubernetes cluster remains organized, efficient, and secure by automatically managing temporary resources and cleaning up after they are no longer needed.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}