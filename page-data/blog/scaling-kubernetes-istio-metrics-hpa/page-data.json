{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/scaling-kubernetes-istio-metrics-hpa/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Autoscaling for Istio-Powered Kubernetes Applications</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>The need for effective autoscaling is very important in Kubernetes. The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a great tool that allows you to automatically scale your application deployments based on various metrics.</p>\n<p>But what if you're using <a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a>, the popular service mesh for Kubernetes? Can you leverage Istio's rich set of metrics to drive your autoscaling decisions?</p>\n<p>The answer is yes üéâ, and in this blog post, we'll explore how to configure the HPA to scale your workloads based on Istio metrics.</p>\n<h2 id=\"-istio-and-service-mesh\" style=\"position:relative;\"><a href=\"#-istio-and-service-mesh\" aria-label=\" istio and service mesh permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Istio and Service Mesh</h2>\n<p><a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a> is a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes. At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.</p>\n<p>One of Istio's key capabilities is its rich telemetry and observability features. Istio automatically collects a wide range of metrics, logs, and traces for all the traffic flowing through the service mesh.</p>\n<p>This telemetry data can be used for various purposes, including monitoring the health and performance of individual services, identifying performance bottlenecks, and troubleshooting issues.</p>\n<h2 id=\"-horizontal-pod-autoscaler-hpa\" style=\"position:relative;\"><a href=\"#-horizontal-pod-autoscaler-hpa\" aria-label=\" horizontal pod autoscaler hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìà Horizontal Pod Autoscaler (HPA)</h2>\n<p>The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a built-in feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. The HPA periodically checks the target metric, and if the value exceeds or falls below the specified thresholds, it will scale the number of pods up or down accordingly.</p>\n<p>When running Istio-powered applications on Kubernetes, the HPA can be configured to scale based on the rich metrics provided by Istio. For example, the HPA can monitor the incoming HTTP traffic to a service and scale the number of pods to handle the load.</p>\n<p>This allows for dynamic and efficient scaling of Istio-based microservices, ensuring that the application can handle fluctuations in traffic without over-provisioning resources.</p>\n<h2 id=\"-istio-metrics-and-the-hpa\" style=\"position:relative;\"><a href=\"#-istio-metrics-and-the-hpa\" aria-label=\" istio metrics and the hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìä Istio Metrics and the HPA</h2>\n<p>To integrate autoscaling into an Istio-powered Kubernetes application, you need to configure the Kubernetes Horizontal Pod Autoscaler (HPA) to monitor the metrics provided by Istio.</p>\n<p>Istio generates detailed metrics about the application's traffic, which can then be used as the basis for scaling decisions. One solution for using Istio metrics with the HPA is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, a general-purpose metrics adapter developed by Zalando.</p>\n<p>The Kube Metrics Adapter can collect and serve custom and external metrics, including Prometheus metrics generated by Istio, for the HPA to use. The Kube Metrics Adapter works by discovering HPA resources in the cluster and then collecting the requested metrics, storing them in memory. It supports configuring the metric collection via annotations on the HPA object.</p>\n<p>This allows you to specify the Prometheus queries to use for retrieving the relevant Istio metrics, such as requests per second. Istio's built-in monitoring capabilities are a key advantage of using a service mesh. Istio automatically collects valuable metrics like HTTP request rates, response status codes, and request durations directly from the Envoy sidecar proxies, without requiring any changes to your application code.</p>\n<p>This rich telemetry data provides deep visibility into your microservices' performance and behavior. Beyond monitoring, these Istio-generated metrics can be leveraged to drive advanced operational capabilities, such as autoscaling and canary deployments, without additional instrumentation.</p>\n<p>I found a very clear diagram in a <a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\" target=\"_blank\" rel=\"noopener noreferrer\">Medium article by Stefan Prodan</a>:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 51.17647058823529%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhUlEQVR42oWSy07zMBCF+/4bnoM3YMfiZwMSVEVQQaGF5uI0SRPnYse5ODn/2DQhXQAjWZGOZr45OfYCtgbcb0rsfAHmOUjSFF2nSe/x4kg8bgU89wOMBWjbzuobV+Jhk6MsUiRJiqZpLGkxAlc0tPUKMN9BHB8JaAY1DQo87Uq4zgd830dtBzXePIHlhiM6MARBgLqu50DDHND3/Qk0L6PrSR+ob6x5/6hPDq+XGe7XIdz9Fow2tqfGy90rLlZ3cN/e8bReoxTC6kuK6N8qQxx65JxRFO25Q3ZsEKcKUpRQStntpnayxDrnyCmnlHNora0eZw3N1GjqClJWk7748mfst9CdQlVVdtsIHIocmqcQpJucRv2nssCOAI7LcHXLEYRHuk0fSkrbUCzvcLy9gRtFcPb7KXwT2SzOc6BxxLMcz581hDQu5Szsnn6nOz2j80v50aF5Q4LCHvrGOjBHKfNV4HkFFhU4HAI64fTefgWaQL8h36dtaoRJhT0rkfEEURz/CfwPrUAFezQgX+8AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"istio hpa\" title=\"\" src=\"/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png\" srcset=\"/static/2688b10269743da00b07b1f0e9204486/04472/istio-hpa.png 170w,\n/static/2688b10269743da00b07b1f0e9204486/9f933/istio-hpa.png 340w,\n/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png 680w,\n/static/2688b10269743da00b07b1f0e9204486/b12f7/istio-hpa.png 1020w,\n/static/2688b10269743da00b07b1f0e9204486/b5a09/istio-hpa.png 1360w,\n/static/2688b10269743da00b07b1f0e9204486/29007/istio-hpa.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\">source</a></div>\n<h2 id=\"Ô∏è-configuring-hpa-with-istio-metrics\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-configuring-hpa-with-istio-metrics\" aria-label=\"Ô∏è configuring hpa with istio metrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è Configuring HPA with Istio Metrics</h2>\n<p>To configure the Kubernetes Horizontal Pod Autoscaler (HPA) to use metrics provided by Istio, follow these steps:</p>\n<ol>\n<li><strong>Enable Telemetry and Prometheus</strong>: Ensure that when installing Istio, the telemetry service and Prometheus are both enabled.</li>\n<li><strong>Install Metrics Adapter</strong>: You'll need a metrics adapter that can query Prometheus and make the Istio metrics available to the HPA. One such adapter is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, developed by Zalando.</li>\n<li><strong>Adapter Functionality</strong>: The Kube Metrics Adapter scans the HPA objects in your cluster, executes the specified Prometheus queries (configured via annotations), and stores the metrics in memory. This allows the HPA to use the rich telemetry data provided by Istio, such as HTTP request rates, as the basis for scaling your application's pods up or down as needed.</li>\n</ol>\n<h3 id=\"Ô∏è-installing-the-custom-metrics-adapter\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-installing-the-custom-metrics-adapter\" aria-label=\"Ô∏è installing the custom metrics adapter permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Installing the Custom Metrics Adapter</h3>\n<p>For our solution, we will need <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/values.yaml#L167\" target=\"_blank\" rel=\"noopener noreferrer\">telemetry to be enabled in Istio</a> and Prometheus to scrape metrics from Istio:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo <span class=\"token function\">add</span> stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo update\nhelm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus prometheus-community/prometheus</code></pre></div>\n<p>It is the default Helm chart for the Prometheus installation. We use this one because Istio has a default configuration to expose metrics for it, i.e., the pod has <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/files/waypoint.yaml#L56\" target=\"_blank\" rel=\"noopener noreferrer\">the following annotations</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus.io/path</span><span class=\"token punctuation\">:</span> /stats/prometheus\n<span class=\"token key atrule\">prometheus.io/port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">15020</span>\n<span class=\"token key atrule\">prometheus.io/scrape</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>Simply having Prometheus installed in your Kubernetes cluster does not automatically make its metrics available for use with the Horizontal Pod Autoscaler (HPA). To leverage Istio's metrics for autoscaling, you'll need to set up the Prometheus Adapter. The Prometheus Adapter is a component that translates Prometheus metrics into a format that the HPA can understand and use. You'll need to provide a custom configuration file, <code class=\"language-text\">prometheus-adapter.yaml</code>, with the following settings:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> http<span class=\"token punctuation\">:</span>//prometheus<span class=\"token punctuation\">-</span>server.monitoring.svc.cluster.local\n  <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n<span class=\"token key atrule\">rules</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">custom</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">seriesQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'istio_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'</span>\n    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">overrides</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">kubernetes_namespace</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"namespace\"</span><span class=\"token punctuation\">}</span>\n        <span class=\"token key atrule\">kubernetes_pod_name</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"pod\"</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">matches</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"^(.*)_total\"</span>\n      <span class=\"token key atrule\">as</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"${1}_per_second\"</span>\n    <span class=\"token key atrule\">metricsQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'sum(rate(&lt;&lt;.Series>>{&lt;&lt;.LabelMatchers>>}[2m]))'</span></code></pre></div>\n<p>This configuration tells the Prometheus Adapter to:</p>\n<ul>\n<li>Connect to the Prometheus server running in the monitoring namespace.</li>\n<li>Query the <code class=\"language-text\">istio_requests_total</code> metric, which provides the total number of requests.</li>\n<li>Convert the metric to a rate (requests per second) using a 2-minute window.</li>\n<li>Expose the metric with a name like <code class=\"language-text\">istio_requests_per_second</code>.</li>\n</ul>\n<p>With the Prometheus Adapter configured and deployed, the HPA will now be able to use the Istio-generated metrics to scale your application's pods as needed. Here, we can see our Prometheus instance URL, port, and one custom rule. Let's focus on this rule:</p>\n<ul>\n<li><strong>seriesQuery</strong>: Needed for metric discovery.</li>\n<li><strong>resources/overrides</strong>: Mapping fields from the metric (<code class=\"language-text\">kubernetes_namespace</code>, <code class=\"language-text\">kubernetes_pod_name</code>) to the names required by Kubernetes (<code class=\"language-text\">namespace</code>, <code class=\"language-text\">pod</code>).</li>\n<li><strong>name/matches, name/as</strong>: Needed to change the metric name. We are transforming this metric, so it is good to change the name <code class=\"language-text\">istio_requests_total</code> to <code class=\"language-text\">istio_requests_per_second</code>.</li>\n<li><strong>metricsQuery</strong>: The actual query (which is actually a query template) and it will be run by the adapter while scraping the metric from Prometheus. <code class=\"language-text\">rate</code> and <code class=\"language-text\">[2m]</code> \"calculates the per-second average rate of increase of the time series in the range vector\" (from Prometheus documentation), here it is the per-second rate of HTTP requests as measured over the last 2 minutes, per time series in the range vector (also, almost from the Prometheus documentation).</li>\n</ul>\n<p>Now, as we have the adapter configuration, we can deploy it using:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus-adapter prometheus-community/prometheus-adapter <span class=\"token parameter variable\">-f</span> prometheus-adapter.yaml</code></pre></div>\n<h3 id=\"-installing-the-test-app\" style=\"position:relative;\"><a href=\"#-installing-the-test-app\" aria-label=\" installing the test app permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üß™ Installing the Test App</h3>\n<p>You will use <a href=\"https://github.com/stefanprodan/podinfo\" target=\"_blank\" rel=\"noopener noreferrer\">podinfo</a> to test the HPA.</p>\n<p>First, create a test namespace with Istio sidecar injection enabled:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/test.yaml</code></pre></div>\n<p>Create the deployment and ClusterIP service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/deployment.yaml,./demo-app/service.yaml</code></pre></div>\n<p>In order to trigger the autoscaling, you'll need a tool to generate traffic. Deploy the load test service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./loadtest/</code></pre></div>\n<p>Verify the install by calling the app API. Exec into the load tester pod and use <code class=\"language-text\">hey</code> to generate load for a couple of seconds:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">loadtester</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get pod <span class=\"token parameter variable\">-l</span> <span class=\"token string\">\"app=loadtester\"</span> <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">'{.items[0].metadata.name}'</span><span class=\"token variable\">)</span></span>\nkubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n\n$ hey <span class=\"token parameter variable\">-z</span> 5s <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898\n\nSummary:\n  Total:\t<span class=\"token number\">5.0138</span> secs\n  Requests/sec:\t<span class=\"token number\">19.9451</span>\n\nStatus code distribution:\n  <span class=\"token punctuation\">[</span><span class=\"token number\">200</span><span class=\"token punctuation\">]</span>\t<span class=\"token number\">100</span> responses\n$ <span class=\"token builtin class-name\">exit</span></code></pre></div>\n<p>The app ClusterIP service exposes port 9898 under the <code class=\"language-text\">http</code> name. When using the <code class=\"language-text\">http</code> prefix, the Envoy sidecar will switch to L7 routing and the telemetry service will collect HTTP metrics.</p>\n<h3 id=\"querying-the-istiometrics\" style=\"position:relative;\"><a href=\"#querying-the-istiometrics\" aria-label=\"querying the istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Querying the Istio¬†metrics</h3>\n<p>The Istio telemetry service collects metrics from the mesh and stores them in Prometheus. One such metric is istio_requests_total, with it you can determine the rate of requests per second a workload receives.\nThis is how you can query Prometheus for the req/sec rate received by podinfo in the last minute, excluding 404s:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"<span class=\"token punctuation\">,</span>\n        reporter=\"destination\"<span class=\"token punctuation\">,</span>\n        response_code<span class=\"token tag\">!=</span>\"404\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  )</code></pre></div>\n<p>The HPA needs to know the req/sec that each pod receives. You can use the container memory usage metric from kubelet to count the number of pods and calculate the Istio request rate per pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  ) /\n  count(\n    count(\n      container_memory_usage_bytes<span class=\"token punctuation\">{</span>\n        namespace=\"test\"<span class=\"token punctuation\">,</span>\n        pod=~\"podinfo.<span class=\"token important\">*\"</span>\n      <span class=\"token punctuation\">}</span>\n    ) by (pod)\n  )</code></pre></div>\n<h3 id=\"configuring-the-hpa-with-istiometrics\" style=\"position:relative;\"><a href=\"#configuring-the-hpa-with-istiometrics\" aria-label=\"configuring the hpa with istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Configuring the HPA with Istio¬†metrics</h3>\n<p>Using the req/sec query you can define a HPA that will scale the podinfo workload based on the number of requests per second that each instance receives:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> autoscaling/v2\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> HorizontalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> test\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/per-replica</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"true\"</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/query</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n      sum(\n        rate(\n          istio_requests_total{\n            destination_workload=\"podinfo\",\n            destination_workload_namespace=\"test\"\n          }[1m]\n        )\n      ) /\n      count(\n        count(\n          container_memory_usage_bytes{\n            namespace=\"test\",\n            pod=~\"podinfo.*\"\n          }\n        ) by (pod)\n      )</span>\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">maxReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">minReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n    <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">metrics</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Object\n      <span class=\"token key atrule\">object</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metricName</span><span class=\"token punctuation\">:</span> istio<span class=\"token punctuation\">-</span>requests<span class=\"token punctuation\">-</span>total\n        <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n          <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n          <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n        <span class=\"token key atrule\">targetValue</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<p>The above configuration will instruct the Horizontal Pod Autoscaler to scale up the deployment when the average traffic load goes over 10 req/sec per replica.</p>\n<p>Create the HPA with:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/hpa.yaml</code></pre></div>\n<p>Start a load test and verify that the adapter computes the metric:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> kube-system logs deployment/kube-metrics-adapter <span class=\"token parameter variable\">-f</span>\nCollected <span class=\"token number\">1</span> new metric<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\nCollected new custom metric <span class=\"token string\">'istio-requests-total'</span> <span class=\"token punctuation\">(</span>44m<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> Pod test/podinfo</code></pre></div>\n<p>List the custom metrics resources:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> <span class=\"token string\">\"/apis/custom.metrics.k8s.io/v1beta1\"</span> <span class=\"token operator\">|</span> jq <span class=\"token builtin class-name\">.</span>\nThe Kubernetes API should <span class=\"token builtin class-name\">return</span> a resource list containing the Istio metric:\n<span class=\"token punctuation\">{</span>\n  <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"APIResourceList\"</span>,\n  <span class=\"token string\">\"apiVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"v1\"</span>,\n  <span class=\"token string\">\"groupVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"custom.metrics.k8s.io/v1beta1\"</span>,\n  <span class=\"token string\">\"resources\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"pods/istio-requests-total\"</span>,\n      <span class=\"token string\">\"singularName\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"\"</span>,\n      <span class=\"token string\">\"namespaced\"</span><span class=\"token builtin class-name\">:</span> true,\n      <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"MetricValueList\"</span>,\n      <span class=\"token string\">\"verbs\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token string\">\"get\"</span>\n      <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>After a couple of seconds the HPA will fetch the metric from the adapter:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   44m/10    <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">1</span></code></pre></div>\n<h3 id=\"autoscaling-based-on-http-traffic\" style=\"position:relative;\"><a href=\"#autoscaling-based-on-http-traffic\" aria-label=\"autoscaling based on http traffic permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Autoscaling Based on HTTP Traffic</h3>\n<p>To test the HPA, you can use the load tester to trigger a scale-up event. You can use other tools besides <code class=\"language-text\">hey</code> (e.g., <code class=\"language-text\">siege</code>). It is important that the tool supports HTTP/1.1, so <code class=\"language-text\">ab</code> (Apache Benchmark) is not the right solution.</p>\n<p>Exec into the tester pod and use <code class=\"language-text\">hey</code> to generate load for 5 minutes:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n$ hey <span class=\"token parameter variable\">-z</span> 5m <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898</code></pre></div>\n<p>Press <code class=\"language-text\">Ctrl+C</code> then <code class=\"language-text\">exit</code> to get out of the load test terminal if you want to stop prematurely. After a minute, the HPA will start to scale up the workload until the requests per second per pod drop under the target value:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">watch</span> kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   25272m/10   <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">3</span></code></pre></div>\n<p>When the load test finishes, the number of requests per second will drop to zero, and the HPA will start to scale down the workload. Note that the HPA has a back-off mechanism that prevents rapid scale-up/down events; the number of replicas will go back to one after a couple of minutes.</p>\n<p>By default, the metrics sync happens once every 30 seconds, and scaling up/down can only happen if there was no rescaling within the last 3‚Äì5 minutes.</p>\n<p>In this way, the HPA prevents rapid execution of conflicting decisions and gives time for the <a href=\"https://github.com/kubernetes/autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Cluster Autoscaler</a> to kick in.</p>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìã Summary</h2>\n<p>In this article, we explored the Kubernetes Horizontal Pod Autoscaler (HPA) using Custom Metrics. We set up the Prometheus Adapter to expose Istio metrics to Kubernetes' Custom Metrics API. However, the workload we tested did not respond effectively to the configured HPA parameters. A less erratic workload should exhibit more predictable behavior with the HPA settings.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":9,"rawMarkdownBody":"\n> **Autoscaling for Istio-Powered Kubernetes Applications**\n\n## üìö Introduction\n\nThe need for effective autoscaling is very important in Kubernetes. The [Kubernetes Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) is a great tool that allows you to automatically scale your application deployments based on various metrics.\n\nBut what if you're using [Istio](https://istio.io/), the popular service mesh for Kubernetes? Can you leverage Istio's rich set of metrics to drive your autoscaling decisions?\n\nThe answer is yes üéâ, and in this blog post, we'll explore how to configure the HPA to scale your workloads based on Istio metrics.\n\n## üöÄ Istio and Service Mesh\n\n[Istio](https://istio.io/) is a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes. At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.\n\nOne of Istio's key capabilities is its rich telemetry and observability features. Istio automatically collects a wide range of metrics, logs, and traces for all the traffic flowing through the service mesh.\n\nThis telemetry data can be used for various purposes, including monitoring the health and performance of individual services, identifying performance bottlenecks, and troubleshooting issues.\n\n## üìà Horizontal Pod Autoscaler (HPA)\n\nThe [Kubernetes Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) is a built-in feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. The HPA periodically checks the target metric, and if the value exceeds or falls below the specified thresholds, it will scale the number of pods up or down accordingly.\n\nWhen running Istio-powered applications on Kubernetes, the HPA can be configured to scale based on the rich metrics provided by Istio. For example, the HPA can monitor the incoming HTTP traffic to a service and scale the number of pods to handle the load.\n\nThis allows for dynamic and efficient scaling of Istio-based microservices, ensuring that the application can handle fluctuations in traffic without over-provisioning resources.\n\n## üìä Istio Metrics and the HPA\n\nTo integrate autoscaling into an Istio-powered Kubernetes application, you need to configure the Kubernetes Horizontal Pod Autoscaler (HPA) to monitor the metrics provided by Istio.\n\nIstio generates detailed metrics about the application's traffic, which can then be used as the basis for scaling decisions. One solution for using Istio metrics with the HPA is the [Kube Metrics Adapter](https://github.com/zalando-incubator/kube-metrics-adapter), a general-purpose metrics adapter developed by Zalando.\n\nThe Kube Metrics Adapter can collect and serve custom and external metrics, including Prometheus metrics generated by Istio, for the HPA to use. The Kube Metrics Adapter works by discovering HPA resources in the cluster and then collecting the requested metrics, storing them in memory. It supports configuring the metric collection via annotations on the HPA object.\n\nThis allows you to specify the Prometheus queries to use for retrieving the relevant Istio metrics, such as requests per second. Istio's built-in monitoring capabilities are a key advantage of using a service mesh. Istio automatically collects valuable metrics like HTTP request rates, response status codes, and request durations directly from the Envoy sidecar proxies, without requiring any changes to your application code.\n\nThis rich telemetry data provides deep visibility into your microservices' performance and behavior. Beyond monitoring, these Istio-generated metrics can be leveraged to drive advanced operational capabilities, such as autoscaling and canary deployments, without additional instrumentation.\n\nI found a very clear diagram in a [Medium article by Stefan Prodan](https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a):\n\n![istio hpa](./istio-hpa.png)\n<div class=\"image-title\"><a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\">source</a></div>\n\n## ‚öôÔ∏è Configuring HPA with Istio Metrics\n\nTo configure the Kubernetes Horizontal Pod Autoscaler (HPA) to use metrics provided by Istio, follow these steps:\n\n1. **Enable Telemetry and Prometheus**: Ensure that when installing Istio, the telemetry service and Prometheus are both enabled.\n2. **Install Metrics Adapter**: You'll need a metrics adapter that can query Prometheus and make the Istio metrics available to the HPA. One such adapter is the [Kube Metrics Adapter](https://github.com/zalando-incubator/kube-metrics-adapter), developed by Zalando.\n3. **Adapter Functionality**: The Kube Metrics Adapter scans the HPA objects in your cluster, executes the specified Prometheus queries (configured via annotations), and stores the metrics in memory. This allows the HPA to use the rich telemetry data provided by Istio, such as HTTP request rates, as the basis for scaling your application's pods up or down as needed.\n\n### üõ†Ô∏è Installing the Custom Metrics Adapter\n\nFor our solution, we will need [telemetry to be enabled in Istio](https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/values.yaml#L167) and Prometheus to scrape metrics from Istio:\n\n```shell\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo update\nhelm -n monitoring install prometheus prometheus-community/prometheus\n```\n\nIt is the default Helm chart for the Prometheus installation. We use this one because Istio has a default configuration to expose metrics for it, i.e., the pod has [the following annotations](https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/files/waypoint.yaml#L56):\n\n```yaml\nprometheus.io/path: /stats/prometheus\nprometheus.io/port: 15020\nprometheus.io/scrape: true\n```\n\nSimply having Prometheus installed in your Kubernetes cluster does not automatically make its metrics available for use with the Horizontal Pod Autoscaler (HPA). To leverage Istio's metrics for autoscaling, you'll need to set up the Prometheus Adapter. The Prometheus Adapter is a component that translates Prometheus metrics into a format that the HPA can understand and use. You'll need to provide a custom configuration file, `prometheus-adapter.yaml`, with the following settings:\n\n```yaml\nprometheus:\n  url: http://prometheus-server.monitoring.svc.cluster.local\n  port: 80\nrules:\n  custom:\n  - seriesQuery: 'istio_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'\n    resources:\n      overrides:\n        kubernetes_namespace: {resource: \"namespace\"}\n        kubernetes_pod_name: {resource: \"pod\"}\n    name:\n      matches: \"^(.*)_total\"\n      as: \"${1}_per_second\"\n    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m]))'\n```\n\nThis configuration tells the Prometheus Adapter to:\n\n- Connect to the Prometheus server running in the monitoring namespace.\n- Query the `istio_requests_total` metric, which provides the total number of requests.\n- Convert the metric to a rate (requests per second) using a 2-minute window.\n- Expose the metric with a name like `istio_requests_per_second`.\n\nWith the Prometheus Adapter configured and deployed, the HPA will now be able to use the Istio-generated metrics to scale your application's pods as needed. Here, we can see our Prometheus instance URL, port, and one custom rule. Let's focus on this rule:\n\n- **seriesQuery**: Needed for metric discovery.\n- **resources/overrides**: Mapping fields from the metric (`kubernetes_namespace`, `kubernetes_pod_name`) to the names required by Kubernetes (`namespace`, `pod`).\n- **name/matches, name/as**: Needed to change the metric name. We are transforming this metric, so it is good to change the name `istio_requests_total` to `istio_requests_per_second`.\n- **metricsQuery**: The actual query (which is actually a query template) and it will be run by the adapter while scraping the metric from Prometheus. `rate` and `[2m]` \"calculates the per-second average rate of increase of the time series in the range vector\" (from Prometheus documentation), here it is the per-second rate of HTTP requests as measured over the last 2 minutes, per time series in the range vector (also, almost from the Prometheus documentation).\n\nNow, as we have the adapter configuration, we can deploy it using:\n\n```shell\nhelm -n monitoring install prometheus-adapter prometheus-community/prometheus-adapter -f prometheus-adapter.yaml\n```\n\n### üß™ Installing the Test App\n\nYou will use [podinfo](https://github.com/stefanprodan/podinfo) to test the HPA.\n\nFirst, create a test namespace with Istio sidecar injection enabled:\n\n```shell\nkubectl apply -f ./demo-app/test.yaml\n```\n\nCreate the deployment and ClusterIP service in the test namespace:\n\n```shell\nkubectl apply -f ./demo-app/deployment.yaml,./demo-app/service.yaml\n```\n\nIn order to trigger the autoscaling, you'll need a tool to generate traffic. Deploy the load test service in the test namespace:\n\n```shell\nkubectl apply -f ./loadtest/\n```\n\nVerify the install by calling the app API. Exec into the load tester pod and use `hey` to generate load for a couple of seconds:\n\n```shell\nexport loadtester=$(kubectl -n test get pod -l \"app=loadtester\" -o jsonpath='{.items[0].metadata.name}')\nkubectl -n test exec -it ${loadtester} -- sh\n\n$ hey -z 5s -c 10 -q 2 http://podinfo.test:9898\n\nSummary:\n  Total:\t5.0138 secs\n  Requests/sec:\t19.9451\n\nStatus code distribution:\n  [200]\t100 responses\n$ exit\n```\n\nThe app ClusterIP service exposes port 9898 under the `http` name. When using the `http` prefix, the Envoy sidecar will switch to L7 routing and the telemetry service will collect HTTP metrics.\n\n### Querying the Istio¬†metrics\n\nThe Istio telemetry service collects metrics from the mesh and stores them in Prometheus. One such metric is istio_requests_total, with it you can determine the rate of requests per second a workload receives.\nThis is how you can query Prometheus for the req/sec rate received by podinfo in the last minute, excluding 404s:\n\n```yaml\nsum(\n    rate(\n      istio_requests_total{\n        destination_workload=\"podinfo\",\n        destination_workload_namespace=\"test\",\n        reporter=\"destination\",\n        response_code!=\"404\"\n      }[1m]\n    )\n  )\n```\n\nThe HPA needs to know the req/sec that each pod receives. You can use the container memory usage metric from kubelet to count the number of pods and calculate the Istio request rate per pod:\n\n```yaml\nsum(\n    rate(\n      istio_requests_total{\n        destination_workload=\"podinfo\",\n        destination_workload_namespace=\"test\"\n      }[1m]\n    )\n  ) /\n  count(\n    count(\n      container_memory_usage_bytes{\n        namespace=\"test\",\n        pod=~\"podinfo.*\"\n      }\n    ) by (pod)\n  )\n```\n\n### Configuring the HPA with Istio¬†metrics\n\nUsing the req/sec query you can define a HPA that will scale the podinfo workload based on the number of requests per second that each instance receives:\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: podinfo\n  namespace: test\n  annotations:\n    metric-config.object.istio-requests-total.prometheus/per-replica: \"true\"\n    metric-config.object.istio-requests-total.prometheus/query: |\n      sum(\n        rate(\n          istio_requests_total{\n            destination_workload=\"podinfo\",\n            destination_workload_namespace=\"test\"\n          }[1m]\n        )\n      ) /\n      count(\n        count(\n          container_memory_usage_bytes{\n            namespace=\"test\",\n            pod=~\"podinfo.*\"\n          }\n        ) by (pod)\n      )\nspec:\n  maxReplicas: 10\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: podinfo\n  metrics:\n    - type: Object\n      object:\n        metricName: istio-requests-total\n        target:\n          apiVersion: v1\n          kind: Pod\n          name: podinfo\n        targetValue: 10\n```\n\nThe above configuration will instruct the Horizontal Pod Autoscaler to scale up the deployment when the average traffic load goes over 10 req/sec per replica.\n\nCreate the HPA with:\n\n```shell\nkubectl apply -f ./demo-app/hpa.yaml\n```\n\nStart a load test and verify that the adapter computes the metric:\n\n```shell\nkubectl -n kube-system logs deployment/kube-metrics-adapter -f\nCollected 1 new metric(s)\nCollected new custom metric 'istio-requests-total' (44m) for Pod test/podinfo\n```\n\nList the custom metrics resources:\n\n```shell\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq .\nThe Kubernetes API should return a resource list containing the Istio metric:\n{\n  \"kind\": \"APIResourceList\",\n  \"apiVersion\": \"v1\",\n  \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"resources\": [\n    {\n      \"name\": \"pods/istio-requests-total\",\n      \"singularName\": \"\",\n      \"namespaced\": true,\n      \"kind\": \"MetricValueList\",\n      \"verbs\": [\n        \"get\"\n      ]\n    }\n  ]\n}\n```\n\nAfter a couple of seconds the HPA will fetch the metric from the adapter:\n\n```shell\nkubectl -n test get hpa/podinfo\nNAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   44m/10    1         10        1\n```\n\n### Autoscaling Based on HTTP Traffic\n\nTo test the HPA, you can use the load tester to trigger a scale-up event. You can use other tools besides `hey` (e.g., `siege`). It is important that the tool supports HTTP/1.1, so `ab` (Apache Benchmark) is not the right solution.\n\nExec into the tester pod and use `hey` to generate load for 5 minutes:\n\n```shell\nkubectl -n test exec -it ${loadtester} -- sh\n$ hey -z 5m -c 10 -q 2 http://podinfo.test:9898\n```\n\nPress `Ctrl+C` then `exit` to get out of the load test terminal if you want to stop prematurely. After a minute, the HPA will start to scale up the workload until the requests per second per pod drop under the target value:\n\n```shell\nwatch kubectl -n test get hpa/podinfo\nNAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   25272m/10   1         10        3\n```\n\nWhen the load test finishes, the number of requests per second will drop to zero, and the HPA will start to scale down the workload. Note that the HPA has a back-off mechanism that prevents rapid scale-up/down events; the number of replicas will go back to one after a couple of minutes.\n\nBy default, the metrics sync happens once every 30 seconds, and scaling up/down can only happen if there was no rescaling within the last 3‚Äì5 minutes.\n\nIn this way, the HPA prevents rapid execution of conflicting decisions and gives time for the [Cluster Autoscaler](https://github.com/kubernetes/autoscaler) to kick in.\n\n## üìã Summary\n\nIn this article, we explored the Kubernetes Horizontal Pod Autoscaler (HPA) using Custom Metrics. We set up the Prometheus Adapter to expose Istio metrics to Kubernetes' Custom Metrics API. However, the workload we tested did not respond effectively to the configured HPA parameters. A less erratic workload should exhibit more predictable behavior with the HPA settings.\n\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1664},"frontmatter":{"id":"de71ab9829cbe751d2164864","path":"/blog/scaling-kubernetes-istio-metrics-hpa/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Scaling Kubernetes Workloads with Istio Metrics and the Horizontal Pod Autoscaler","keywords":["Kubernetes","Istio","Horizontal Pod Autoscaler","Autoscaling","Performance Optimization"],"excerpt":"Learn how to effectively scale Kubernetes workloads using Istio metrics and the Horizontal Pod Autoscaler for optimal performance.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVR42mVTS09TQRSe3gLFDUFRowkIioBGYn1VhYKIgEaiQZDb2xKkhfKQimIRSukTWqgCbVEKWKCCD+JSXRgXRly4MjGu3PgPXOjWjSw+z4ytj7j4ZuacM+c7rxmWb5uGxuQDa3ZBJROak6AzuzwCSU6eSSfJf9lk9y8fxQ1NTwjZA7PYaY+D5V6dxPbOEMrdcZS57qPKt4Tjw/PQOedRPZqAjs56sp3yLuHo0BxqSMflE84F4VPpWUTh9Qjq7zxGni0Mltc7hZyOCTRMPkFd4AGaptYgR56iZiyBKv8yjjjmiHgZZ0g+SQErvYtCPhtYQV1wBefGV0Xgi6FH2E3Vsuz2IDQtfpTa76KIInFob82KfW9fRGRQQThIOl5BuSsO7WAM+2/OoOhGFPv6Z5Bvm8IB+z1spcSYpHggGdxQNTn/oHEYrNEJDdk4IS+Nl84JT1NLNrcFwC45oKIe8/u81/w+7y+TDB6kU2MzFb57kGb0It3oQwYhXfFBTcE4eFA12flApOS9FNSpnfRMMtJiGiOM08T85OwlZz5hykB2kOwSF1MQFSne5P4vBKHqyjiqTTYMGAwoMQ0R8Sg2GcMo6X2LXOtLpClBESCVVVYLD/w/GdeJp3S+rQ8vlFr0tHXhnaxHjnEUFc7PWFvfQPTZBvZ0r9N7GxZOvA2l1gQ0Rr94f78JqW2ZNNhsSxDMYTZjpbWBRh7Be0MZdJTBhdtf8frjd7z68APH7J+ofLcg3WWOodb2HAWWGJhhRJTOy8wyB1BwbRpaemJMOxhBuNMMf2cXPJYe7LDGUNL9Bh0z3yCHvmBL6yr9ECc5elHcHsehrocotCwQoRsZ9MO2WSdQ3B/FYfoI+rEEfgLRw8Uf6wY3YQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/b82cdf0ead47af1ad1802eff29c72c00/9613e/istio-hpa-cover.png","srcSet":"/static/b82cdf0ead47af1ad1802eff29c72c00/23c6c/istio-hpa-cover.png 750w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/d7b18/istio-hpa-cover.png 1080w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/a6044/istio-hpa-cover.png 1366w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/9613e/istio-hpa-cover.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/b82cdf0ead47af1ad1802eff29c72c00/f8e3a/istio-hpa-cover.webp 750w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/2af51/istio-hpa-cover.webp 1080w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/d339b/istio-hpa-cover.webp 1366w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/9b00e/istio-hpa-cover.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.574375}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/improve-k8s-image-caching/","title":"Improve Container Image Availability and Speed with Caching in Kubernetes üï∏","date":"2024-10-25 19:36:00"},"excerpt":"A Guide to Tools and Strategies of image cachingüö¶ üöõ Intro When deploying a containerized application to a Kubernetes cluster, delays can‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/carvel-kapp-kubernetes-management/","title":"Simplify Kubernetes Application Management with Carvel kapp","date":"2024-10-25 18:06:00"},"excerpt":"Take control of your Kubernetes resourcesüî• üìî Introduction Kubernetes applications involve juggling multiple resources, configurations, and‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}