{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/scaling-kubernetes-istio-metrics-hpa/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Autoscaling for Istio-Powered Kubernetes Applications</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>The need for effective autoscaling is very important in Kubernetes. The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a great tool that allows you to automatically scale your application deployments based on various metrics.</p>\n<p>But what if you're using <a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a>, the popular service mesh for Kubernetes? Can you leverage Istio's rich set of metrics to drive your autoscaling decisions?</p>\n<p>The answer is yes üéâ, and in this blog post, we'll explore how to configure the HPA to scale your workloads based on Istio metrics.</p>\n<h2 id=\"-istio-and-service-mesh\" style=\"position:relative;\"><a href=\"#-istio-and-service-mesh\" aria-label=\" istio and service mesh permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Istio and Service Mesh</h2>\n<p><a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a> is a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes. At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.</p>\n<p>One of Istio's key capabilities is its rich telemetry and observability features. Istio automatically collects a wide range of metrics, logs, and traces for all the traffic flowing through the service mesh.</p>\n<p>This telemetry data can be used for various purposes, including monitoring the health and performance of individual services, identifying performance bottlenecks, and troubleshooting issues.</p>\n<h2 id=\"-horizontal-pod-autoscaler-hpa\" style=\"position:relative;\"><a href=\"#-horizontal-pod-autoscaler-hpa\" aria-label=\" horizontal pod autoscaler hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìà Horizontal Pod Autoscaler (HPA)</h2>\n<p>The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a built-in feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. The HPA periodically checks the target metric, and if the value exceeds or falls below the specified thresholds, it will scale the number of pods up or down accordingly.</p>\n<p>When running Istio-powered applications on Kubernetes, the HPA can be configured to scale based on the rich metrics provided by Istio. For example, the HPA can monitor the incoming HTTP traffic to a service and scale the number of pods to handle the load.</p>\n<p>This allows for dynamic and efficient scaling of Istio-based microservices, ensuring that the application can handle fluctuations in traffic without over-provisioning resources.</p>\n<h2 id=\"-istio-metrics-and-the-hpa\" style=\"position:relative;\"><a href=\"#-istio-metrics-and-the-hpa\" aria-label=\" istio metrics and the hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìä Istio Metrics and the HPA</h2>\n<p>To integrate autoscaling into an Istio-powered Kubernetes application, you need to configure the Kubernetes Horizontal Pod Autoscaler (HPA) to monitor the metrics provided by Istio.</p>\n<p>Istio generates detailed metrics about the application's traffic, which can then be used as the basis for scaling decisions. One solution for using Istio metrics with the HPA is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, a general-purpose metrics adapter developed by Zalando.</p>\n<p>The Kube Metrics Adapter can collect and serve custom and external metrics, including Prometheus metrics generated by Istio, for the HPA to use. The Kube Metrics Adapter works by discovering HPA resources in the cluster and then collecting the requested metrics, storing them in memory. It supports configuring the metric collection via annotations on the HPA object.</p>\n<p>This allows you to specify the Prometheus queries to use for retrieving the relevant Istio metrics, such as requests per second. Istio's built-in monitoring capabilities are a key advantage of using a service mesh. Istio automatically collects valuable metrics like HTTP request rates, response status codes, and request durations directly from the Envoy sidecar proxies, without requiring any changes to your application code.</p>\n<p>This rich telemetry data provides deep visibility into your microservices' performance and behavior. Beyond monitoring, these Istio-generated metrics can be leveraged to drive advanced operational capabilities, such as autoscaling and canary deployments, without additional instrumentation.</p>\n<p>I found a very clear diagram in a <a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\" target=\"_blank\" rel=\"noopener noreferrer\">Medium article by Stefan Prodan</a>:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 51.17647058823529%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfUlEQVR42oWS227CMBBE+/9v/Y3+QB8rtQ8VqigtErRACZCLQ0LusZPYJJ7a5i4VutJK0ch7PDvxHUxJ9KcUlkdBXBtxkmC7bZXe4dtmGM4pPGcBQnwIsTX61GF4n+QoiwRxnIBzbkh3B+BwzjB3CxDPwSaKFFAPtmZwZFE49gKeR9SgMPrMZRhMM4QBge/7aJrmHLiDStntQZfVdX/r5+ellJcOnz5y9L9COCtL3biG2B98WExw//kGd/aD0XiMkjKjD2YlXoYZNoFnnAshLh2SiGOT1mC0RK3sa1e6LFZinKcoVK5JmqJtW6NvMq5mGvCmAmPVUT8CZSeUWKOqKrPGASjLHG2agFa1Cf6gXysDFILDdggeeyn8IFJ/00PNdqsV/R7i12c4QQh7tTyGL/d9BSiQZjlGqwaUaZfsFLZy1Lbb/TM6hX/ToV6FUqpOc5VJY1zo1t9ZUYGEBdZrgiAIju/tJlDnogF1fYLpFrxBEFdYkhJpEiEMw3+BvyS7BbM06mUUAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"istio hpa\" title=\"\" src=\"/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png\" srcset=\"/static/2688b10269743da00b07b1f0e9204486/04472/istio-hpa.png 170w,\n/static/2688b10269743da00b07b1f0e9204486/9f933/istio-hpa.png 340w,\n/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png 680w,\n/static/2688b10269743da00b07b1f0e9204486/b12f7/istio-hpa.png 1020w,\n/static/2688b10269743da00b07b1f0e9204486/b5a09/istio-hpa.png 1360w,\n/static/2688b10269743da00b07b1f0e9204486/29007/istio-hpa.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\">source</a></div>\n<h2 id=\"Ô∏è-configuring-hpa-with-istio-metrics\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-configuring-hpa-with-istio-metrics\" aria-label=\"Ô∏è configuring hpa with istio metrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è Configuring HPA with Istio Metrics</h2>\n<p>To configure the Kubernetes Horizontal Pod Autoscaler (HPA) to use metrics provided by Istio, follow these steps:</p>\n<ol>\n<li><strong>Enable Telemetry and Prometheus</strong>: Ensure that when installing Istio, the telemetry service and Prometheus are both enabled.</li>\n<li><strong>Install Metrics Adapter</strong>: You'll need a metrics adapter that can query Prometheus and make the Istio metrics available to the HPA. One such adapter is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, developed by Zalando.</li>\n<li><strong>Adapter Functionality</strong>: The Kube Metrics Adapter scans the HPA objects in your cluster, executes the specified Prometheus queries (configured via annotations), and stores the metrics in memory. This allows the HPA to use the rich telemetry data provided by Istio, such as HTTP request rates, as the basis for scaling your application's pods up or down as needed.</li>\n</ol>\n<h3 id=\"Ô∏è-installing-the-custom-metrics-adapter\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-installing-the-custom-metrics-adapter\" aria-label=\"Ô∏è installing the custom metrics adapter permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Installing the Custom Metrics Adapter</h3>\n<p>For our solution, we will need <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/values.yaml#L167\" target=\"_blank\" rel=\"noopener noreferrer\">telemetry to be enabled in Istio</a> and Prometheus to scrape metrics from Istio:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo <span class=\"token function\">add</span> stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo update\nhelm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus prometheus-community/prometheus</code></pre></div>\n<p>It is the default Helm chart for the Prometheus installation. We use this one because Istio has a default configuration to expose metrics for it, i.e., the pod has <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/files/waypoint.yaml#L56\" target=\"_blank\" rel=\"noopener noreferrer\">the following annotations</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus.io/path</span><span class=\"token punctuation\">:</span> /stats/prometheus\n<span class=\"token key atrule\">prometheus.io/port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">15020</span>\n<span class=\"token key atrule\">prometheus.io/scrape</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>Simply having Prometheus installed in your Kubernetes cluster does not automatically make its metrics available for use with the Horizontal Pod Autoscaler (HPA). To leverage Istio's metrics for autoscaling, you'll need to set up the Prometheus Adapter. The Prometheus Adapter is a component that translates Prometheus metrics into a format that the HPA can understand and use. You'll need to provide a custom configuration file, <code class=\"language-text\">prometheus-adapter.yaml</code>, with the following settings:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> http<span class=\"token punctuation\">:</span>//prometheus<span class=\"token punctuation\">-</span>server.monitoring.svc.cluster.local\n  <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n<span class=\"token key atrule\">rules</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">custom</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">seriesQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'istio_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'</span>\n    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">overrides</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">kubernetes_namespace</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"namespace\"</span><span class=\"token punctuation\">}</span>\n        <span class=\"token key atrule\">kubernetes_pod_name</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"pod\"</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">matches</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"^(.*)_total\"</span>\n      <span class=\"token key atrule\">as</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"${1}_per_second\"</span>\n    <span class=\"token key atrule\">metricsQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'sum(rate(&lt;&lt;.Series>>{&lt;&lt;.LabelMatchers>>}[2m]))'</span></code></pre></div>\n<p>This configuration tells the Prometheus Adapter to:</p>\n<ul>\n<li>Connect to the Prometheus server running in the monitoring namespace.</li>\n<li>Query the <code class=\"language-text\">istio_requests_total</code> metric, which provides the total number of requests.</li>\n<li>Convert the metric to a rate (requests per second) using a 2-minute window.</li>\n<li>Expose the metric with a name like <code class=\"language-text\">istio_requests_per_second</code>.</li>\n</ul>\n<p>With the Prometheus Adapter configured and deployed, the HPA will now be able to use the Istio-generated metrics to scale your application's pods as needed. Here, we can see our Prometheus instance URL, port, and one custom rule. Let's focus on this rule:</p>\n<ul>\n<li><strong>seriesQuery</strong>: Needed for metric discovery.</li>\n<li><strong>resources/overrides</strong>: Mapping fields from the metric (<code class=\"language-text\">kubernetes_namespace</code>, <code class=\"language-text\">kubernetes_pod_name</code>) to the names required by Kubernetes (<code class=\"language-text\">namespace</code>, <code class=\"language-text\">pod</code>).</li>\n<li><strong>name/matches, name/as</strong>: Needed to change the metric name. We are transforming this metric, so it is good to change the name <code class=\"language-text\">istio_requests_total</code> to <code class=\"language-text\">istio_requests_per_second</code>.</li>\n<li><strong>metricsQuery</strong>: The actual query (which is actually a query template) and it will be run by the adapter while scraping the metric from Prometheus. <code class=\"language-text\">rate</code> and <code class=\"language-text\">[2m]</code> \"calculates the per-second average rate of increase of the time series in the range vector\" (from Prometheus documentation), here it is the per-second rate of HTTP requests as measured over the last 2 minutes, per time series in the range vector (also, almost from the Prometheus documentation).</li>\n</ul>\n<p>Now, as we have the adapter configuration, we can deploy it using:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus-adapter prometheus-community/prometheus-adapter <span class=\"token parameter variable\">-f</span> prometheus-adapter.yaml</code></pre></div>\n<h3 id=\"-installing-the-test-app\" style=\"position:relative;\"><a href=\"#-installing-the-test-app\" aria-label=\" installing the test app permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üß™ Installing the Test App</h3>\n<p>You will use <a href=\"https://github.com/stefanprodan/podinfo\" target=\"_blank\" rel=\"noopener noreferrer\">podinfo</a> to test the HPA.</p>\n<p>First, create a test namespace with Istio sidecar injection enabled:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/test.yaml</code></pre></div>\n<p>Create the deployment and ClusterIP service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/deployment.yaml,./demo-app/service.yaml</code></pre></div>\n<p>In order to trigger the autoscaling, you'll need a tool to generate traffic. Deploy the load test service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./loadtest/</code></pre></div>\n<p>Verify the install by calling the app API. Exec into the load tester pod and use <code class=\"language-text\">hey</code> to generate load for a couple of seconds:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">loadtester</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get pod <span class=\"token parameter variable\">-l</span> <span class=\"token string\">\"app=loadtester\"</span> <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">'{.items[0].metadata.name}'</span><span class=\"token variable\">)</span></span>\nkubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n\n$ hey <span class=\"token parameter variable\">-z</span> 5s <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898\n\nSummary:\n  Total:\t<span class=\"token number\">5.0138</span> secs\n  Requests/sec:\t<span class=\"token number\">19.9451</span>\n\nStatus code distribution:\n  <span class=\"token punctuation\">[</span><span class=\"token number\">200</span><span class=\"token punctuation\">]</span>\t<span class=\"token number\">100</span> responses\n$ <span class=\"token builtin class-name\">exit</span></code></pre></div>\n<p>The app ClusterIP service exposes port 9898 under the <code class=\"language-text\">http</code> name. When using the <code class=\"language-text\">http</code> prefix, the Envoy sidecar will switch to L7 routing and the telemetry service will collect HTTP metrics.</p>\n<h3 id=\"querying-the-istiometrics\" style=\"position:relative;\"><a href=\"#querying-the-istiometrics\" aria-label=\"querying the istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Querying the Istio¬†metrics</h3>\n<p>The Istio telemetry service collects metrics from the mesh and stores them in Prometheus. One such metric is istio_requests_total, with it you can determine the rate of requests per second a workload receives.\nThis is how you can query Prometheus for the req/sec rate received by podinfo in the last minute, excluding 404s:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"<span class=\"token punctuation\">,</span>\n        reporter=\"destination\"<span class=\"token punctuation\">,</span>\n        response_code<span class=\"token tag\">!=</span>\"404\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  )</code></pre></div>\n<p>The HPA needs to know the req/sec that each pod receives. You can use the container memory usage metric from kubelet to count the number of pods and calculate the Istio request rate per pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  ) /\n  count(\n    count(\n      container_memory_usage_bytes<span class=\"token punctuation\">{</span>\n        namespace=\"test\"<span class=\"token punctuation\">,</span>\n        pod=~\"podinfo.<span class=\"token important\">*\"</span>\n      <span class=\"token punctuation\">}</span>\n    ) by (pod)\n  )</code></pre></div>\n<h3 id=\"configuring-the-hpa-with-istiometrics\" style=\"position:relative;\"><a href=\"#configuring-the-hpa-with-istiometrics\" aria-label=\"configuring the hpa with istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Configuring the HPA with Istio¬†metrics</h3>\n<p>Using the req/sec query you can define a HPA that will scale the podinfo workload based on the number of requests per second that each instance receives:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> autoscaling/v2\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> HorizontalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> test\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/per-replica</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"true\"</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/query</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n      sum(\n        rate(\n          istio_requests_total{\n            destination_workload=\"podinfo\",\n            destination_workload_namespace=\"test\"\n          }[1m]\n        )\n      ) /\n      count(\n        count(\n          container_memory_usage_bytes{\n            namespace=\"test\",\n            pod=~\"podinfo.*\"\n          }\n        ) by (pod)\n      )</span>\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">maxReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">minReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n    <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">metrics</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Object\n      <span class=\"token key atrule\">object</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metricName</span><span class=\"token punctuation\">:</span> istio<span class=\"token punctuation\">-</span>requests<span class=\"token punctuation\">-</span>total\n        <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n          <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n          <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n        <span class=\"token key atrule\">targetValue</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<p>The above configuration will instruct the Horizontal Pod Autoscaler to scale up the deployment when the average traffic load goes over 10 req/sec per replica.</p>\n<p>Create the HPA with:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/hpa.yaml</code></pre></div>\n<p>Start a load test and verify that the adapter computes the metric:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> kube-system logs deployment/kube-metrics-adapter <span class=\"token parameter variable\">-f</span>\nCollected <span class=\"token number\">1</span> new metric<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\nCollected new custom metric <span class=\"token string\">'istio-requests-total'</span> <span class=\"token punctuation\">(</span>44m<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> Pod test/podinfo</code></pre></div>\n<p>List the custom metrics resources:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> <span class=\"token string\">\"/apis/custom.metrics.k8s.io/v1beta1\"</span> <span class=\"token operator\">|</span> jq <span class=\"token builtin class-name\">.</span>\nThe Kubernetes API should <span class=\"token builtin class-name\">return</span> a resource list containing the Istio metric:\n<span class=\"token punctuation\">{</span>\n  <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"APIResourceList\"</span>,\n  <span class=\"token string\">\"apiVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"v1\"</span>,\n  <span class=\"token string\">\"groupVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"custom.metrics.k8s.io/v1beta1\"</span>,\n  <span class=\"token string\">\"resources\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"pods/istio-requests-total\"</span>,\n      <span class=\"token string\">\"singularName\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"\"</span>,\n      <span class=\"token string\">\"namespaced\"</span><span class=\"token builtin class-name\">:</span> true,\n      <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"MetricValueList\"</span>,\n      <span class=\"token string\">\"verbs\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token string\">\"get\"</span>\n      <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>After a couple of seconds the HPA will fetch the metric from the adapter:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   44m/10    <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">1</span></code></pre></div>\n<h3 id=\"autoscaling-based-on-http-traffic\" style=\"position:relative;\"><a href=\"#autoscaling-based-on-http-traffic\" aria-label=\"autoscaling based on http traffic permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Autoscaling Based on HTTP Traffic</h3>\n<p>To test the HPA, you can use the load tester to trigger a scale-up event. You can use other tools besides <code class=\"language-text\">hey</code> (e.g., <code class=\"language-text\">siege</code>). It is important that the tool supports HTTP/1.1, so <code class=\"language-text\">ab</code> (Apache Benchmark) is not the right solution.</p>\n<p>Exec into the tester pod and use <code class=\"language-text\">hey</code> to generate load for 5 minutes:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n$ hey <span class=\"token parameter variable\">-z</span> 5m <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898</code></pre></div>\n<p>Press <code class=\"language-text\">Ctrl+C</code> then <code class=\"language-text\">exit</code> to get out of the load test terminal if you want to stop prematurely. After a minute, the HPA will start to scale up the workload until the requests per second per pod drop under the target value:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">watch</span> kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   25272m/10   <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">3</span></code></pre></div>\n<p>When the load test finishes, the number of requests per second will drop to zero, and the HPA will start to scale down the workload. Note that the HPA has a back-off mechanism that prevents rapid scale-up/down events; the number of replicas will go back to one after a couple of minutes.</p>\n<p>By default, the metrics sync happens once every 30 seconds, and scaling up/down can only happen if there was no rescaling within the last 3‚Äì5 minutes.</p>\n<p>In this way, the HPA prevents rapid execution of conflicting decisions and gives time for the <a href=\"https://github.com/kubernetes/autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Cluster Autoscaler</a> to kick in.</p>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìã Summary</h2>\n<p>In this article, we explored the Kubernetes Horizontal Pod Autoscaler (HPA) using Custom Metrics. We set up the Prometheus Adapter to expose Istio metrics to Kubernetes' Custom Metrics API. However, the workload we tested did not respond effectively to the configured HPA parameters. A less erratic workload should exhibit more predictable behavior with the HPA settings.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":9,"rawMarkdownBody":"\n> **Autoscaling for Istio-Powered Kubernetes Applications**\n\n## üìö Introduction\n\nThe need for effective autoscaling is very important in Kubernetes. The [Kubernetes Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) is a great tool that allows you to automatically scale your application deployments based on various metrics.\n\nBut what if you're using [Istio](https://istio.io/), the popular service mesh for Kubernetes? Can you leverage Istio's rich set of metrics to drive your autoscaling decisions?\n\nThe answer is yes üéâ, and in this blog post, we'll explore how to configure the HPA to scale your workloads based on Istio metrics.\n\n## üöÄ Istio and Service Mesh\n\n[Istio](https://istio.io/) is a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes. At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.\n\nOne of Istio's key capabilities is its rich telemetry and observability features. Istio automatically collects a wide range of metrics, logs, and traces for all the traffic flowing through the service mesh.\n\nThis telemetry data can be used for various purposes, including monitoring the health and performance of individual services, identifying performance bottlenecks, and troubleshooting issues.\n\n## üìà Horizontal Pod Autoscaler (HPA)\n\nThe [Kubernetes Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) is a built-in feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. The HPA periodically checks the target metric, and if the value exceeds or falls below the specified thresholds, it will scale the number of pods up or down accordingly.\n\nWhen running Istio-powered applications on Kubernetes, the HPA can be configured to scale based on the rich metrics provided by Istio. For example, the HPA can monitor the incoming HTTP traffic to a service and scale the number of pods to handle the load.\n\nThis allows for dynamic and efficient scaling of Istio-based microservices, ensuring that the application can handle fluctuations in traffic without over-provisioning resources.\n\n## üìä Istio Metrics and the HPA\n\nTo integrate autoscaling into an Istio-powered Kubernetes application, you need to configure the Kubernetes Horizontal Pod Autoscaler (HPA) to monitor the metrics provided by Istio.\n\nIstio generates detailed metrics about the application's traffic, which can then be used as the basis for scaling decisions. One solution for using Istio metrics with the HPA is the [Kube Metrics Adapter](https://github.com/zalando-incubator/kube-metrics-adapter), a general-purpose metrics adapter developed by Zalando.\n\nThe Kube Metrics Adapter can collect and serve custom and external metrics, including Prometheus metrics generated by Istio, for the HPA to use. The Kube Metrics Adapter works by discovering HPA resources in the cluster and then collecting the requested metrics, storing them in memory. It supports configuring the metric collection via annotations on the HPA object.\n\nThis allows you to specify the Prometheus queries to use for retrieving the relevant Istio metrics, such as requests per second. Istio's built-in monitoring capabilities are a key advantage of using a service mesh. Istio automatically collects valuable metrics like HTTP request rates, response status codes, and request durations directly from the Envoy sidecar proxies, without requiring any changes to your application code.\n\nThis rich telemetry data provides deep visibility into your microservices' performance and behavior. Beyond monitoring, these Istio-generated metrics can be leveraged to drive advanced operational capabilities, such as autoscaling and canary deployments, without additional instrumentation.\n\nI found a very clear diagram in a [Medium article by Stefan Prodan](https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a):\n\n![istio hpa](./istio-hpa.png)\n<div class=\"image-title\"><a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\">source</a></div>\n\n## ‚öôÔ∏è Configuring HPA with Istio Metrics\n\nTo configure the Kubernetes Horizontal Pod Autoscaler (HPA) to use metrics provided by Istio, follow these steps:\n\n1. **Enable Telemetry and Prometheus**: Ensure that when installing Istio, the telemetry service and Prometheus are both enabled.\n2. **Install Metrics Adapter**: You'll need a metrics adapter that can query Prometheus and make the Istio metrics available to the HPA. One such adapter is the [Kube Metrics Adapter](https://github.com/zalando-incubator/kube-metrics-adapter), developed by Zalando.\n3. **Adapter Functionality**: The Kube Metrics Adapter scans the HPA objects in your cluster, executes the specified Prometheus queries (configured via annotations), and stores the metrics in memory. This allows the HPA to use the rich telemetry data provided by Istio, such as HTTP request rates, as the basis for scaling your application's pods up or down as needed.\n\n### üõ†Ô∏è Installing the Custom Metrics Adapter\n\nFor our solution, we will need [telemetry to be enabled in Istio](https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/values.yaml#L167) and Prometheus to scrape metrics from Istio:\n\n```shell\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo update\nhelm -n monitoring install prometheus prometheus-community/prometheus\n```\n\nIt is the default Helm chart for the Prometheus installation. We use this one because Istio has a default configuration to expose metrics for it, i.e., the pod has [the following annotations](https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/files/waypoint.yaml#L56):\n\n```yaml\nprometheus.io/path: /stats/prometheus\nprometheus.io/port: 15020\nprometheus.io/scrape: true\n```\n\nSimply having Prometheus installed in your Kubernetes cluster does not automatically make its metrics available for use with the Horizontal Pod Autoscaler (HPA). To leverage Istio's metrics for autoscaling, you'll need to set up the Prometheus Adapter. The Prometheus Adapter is a component that translates Prometheus metrics into a format that the HPA can understand and use. You'll need to provide a custom configuration file, `prometheus-adapter.yaml`, with the following settings:\n\n```yaml\nprometheus:\n  url: http://prometheus-server.monitoring.svc.cluster.local\n  port: 80\nrules:\n  custom:\n  - seriesQuery: 'istio_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'\n    resources:\n      overrides:\n        kubernetes_namespace: {resource: \"namespace\"}\n        kubernetes_pod_name: {resource: \"pod\"}\n    name:\n      matches: \"^(.*)_total\"\n      as: \"${1}_per_second\"\n    metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m]))'\n```\n\nThis configuration tells the Prometheus Adapter to:\n\n- Connect to the Prometheus server running in the monitoring namespace.\n- Query the `istio_requests_total` metric, which provides the total number of requests.\n- Convert the metric to a rate (requests per second) using a 2-minute window.\n- Expose the metric with a name like `istio_requests_per_second`.\n\nWith the Prometheus Adapter configured and deployed, the HPA will now be able to use the Istio-generated metrics to scale your application's pods as needed. Here, we can see our Prometheus instance URL, port, and one custom rule. Let's focus on this rule:\n\n- **seriesQuery**: Needed for metric discovery.\n- **resources/overrides**: Mapping fields from the metric (`kubernetes_namespace`, `kubernetes_pod_name`) to the names required by Kubernetes (`namespace`, `pod`).\n- **name/matches, name/as**: Needed to change the metric name. We are transforming this metric, so it is good to change the name `istio_requests_total` to `istio_requests_per_second`.\n- **metricsQuery**: The actual query (which is actually a query template) and it will be run by the adapter while scraping the metric from Prometheus. `rate` and `[2m]` \"calculates the per-second average rate of increase of the time series in the range vector\" (from Prometheus documentation), here it is the per-second rate of HTTP requests as measured over the last 2 minutes, per time series in the range vector (also, almost from the Prometheus documentation).\n\nNow, as we have the adapter configuration, we can deploy it using:\n\n```shell\nhelm -n monitoring install prometheus-adapter prometheus-community/prometheus-adapter -f prometheus-adapter.yaml\n```\n\n### üß™ Installing the Test App\n\nYou will use [podinfo](https://github.com/stefanprodan/podinfo) to test the HPA.\n\nFirst, create a test namespace with Istio sidecar injection enabled:\n\n```shell\nkubectl apply -f ./demo-app/test.yaml\n```\n\nCreate the deployment and ClusterIP service in the test namespace:\n\n```shell\nkubectl apply -f ./demo-app/deployment.yaml,./demo-app/service.yaml\n```\n\nIn order to trigger the autoscaling, you'll need a tool to generate traffic. Deploy the load test service in the test namespace:\n\n```shell\nkubectl apply -f ./loadtest/\n```\n\nVerify the install by calling the app API. Exec into the load tester pod and use `hey` to generate load for a couple of seconds:\n\n```shell\nexport loadtester=$(kubectl -n test get pod -l \"app=loadtester\" -o jsonpath='{.items[0].metadata.name}')\nkubectl -n test exec -it ${loadtester} -- sh\n\n$ hey -z 5s -c 10 -q 2 http://podinfo.test:9898\n\nSummary:\n  Total:\t5.0138 secs\n  Requests/sec:\t19.9451\n\nStatus code distribution:\n  [200]\t100 responses\n$ exit\n```\n\nThe app ClusterIP service exposes port 9898 under the `http` name. When using the `http` prefix, the Envoy sidecar will switch to L7 routing and the telemetry service will collect HTTP metrics.\n\n### Querying the Istio¬†metrics\n\nThe Istio telemetry service collects metrics from the mesh and stores them in Prometheus. One such metric is istio_requests_total, with it you can determine the rate of requests per second a workload receives.\nThis is how you can query Prometheus for the req/sec rate received by podinfo in the last minute, excluding 404s:\n\n```yaml\nsum(\n    rate(\n      istio_requests_total{\n        destination_workload=\"podinfo\",\n        destination_workload_namespace=\"test\",\n        reporter=\"destination\",\n        response_code!=\"404\"\n      }[1m]\n    )\n  )\n```\n\nThe HPA needs to know the req/sec that each pod receives. You can use the container memory usage metric from kubelet to count the number of pods and calculate the Istio request rate per pod:\n\n```yaml\nsum(\n    rate(\n      istio_requests_total{\n        destination_workload=\"podinfo\",\n        destination_workload_namespace=\"test\"\n      }[1m]\n    )\n  ) /\n  count(\n    count(\n      container_memory_usage_bytes{\n        namespace=\"test\",\n        pod=~\"podinfo.*\"\n      }\n    ) by (pod)\n  )\n```\n\n### Configuring the HPA with Istio¬†metrics\n\nUsing the req/sec query you can define a HPA that will scale the podinfo workload based on the number of requests per second that each instance receives:\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: podinfo\n  namespace: test\n  annotations:\n    metric-config.object.istio-requests-total.prometheus/per-replica: \"true\"\n    metric-config.object.istio-requests-total.prometheus/query: |\n      sum(\n        rate(\n          istio_requests_total{\n            destination_workload=\"podinfo\",\n            destination_workload_namespace=\"test\"\n          }[1m]\n        )\n      ) /\n      count(\n        count(\n          container_memory_usage_bytes{\n            namespace=\"test\",\n            pod=~\"podinfo.*\"\n          }\n        ) by (pod)\n      )\nspec:\n  maxReplicas: 10\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: podinfo\n  metrics:\n    - type: Object\n      object:\n        metricName: istio-requests-total\n        target:\n          apiVersion: v1\n          kind: Pod\n          name: podinfo\n        targetValue: 10\n```\n\nThe above configuration will instruct the Horizontal Pod Autoscaler to scale up the deployment when the average traffic load goes over 10 req/sec per replica.\n\nCreate the HPA with:\n\n```shell\nkubectl apply -f ./demo-app/hpa.yaml\n```\n\nStart a load test and verify that the adapter computes the metric:\n\n```shell\nkubectl -n kube-system logs deployment/kube-metrics-adapter -f\nCollected 1 new metric(s)\nCollected new custom metric 'istio-requests-total' (44m) for Pod test/podinfo\n```\n\nList the custom metrics resources:\n\n```shell\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq .\nThe Kubernetes API should return a resource list containing the Istio metric:\n{\n  \"kind\": \"APIResourceList\",\n  \"apiVersion\": \"v1\",\n  \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n  \"resources\": [\n    {\n      \"name\": \"pods/istio-requests-total\",\n      \"singularName\": \"\",\n      \"namespaced\": true,\n      \"kind\": \"MetricValueList\",\n      \"verbs\": [\n        \"get\"\n      ]\n    }\n  ]\n}\n```\n\nAfter a couple of seconds the HPA will fetch the metric from the adapter:\n\n```shell\nkubectl -n test get hpa/podinfo\nNAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   44m/10    1         10        1\n```\n\n### Autoscaling Based on HTTP Traffic\n\nTo test the HPA, you can use the load tester to trigger a scale-up event. You can use other tools besides `hey` (e.g., `siege`). It is important that the tool supports HTTP/1.1, so `ab` (Apache Benchmark) is not the right solution.\n\nExec into the tester pod and use `hey` to generate load for 5 minutes:\n\n```shell\nkubectl -n test exec -it ${loadtester} -- sh\n$ hey -z 5m -c 10 -q 2 http://podinfo.test:9898\n```\n\nPress `Ctrl+C` then `exit` to get out of the load test terminal if you want to stop prematurely. After a minute, the HPA will start to scale up the workload until the requests per second per pod drop under the target value:\n\n```shell\nwatch kubectl -n test get hpa/podinfo\nNAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   25272m/10   1         10        3\n```\n\nWhen the load test finishes, the number of requests per second will drop to zero, and the HPA will start to scale down the workload. Note that the HPA has a back-off mechanism that prevents rapid scale-up/down events; the number of replicas will go back to one after a couple of minutes.\n\nBy default, the metrics sync happens once every 30 seconds, and scaling up/down can only happen if there was no rescaling within the last 3‚Äì5 minutes.\n\nIn this way, the HPA prevents rapid execution of conflicting decisions and gives time for the [Cluster Autoscaler](https://github.com/kubernetes/autoscaler) to kick in.\n\n## üìã Summary\n\nIn this article, we explored the Kubernetes Horizontal Pod Autoscaler (HPA) using Custom Metrics. We set up the Prometheus Adapter to expose Istio metrics to Kubernetes' Custom Metrics API. However, the workload we tested did not respond effectively to the configured HPA parameters. A less erratic workload should exhibit more predictable behavior with the HPA settings.\n\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1664},"frontmatter":{"id":"de71ab9829cbe751d2164864","path":"/blog/scaling-kubernetes-istio-metrics-hpa/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Scaling Kubernetes Workloads with Istio Metrics and the Horizontal Pod Autoscaler","keywords":["Kubernetes","Istio","Horizontal Pod Autoscaler","Autoscaling","Performance Optimization"],"excerpt":"Learn how to effectively scale Kubernetes workloads using Istio metrics and the Horizontal Pod Autoscaler for optimal performance.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAChklEQVR42nVTy0/TQRDelmKJByJKTIgYLBFQgiJoU6CQIIKGRIMUf30pbaHUasEXUPqj75aWBiglvN8gSjwaDl5MrIlnY7x48F/woEcvHD5nVzDRxMOX2Xl9Mzs7y0o8U1BbwmB3/FBIAShIKqXfZ25TGslGOpO4PSB8XHIbj1WYglC7UygYmkPRsxWwUw8mcNI5Dp1/GbrRJTSFVnFlZEGgObKOyyQbyMfttd55tEQ3UE+61reEOpL6wApKB9JoT71AsWca7PTDSRT2JtEx8RKt8U0YJnchTb/CNUrkhLUHxC2xDdRRwUYi5vr1sS20xbdwI7GNq+S7Ob4DjScNVtCTQJ41jAuDsyh/nEH5o2lcok64LCM0hdYEycXhOdQHltFAHdWQv/LpDMqfZHCeJB9bFeUXOpNgSpqBmFOnDIXhAJ0+MIKafE3BVUGi9S2KqzaH11Bgi4PdHoGia1TEM8MoxctipkxpDCKXD5agMoWQYw4h1xwWEDoV4+BFcyiGPwZvQkVxh8j5I4NESAdmoYqWBL1YhJLJKFFFg5cqekn3CyIezKXSFBCE/wNTdCfQYunHkNGICsswEUdx1JJGhec9ip1vqMsx0d1hV/nWpFiVf4m4jVEca7cNYM/UCretDx8kPU6YY2iUv2I3u4/M632U3s/SPvpE0hEaQ5VzE2pzhAj+7jTPGsExxxiYbLdj+24Hzngy+CjpoLUmcCv1He8+/8TbT/vQDn4B6woI0hLHPNo8e9A4FqgbmYhCgizfHoemfwrVPrJXD6cx4+xGtM+FsMOFIuc8zrmz6Jv9AVPqG47fe04/Q4bKGEJFLy23awdne5bF9XLph/FVKaP1qZEXoY+t4xekk8U8iHPi5gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/b82cdf0ead47af1ad1802eff29c72c00/9613e/istio-hpa-cover.png","srcSet":"/static/b82cdf0ead47af1ad1802eff29c72c00/23c6c/istio-hpa-cover.png 750w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/d7b18/istio-hpa-cover.png 1080w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/a6044/istio-hpa-cover.png 1366w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/9613e/istio-hpa-cover.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/b82cdf0ead47af1ad1802eff29c72c00/f8e3a/istio-hpa-cover.webp 750w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/2af51/istio-hpa-cover.webp 1080w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/d339b/istio-hpa-cover.webp 1366w,\n/static/b82cdf0ead47af1ad1802eff29c72c00/9b00e/istio-hpa-cover.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.574375}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/improve-k8s-image-caching/","title":"Improve Container Image Availability and Speed with Caching in Kubernetes üï∏","date":"2024-10-25 19:36:00"},"excerpt":"A Guide to Tools and Strategies of image cachingüö¶ üöõ Intro When deploying a containerized application to a Kubernetes cluster, delays can‚Ä¶","html":"<blockquote>\n<p><strong>A Guide to Tools and Strategies of image cachingüö¶</strong></p>\n</blockquote>\n<h2 id=\"-intro\" style=\"position:relative;\"><a href=\"#-intro\" aria-label=\" intro permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöõ Intro</h2>\n<p>When deploying a containerized application to a Kubernetes cluster, delays can occur due to the time it takes to pull necessary container images from the registry.</p>\n<p>This delay can be especially problematic in scenarios where the application needs to scale out horizontally or process high-speed real-time data. Fortunately, there are several tools and strategies available to improve container image availability and caching in Kubernetes.</p>\n<p>In this blog post, we will explore a comprehensive guide to these tools and strategies, including <a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a>, <a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, Kubernetes built-in image caching features, local caches, and monitoring and cleaning up unused images.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 535px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 214.11764705882354%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAArCAYAAAB4pah1AAAACXBIWXMAAAsTAAALEwEAmpwYAAADNUlEQVR42p3X124sIRAE0Pn/z/Kjnxwk55xzzomrM1KNZies9xoJwQ5QVHd1A1uVTvn5+anbh4eHsrm5WQ4PD8vV1dXE2LRSjQFeXFyUpaWlsrW1VQ4ODv4O2C7Y7ezslK+vrzJrGQQMk+Pj47K+vl73v7+//wYYsLe3t3J6elpOTk7K4+Pj30zOgtfX19pvHx8fNbP9/f1apFlAqy7Yy8tL2d7erhmmfH5+1r6cBbTqMqNqwNoLge7u7v4KWnXBtN0F6QPd29srT09Po6BVwLpmjjEQQtTnmqF5VXYNMy1VLVAJo9pMG9Czs7MJAg1ge7eknNhbXV2tN1peXi4LCwvl/Py8CZ+YL6y68Vnxh8ld6ibb7PLyss6Y9/f3CRNvb28bgXo+vLu7K0dHR82CIf+1vzssAE5V+ebmpgFtA7SrgvH19fV0lTNgYhu0qzgwdWocDp2DfJbfafn0N7BBQKZTmJ/aAlD+/v5+dsCcMLLFAuoDTxWDUm+m06bNxEmd8vz8XEdAO5R+YznBELuhBeljnetgJpPbia8IZqmocodMWVxcrDPLb+NaWTNoMhWZmXMx6cYVTFdtqhUNNk+mBbTqhkyuzPjTAt8B62ttakNzsDQvB8UEoMmJNcq6T/zWp7D4tNimmLq3JYM5yfVRQOIwxVEVn8VEv7GMH83L0dYDTNgAYip2WGBloeMsrHwzbpPk+4TKdnNwjhWO/+0V0Us9fmIuttTkeCz0Bfbc3NyE8uZGca7oAcpZLLDlpyhqQbuvTYyaqxKxB7iystL4BzAWFDaZf7XxrXFMc4341gMUDs5EdwXnMwk4EJUwFusbT0tx4D1Ag+LLIuyishYYVlo1lsSXLKja923Sb2Njo/ZJMkQFOD8/X7NPPCZ74l+xWOWYspMJ3jD8aFIU1udH12l8NfQkaeIQGHRx5iO2qXzZ7hvPnZx3Y/siq3IQ2BkLJhFGgDsfLdT6JkbX1tYaloNPkTDMs4P5Yiv5mthr57P+2EO+yl2CEQZUzMmCjQtLH0NjGFM45+Yg4Ng9zHzmzvoc7uWymscPszB0ukTRrgD/9WjPHx911vf1VEAMmcx/Y/4aAvwHjN8Qs+BOyI0AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"lifecycle\" title=\"\" src=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png\" srcset=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/04472/lifecycle.png 170w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/9f933/lifecycle.png 340w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png 535w\" sizes=\"(max-width: 535px) 100vw, 535px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"Ô∏è-preamble\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-preamble\" aria-label=\"Ô∏è preamble permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ö°Ô∏è Preamble</h2>\n<p>When you deploy a workload to Kubernetes, the containers in a certain Pod are naturally based on OCI container Images. These Images can be pulled from private/public repositories of many kinds. Kubernetes caches the images locally on every node that has pulled them, so that other Pods might use the same image. The settings for how and when Kubernetes pulls images can be found in the <a href=\"https://kubernetes.io/docs/concepts/containers/images/\" target=\"_blank\" rel=\"noopener noreferrer\">documentation</a>.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 77.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACOElEQVR42q2U2W7TUBCG86QIiUfgnifgikVcgtTCHWIRBcRFUaVEIoLSliiFhLRZmjhOvKSpt+Oz+GfmOI7aCxIJYWlizznj/3yzxLWiAISkn/901Q5aGR6+XaLvSnILaFOgKEozxqyfN61VZgWffopwb2eBz8cJuRq51JBSQikFScZ3JelZrnxd7Wu7plc+G4vWvKVGvZ0gEwq+58Gdzm0gC/meD2fiIs+VFQiDAJPxlHxpY5aXS4wvHHo3t2sMWWNMpcoTzvwrjL0JEQkizXHuxxjMRtAyg6YDhmGMs9kQKqf9PIezzPDbHUOKGIIEWbHGqjmdZrTC/bqDuy+bSBNK3yg8ac5xZ7eBNI3pWIOdIw+3XzQw865seV6fLnDreRN9ygqFgWFBJmQ6Ftz72sOz/VOklIIh6v2TAR5/+IEkE1aw3h7h0fsThFFqD/zem+LBuyO4i9hmUFSCVQPmzgizi/N1kf3ZBO6wZ9PjZoSeC2fQhcgyW9PLwMOk30GWJramxXVC7pYQEmmW22D2udBRkpYdJcuINEmF3av2mV6qstN2bK4PJQcxLV+rsbJ0SUpN0QZRHK1jeZ+JmOzGYG8TDCON0VxgHGg4QXbj5VJQ/l2Q/wVMVKWkqTHfeikOfmnsHSs0OjlCGhWOqwY730RYUVpSMqMlvnRTvDo0+Ngq8OZQIE5IUJeCatXZjYLrdFb3KKPu+wsEQYhgKbZ/HLYFMEWn20Or/RNRFK1r98+CVW2rr8w2wj8fHdY5jW02ZAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cr\" title=\"\" src=\"/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png\" srcset=\"/static/13fb62f279e7732af7156210ef5c9ba1/04472/cr.png 170w,\n/static/13fb62f279e7732af7156210ef5c9ba1/9f933/cr.png 340w,\n/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png 680w,\n/static/13fb62f279e7732af7156210ef5c9ba1/3c051/cr.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>In most use cases, it's not enough. Most Cloud Kubernetes clusters today require auto-scaling and are dynamically allocated Nodes based on the customer's usage. What if multiple nodes have to pull the same image multiple times? And if this image is heavy, that can take minutes. In the applicative autoscaling world, that is a relatively long time.</p>\n<h2 id=\"-the-solution\" style=\"position:relative;\"><a href=\"#-the-solution\" aria-label=\" the solution permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí• The Solution</h2>\n<p>The expected solution needs to have a cache layer on top of Kubernetes, so that Kubernetes has a centralized image cache and all nodes \"pull\" from it. But because the cache needs to be very fast, the caching solutions need to sit inside Kubernetes, and all nodes should have the fastest latency towards it.</p>\n<h2 id=\"-existing-solutions\" style=\"position:relative;\"><a href=\"#-existing-solutions\" aria-label=\" existing solutions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÆ Existing Solutions</h2>\n<p>The widely-used approach to tackle the problem of delays in pulling container images from the registry is to have a registry mirror running inside the cluster. Two widely used solutions are the in-cluster self-hosted registry and pull-through cache.</p>\n<ul>\n<li><strong>In-cluster self-hosted registry:</strong> A local registry is run within the Kubernetes cluster and is configured as a mirror registry in the container runtime. Any image pull request is directed to the in-cluster registry.</li>\n<li><strong>Pull-through cache:</strong> A cache of container images is built and managed directly on the worker nodes.</li>\n</ul>\n<p>Other existing solutions include using a reliable caching solution like <a href=\"https://github.com/kuikproject/kuik\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, enabling image caching in Kubernetes, using a local cache, optimizing container image builds, and monitoring and cleaning up unused images.</p>\n<h2 id=\"-harbor\" style=\"position:relative;\"><a href=\"#-harbor\" aria-label=\" harbor permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö¢ Harbor</h2>\n<p><a href=\"https://goharbor.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Harbor</a> is a CNCF Graduated project that functions as a container registry, but most importantly as a Pull Through Proxy Cache.</p>\n<p>A pull-through proxy cache is a caching mechanism designed to optimize the distribution and retrieval of container images within a container registry environment. It acts as an intermediary between clients (such as container runtimes or build systems) and the upstream container registry.</p>\n<p>When a client requests a container image, the pull-through proxy cache checks if it already has a local copy of the requested image. If the image is present, the proxy cache serves it directly to the client, eliminating the need to download it from the upstream registry. This reduces network latency and conserves bandwidth.</p>\n<p>If the requested image is not present in the local cache, the proxy cache acts as a regular proxy and forwards the request to the upstream registry. The proxy cache then retrieves the image from the registry and serves it to the client. Additionally, the proxy cache stores a copy of the image in its local cache for future requests.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 125.88235294117646%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAADlklEQVR42q1Vz4scRRTef0oET55V8ODJsyD+AYKRoAfJRUQEIadEggkSSCJkiZcNJioTze7i7CS7mx13fmzv9OzMdO/2TE//7uqq6s9X1d2707O7QsCCoutVdX316nvfe7UCagnLEcQS/0db4QK48yTE96se3IAM5JAyR56/fteAjOe4/sjHVz+5GB6nkCJDyhg458iyYswW7CRl+ru4rr9pqkFXFOrMFzDtlBYYjicWXNejMUeapLDGE3heiIwTWJJgMhojCCKyOaIogk3rYRDXAdVAnRgzjsedIwxtA7nI4cUMj/ePYDkDbU/DFE+6Q9jOEBAStp/gac+EMxvRLfgZoOJMEGCQZHj/1jau/97WXFpegndubuPWX/uaH8OJ8N4PL3F3s6ftthXg3ZsvsNoy6HdBZyx5GEYxnrf20B1YxQFBiD+3CjuXEq7n4VnzFYzRCdkCM3eOBtmmNdV0nbuylByHhgl3NtMeqGBYoyE8dwYVRMXT5MhE4M31ukfcWSMTSeQToKhfmUiB7Urcf8awtpVSNLOavk7lUViwZoykFuBeg5RBk0IsAAoNKNHqZ/ji9hxf33OJ6JCiGqF/cADHcTAYDHBAY8M4JDyGjT0XH303xac3ZggTqSmQixxK4kjpaW3DxovOFJXnak6tKQ8ULapXbW1d/VtcX8msdmXVgzCE0WtjYHTBhahduQJVXR2idMhSn2ym17mocSg16b4/x9bODrrmMbkt9XwFFIZRDbToxT4NyJcAVev1/8EvrUdodoaUJYk+VQiST5zCPDlBxiK9sbp65ek5DwsOc4psjK3dDdjWuLhmeXpnyvDxzy2s7/fL+fw08gr0nIeFDkVJblbKCKcVZDQNce3hS7QOZ5cDiksAGf2g+Cl4LTayJIZtdpHGQQl0xu2Zh/xiwCprVGdl6UooQ+Z+qA8rbJrP+KlULvGQ1wqlroGUn6Ikf7EmqhQVPFugJa/rUGdLKQVJig9TKk1OgKlP3mRSzymJCFFU9AfPJ/j8zi5MJ9U2L/fqir38JkT0vrTHEo19id/2BDqTQlKiDNRm38ebVzfx1pdNfHKjXWb2wpuyDJiSrJ6+4njY5PixwQlA1AB/3XXxxpV1vP3ZH/jwm79rYBcCqmbPM6p9IexpBDesgnW2/u1qDx9ca6DZdWoyOgdYBWTuutimFOz1evodWW5jsw+js4Mo8Kud/+1hVWWqGrfcuK48F6/9C56mf4tpydg9AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"harbor\" title=\"\" src=\"/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png\" srcset=\"/static/f87b305ff11726efc93f028b32db411e/04472/harbor.png 170w,\n/static/f87b305ff11726efc93f028b32db411e/9f933/harbor.png 340w,\n/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png 680w,\n/static/f87b305ff11726efc93f028b32db411e/3c051/harbor.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<hr>\n<h2 id=\"-kube-fledged\" style=\"position:relative;\"><a href=\"#-kube-fledged\" aria-label=\" kube fledged permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê¶ kube-fledged</h2>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.176470588235293%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABBklEQVR42o2QQUvDQBCF+7s9iEXxINVz7x6tFkEPepBeom2h2ELBmNrYUDTZJNoQkyZp0v1cg9qDVRx4MDvz9uMxFdZVUcBwCJYFhgGaBlnGf6qyHriEmzYymIEQyNGoHEu5snz08i/gj+W9DmGAnE7Jk3Tt518TpgtJ5yHGD3OeXhe09BjPtPGGJrH7wuF1QGcUESYFt5M53XGMYadY/gInyGndvZEVcgXM1aN67LB35lJT2lfaaAgOLme0xwn1C4/NI5vtpsNOUyifYOvEoaq0eyqonbsl4xu4VAfRn9MypWbE9CcJPTNmoNJEaYEpUvrWnN7jvEzY6AbUr3zljRh8zr9O8g7kE8LJCduHhQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"fledged\" title=\"\" src=\"/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png\" srcset=\"/static/8243b2682f856240dd3fae34b0836534/04472/fledged.png 170w,\n/static/8243b2682f856240dd3fae34b0836534/9f933/fledged.png 340w,\n/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png 680w,\n/static/8243b2682f856240dd3fae34b0836534/8ae3e/fledged.png 756w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a> is a Kubernetes add-on or operator for creating and managing a cache of container images directly on the worker nodes of a Kubernetes cluster. It allows a user to define a list of images and onto which worker nodes those images should be cached (i.e. pulled). As a result, application pods start almost instantly, since the images need not be pulled from the registry. kube-fledged provides CRUD APIs to manage the lifecycle of the image cache, and supports several configurable parameters in order to customize the functioning as per one's needs.</p>\n<p>kube-fledged is designed and built as a general-purpose solution for managing an image cache in Kubernetes. Though the primary use case is to enable rapid Pod start-up and scaling, the solution supports a wide variety of use cases as mentioned below.</p>\n<h3 id=\"how-kube-fledged-works\" style=\"position:relative;\"><a href=\"#how-kube-fledged-works\" aria-label=\"how kube fledged works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How kube-fledged works</h3>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADRklEQVR42n1TbUxTVxg+4CyVigSn1g+WuOhm7FBmCH78EUOEuIZp/VGXaU2VZNO44DcDxHgNWqXUIm3px+39aO+9pd9XoJWyVlcRqJ3KhPBbXbaQ7I8//aW99/W2SEJi4ps8Oed9z3Pe85z3PQehRabVYrLjx5hW/Zn+tQ063RpAqLicptvKOO9rBcf8I6eo3wtEDCtGn1rRojkUHJ2OXXNcxxInT7LVCOZjqz3M9fUB/7N1/oFnq9zui/lY069YqV6PybHFiT/yP2vhcHjJPBcVtRxoKVEipUJyl0mQV1ZWLtPr9XKQEjXVNJViCMn0dXXyBalF8zfBvrBYLCVajFi542akb7dxiK/tHorVGPhT+fVe5+PLLPuU5waeB93EpGlhXzXm3bkd467t7GS/RoslH719v6LVSJbVd/iVu8yJf/fZH8Je1zjU9sQCeQrjm4o+Ss9BMvkKIpHZl0illeXj3xkGr1QZ41B1a1CNGhu1K1Uq1XKkRIrm5tYy7RlsucQpKT90bqNMc26r7OfWb8qPtldIsVKtHlvb0olv6ugbVrZLhy/UrcoQq9tmiDq337r3rdRalQwzBKsG7zzUhI2pgzHT2I+kcURjNkfVdkus0e1I1jvwR/vNtvhhc39cbXKM1NvIsT39vsxeO5ep/djQonypCiXA8amlVnx8Wsel4CdvXNB4hsVL5AOBI7Oih3wi0u4M+OisyIf+Fnh+RhiJzkIsMCOOhGYh5J8S77rG9uRVXjKZFIWEM+yMoo+cmKvwcVDJh4SlAS+oCR4c1hScv8tDZ98QXJDGLst9cOIT8Bv5B/xCj4onqITQy/4FDJFVb7jKtn1/Izy5pZNJIQyPleLE5H8Gehw68AdCG54Ck/NP6LGNQrP1Hpy1DcMJGw/ttjj0OtPwFcvCihAjFgdo8RQ9CvaepG6rNT29j3kBqq7IG0nh/4p+Kju32R+B1cEBYUWQgwYiCrRrArrtSTBaE9AtJbe60kBRWeilJqCHeixKEDy+aXAzU01fXuUOVncFNdtuD1SgsDa8xOoayxxhEvCDd+hdg2cwdxpP5LzEkxxFZHKkO5Oj8pB8QoKHyEpr2fcMmRUIIvP2jiNdM/+GofBrPgAU7aW+ygepZQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"working\" title=\"\" src=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png\" srcset=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/04472/working.png 170w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/9f933/working.png 340w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png 680w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/b12f7/working.png 1020w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/525d3/working.png 1090w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Kubernetes allows developers to extend the Kubernetes API via Custom Resources. kube-fledged defines a custom resource of kind \"ImageCache\" and implements a custom controller (named kubefledged-controller). kubefledged-controller does the heavy-lifting for managing image cache. Users can use kubectl commands for creation and deletion of ImageCache resources.</p>\n<h2 id=\"-kubernetes-image-puller\" style=\"position:relative;\"><a href=\"#-kubernetes-image-puller\" aria-label=\" kubernetes image puller permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê≥ kubernetes-image-puller</h2>\n<p>To cache images, <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Image Puller</a> creates a Daemonset on the desired cluster, which in turn creates a pod on each node in the cluster consisting of a list of containers with command <code class=\"language-text\">sleep 720h</code>. This ensures that all nodes in the cluster have those images cached. The sleep binary being used is <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main/sleep\" target=\"_blank\" rel=\"noopener noreferrer\">golang-based</a> (please see <a href=\"https://github.com/che-incubator/kubernetes-image-puller#scratch-images\" target=\"_blank\" rel=\"noopener noreferrer\">Scratch Images</a>). We also periodically check the health of the daemonset and re-create it if necessary.</p>\n<p>The application can be deployed via Helm or by processing and applying OpenShift Templates. Also, there is a community-supported operator available on the <a href=\"https://operatorhub.io/operator/kubernetes-imagepuller-operator\" target=\"_blank\" rel=\"noopener noreferrer\">OperatorHub</a>.</p>\n<p>üí° Kubernetes Image Puller deploys a huge number of containers (one container per image and per node / uses a daemonset for the caching mechanism), to fulfill the caching feature.</p>\n<p>Let's take this example: With 5 nodes &#x26; 10 images in cache, we already have 50 containers within the cluster dedicated for the caching feature.</p>\n<h2 id=\"Ô∏è-tugger\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-tugger\" aria-label=\"Ô∏è tugger permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Tugger</h2>\n<p><a href=\"https://github.com/jainishshah17/tugger\" target=\"_blank\" rel=\"noopener noreferrer\">Tugger</a> uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<p>üí° Tugger uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<h2 id=\"Ô∏è-kube-image-keeper-kuik\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kube-image-keeper-kuik\" aria-label=\"Ô∏è kube image keeper kuik permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è kube-image-keeper (kuik)</h2>\n<p><a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kube-image-keeper</a> (a.k.a. kuik, which is pronounced /kw…™k/, like \"quick\") is a container image caching system for Kubernetes. It saves the container images used by your pods in its own local registry so that these images remain available if the original becomes unavailable.</p>\n<h3 id=\"how-it-works\" style=\"position:relative;\"><a href=\"#how-it-works\" aria-label=\"how it works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How it works</h3>\n<p>When a pod is created, kuik's mutating webhook rewrites its images on the fly, adding a <code class=\"language-text\">localhost:{port}/</code> prefix (the port is 7439 by default, and is configurable).</p>\n<p>On <code class=\"language-text\">localhost:{port}</code>, there is an image proxy that serves images from kuik's caching registry (when the images have been cached) or directly from the original registry (when the images haven't been cached yet).</p>\n<p>One controller watches pods, and when it notices new images, it creates CachedImage custom resources for these images.</p>\n<p>Another controller watches these CachedImage custom resources, and copies images from source registries to kuik's caching registry accordingly.</p>\n<h3 id=\"architecture-and-components\" style=\"position:relative;\"><a href=\"#architecture-and-components\" aria-label=\"architecture and components permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Architecture and components</h3>\n<p>In kuik's namespace, you will find:</p>\n<ul>\n<li>A Deployment to run kuik's controllers.</li>\n<li>A DaemonSet to run kuik's image proxy.</li>\n<li>A StatefulSet to run kuik's image cache (a Deployment is used instead when this component runs in HA mode).</li>\n</ul>\n<p>The image cache will obviously require a bit of disk space to run (see <a href=\"https://github.com/enix/kube-image-keeper#garbage-collection-and-limitations\" target=\"_blank\" rel=\"noopener noreferrer\">Garbage collection and limitations</a>). Otherwise, kuik's components are fairly lightweight in terms of compute resources. This shows CPU and RAM usage with the default setup, featuring two controllers in HA mode:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl <span class=\"token function\">top</span> pods\nNAME                                             CPU<span class=\"token punctuation\">(</span>cores<span class=\"token punctuation\">)</span>   MEMORY<span class=\"token punctuation\">(</span>bytes<span class=\"token punctuation\">)</span>\nkube-image-keeper-0                              1m           86Mi\nkube-image-keeper-controllers-5b5cc9fcc6-bv6cp   1m           16Mi\nkube-image-keeper-controllers-5b5cc9fcc6-tjl7t   3m           24Mi\nkube-image-keeper-proxy-54lzk                    1m           19Mi</code></pre></div>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 54.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5qiQf/xAAVEAEBAAAAAAAAAAAAAAAAAAAQAf/aAAgBAQABBQJr/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERQRAxUXH/2gAIAQEAAT8hcqyHQp7gtFv0/9oADAMBAAIAAwAAABAwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQADAQADAAAAAAAAAAAAAAEAESExUXGB/9oACAEBAAE/ELFYruxRFZUGtd8QC7OHqCmr9T//2Q=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuik\" title=\"\" src=\"/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg\" srcset=\"/static/13e29d44f62c2c264156b503f96d7501/651be/kuik.jpg 170w,\n/static/13e29d44f62c2c264156b503f96d7501/d30a3/kuik.jpg 340w,\n/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg 680w,\n/static/13e29d44f62c2c264156b503f96d7501/990cb/kuik.jpg 1020w,\n/static/13e29d44f62c2c264156b503f96d7501/151cf/kuik.jpg 1181w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-warm-image\" style=\"position:relative;\"><a href=\"#-warm-image\" aria-label=\" warm image permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• warm-image</h2>\n<p>The WarmImage CRD takes an image reference (with optional secrets) and prefetches it onto every node in your cluster. To install this custom resource onto your cluster, you may simply run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\n<span class=\"token function\">curl</span> https://raw.githubusercontent.com/mattmoor/warm-image/master/release.yaml <span class=\"token punctuation\">\\</span>\n    <span class=\"token operator\">|</span> kubectl create <span class=\"token parameter variable\">-f</span> -</code></pre></div>\n<p>Alternately you may git clone this repository and run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\nkubectl create <span class=\"token parameter variable\">-f</span> release.yaml</code></pre></div>\n<hr>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèÅ Conclusion</h2>\n<p>In this post, we showed you how to speed up Pod startup by caching images on nodes. By prefetching container images on worker nodes in your Kubernetes Cluster, you can significantly reduce Pod startup times, even for large images, down to a few seconds. This technique can greatly benefit customers running workloads such as machine learning, simulation, data analytics, and code builds, improving container startup performance and overall workload efficiency.</p>\n<p>By eliminating the need for additional management of infrastructure or Kubernetes resources, this approach offers a cost-efficient solution for addressing the slow container startup problem in Kubernetes-based environments.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/carvel-kapp-kubernetes-management/","title":"Simplify Kubernetes Application Management with Carvel kapp","date":"2024-10-25 18:06:00"},"excerpt":"Take control of your Kubernetes resourcesüî• üìî Introduction Kubernetes applications involve juggling multiple resources, configurations, and‚Ä¶","html":"<blockquote>\n<p><strong>Take control of your Kubernetes resourcesüî•</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìî Introduction</h2>\n<p>Kubernetes applications involve juggling multiple resources, configurations, and dependencies. Carvel kapp offers a straightforward solution to this complexity. It treats related Kubernetes resources as logical \"applications,\" allowing you to deploy, update, and manage them as cohesive units. With kapp, you can safely apply changes, preview diffs, and prune obsolete resources‚Ää‚Äî‚Ääall while minimizing disruptions to running workloads.</p>\n<h2 id=\"kapp-overview\" style=\"position:relative;\"><a href=\"#kapp-overview\" aria-label=\"kapp overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>kapp Overview</h2>\n<p>Carvel kapp is a lightweight, command-line tool that simplifies the deployment and management of Kubernetes applications. Unlike some other tools, kapp does not require any server-side components, elevated privileges, or custom resources, making it highly portable and easy to use, even in RBAC-constrained clusters.</p>\n<p>At its core, kapp is designed to be explicit, providing visibility into the changes it will apply to your cluster before executing them. It calculates the differences between your desired configuration and the live cluster state, presenting a clear set of create, update, and delete operations for your approval.</p>\n<p>One of kapp's standout features is its dependency-aware resource ordering. It intelligently manages the deployment order of certain resources, ensuring that dependencies are respected. For instance, Custom Resource Definitions (CRDs) and Namespaces are installed before the resources that depend on them. Additionally, kapp allows you to declare your own dependency rules, such as requiring a Job to complete database migrations before updating a Deployment.</p>\n<h2 id=\"deploy-kubernetes-applications-with-kapp\" style=\"position:relative;\"><a href=\"#deploy-kubernetes-applications-with-kapp\" aria-label=\"deploy kubernetes applications with kapp permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deploy Kubernetes Applications with kapp</h2>\n<p>Before getting too deep, let's get some basic preparations out of the way:</p>\n<ol>\n<li><strong>Create a Kubernetes cluster.</strong></li>\n<li><strong>Install <a href=\"https://carvel.dev/ytt/\" target=\"_blank\" rel=\"noopener noreferrer\">ytt</a>, <a href=\"https://carvel.dev/kbld/\" target=\"_blank\" rel=\"noopener noreferrer\">kbld</a>, <a href=\"https://carvel.dev/kapp/\" target=\"_blank\" rel=\"noopener noreferrer\">kapp</a></strong> by following instructions in the <a href=\"https://carvel.dev/#install\" target=\"_blank\" rel=\"noopener noreferrer\">Install section on carvel.dev</a>.</li>\n</ol>\n<p>To get started with our example application, clone <a href=\"https://github.com/seifrajhi/kapp-k8s-demo.git\" target=\"_blank\" rel=\"noopener noreferrer\">kapp-k8s-demo</a> locally:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/kapp-k8s-demo.git\n<span class=\"token builtin class-name\">cd</span> kapp-k8s-demo</code></pre></div>\n<p>This directory contains a simple Go application that consists of main.go (an HTTP web server) and a Dockerfile¬†.\nMultiple step-* directories contain variations of application configuration that we will use in each step.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ <span class=\"token function\">ls</span> <span class=\"token parameter variable\">-l</span>\nDockerfile\nmain.go\nconfig-minimal</code></pre></div>\n<p>Typically, an application deployed to Kubernetes will include <strong>Deployment</strong> and <strong>Service</strong> resources in its configuration. In our example, <code class=\"language-text\">config-minimal/</code> directory contains <code class=\"language-text\">config.yml</code> which includes exactly that. (Note that the Docker image is already preset and the environment variable <code class=\"language-text\">HELLO_MSG</code> is hard coded. We'll get to those shortly.)</p>\n<p>Traditionally, you can use <code class=\"language-text\">kubectl apply -f config-minimal/config.yml</code> to deploy this application. However, <code class=\"language-text\">kubectl</code> does not indicate which resources are affected and how they are affected before applying changes, and does not yet have a robust prune functionality to converge a set of resources.</p>\n<p><strong>kapp</strong> addresses and improves on several <code class=\"language-text\">kubectl</code> limitations as it was designed from the start around the notion of a \"Kubernetes Application\" ‚Äî a set of resources with the same label:</p>\n<ul>\n<li><strong>Visibility and Confidence</strong>: <code class=\"language-text\">kapp</code> separates the change calculation phase (diff) from the change apply phase (apply) to give users visibility and confidence regarding what's about to change in the cluster.</li>\n<li><strong>Resource Tracking</strong>: <code class=\"language-text\">kapp</code> tracks and converges resources based on a unique generated label, freeing its users from worrying about cleaning up old deleted resources as the application is updated.</li>\n<li><strong>Order Management</strong>: <code class=\"language-text\">kapp</code> orders certain resources so that the Kubernetes API server can successfully process them (e.g., CRDs and namespaces before other resources).</li>\n<li><strong>Readiness Wait</strong>: <code class=\"language-text\">kapp</code> tries to wait for resources to become ready before considering the deploy a success.</li>\n</ul>\n<p>For more information, you can visit the <a href=\"https://carvel.dev/kapp/docs/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Carvel kapp documentation</a>.</p>\n<p>Let us deploy our application with kapp:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp deploy <span class=\"token parameter variable\">-a</span> simple-app <span class=\"token parameter variable\">-f</span> step-1-minimal/\nTarget cluster <span class=\"token punctuation\">[</span><span class=\"token string\">'https://192.168.99.111:8443'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>https://192.168.99.111:8443<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nChanges\nNamespace  Name        Kind        Conds.  Age  Op      Op st.  Wait to    Rs  Ri\ndefault    simple-app  Deployment  -       -    create  -       reconcile  -   -\n^          simple-app  Service     -       -    create  -       reconcile  -   -\nOp:      <span class=\"token number\">2</span> create, <span class=\"token number\">0</span> delete, <span class=\"token number\">0</span> update, <span class=\"token number\">0</span> noop\nWait to: <span class=\"token number\">2</span> reconcile, <span class=\"token number\">0</span> delete, <span class=\"token number\">0</span> noop\nContinue? <span class=\"token punctuation\">[</span>yN<span class=\"token punctuation\">]</span>: y\n<span class=\"token number\">8</span>:17:44PM: ---- applying <span class=\"token number\">2</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">0</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:44PM: create deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:44PM: create service/simple-app <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:44PM: ---- waiting on <span class=\"token number\">2</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">0</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:45PM: ok: reconcile service/simple-app <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM: ongoing: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:  ^ Waiting <span class=\"token keyword\">for</span> generation <span class=\"token number\">2</span> to be observed\n<span class=\"token number\">8</span>:17:45PM:    ok: waiting on replicaset/simple-app-7fbc6b7c9b <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:    ongoing: waiting on pod/simple-app-7fbc6b7c9b-g92t7 <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:     ^ Pending: ContainerCreating\n<span class=\"token number\">8</span>:17:45PM: ---- waiting on <span class=\"token number\">1</span> changes <span class=\"token punctuation\">[</span><span class=\"token number\">1</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:45PM: ongoing: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:  ^ Waiting <span class=\"token keyword\">for</span> <span class=\"token number\">1</span> unavailable replicas\n<span class=\"token number\">8</span>:17:45PM:    ok: waiting on replicaset/simple-app-7fbc6b7c9b <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:    ongoing: waiting on pod/simple-app-7fbc6b7c9b-g92t7 <span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:45PM:     ^ Pending: ContainerCreating\n<span class=\"token number\">8</span>:17:49PM: ok: reconcile deployment/simple-app <span class=\"token punctuation\">(</span>apps/v1<span class=\"token punctuation\">)</span> namespace: default\n<span class=\"token number\">8</span>:17:49PM: ---- applying complete <span class=\"token punctuation\">[</span><span class=\"token number\">2</span>/2 done<span class=\"token punctuation\">]</span> ----\n<span class=\"token number\">8</span>:17:49PM: ---- waiting complete <span class=\"token punctuation\">[</span><span class=\"token number\">2</span>/2 done<span class=\"token punctuation\">]</span> ----\nSucceeded</code></pre></div>\n<p>Our simple-app received a unique label kapp.k14s.io/app=1557433075084066000 for resource tracking:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp <span class=\"token function\">ls</span>\nTarget cluster <span class=\"token punctuation\">[</span><span class=\"token string\">'https://192.168.99.111:8443'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>https://192.168.99.111:8443<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nApps <span class=\"token keyword\">in</span> namespace <span class=\"token string\">'default'</span>\nName        Namespaces  Lcs   Lca\nsimple-app  default     <span class=\"token boolean\">true</span>  23s\n<span class=\"token number\">1</span> apps\nSucceeded</code></pre></div>\n<p>Using this label, kapp tracks and allows inspection of all Kubernetes resources created for sample-app:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp inspect <span class=\"token parameter variable\">-a</span> simple-app <span class=\"token parameter variable\">--tree</span>\nTarget cluster <span class=\"token string\">'https://192.168.99.111:8443'</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\nResources <span class=\"token keyword\">in</span> app <span class=\"token string\">'simple-app'</span>\nNamespace  Name                              Kind        Owner    Conds.  Rs  Ri  Age\ndefault    simple-app                        Deployment  kapp     <span class=\"token number\">2</span>/2 t   ok  -   46s\ndefault       simple-app-7fbc6b7c9b          ReplicaSet  cluster  -       ok  -   46s\ndefault         simple-app-7fbc6b7c9b-g92t7  Pod         cluster  <span class=\"token number\">4</span>/4 t   ok  -   46s\ndefault    simple-app                        Service     kapp     -       ok  -   46s\ndefault       simple-app                     Endpoints   cluster  -       ok  -   46s\nRs: Reconcile state\nRi: Reconcile information\n<span class=\"token number\">5</span> resources\nSucceeded</code></pre></div>\n<p>Note that it even knows about resources it did not directly create (such as ReplicaSet and Endpoints).</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kapp logs <span class=\"token parameter variable\">-f</span> <span class=\"token parameter variable\">-a</span> simple-app\nTarget cluster <span class=\"token string\">'https://192.168.99.111:8443'</span> <span class=\"token punctuation\">(</span>nodes: minikube<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># starting tailing 'simple-app-7fbc6b7c9b-g92t7 > simple-app' logs</span>\nsimple-app-7fbc6b7c9b-g92t7 <span class=\"token operator\">></span> simple-app <span class=\"token operator\">|</span> <span class=\"token number\">2020</span>/12/14 01:17:48 Server started</code></pre></div>\n<p>inspect and logs commands demonstrate why it's convenient to view resources in \"bulk\" (via a label). For example, logs command will tail any existing or new Pod that is part of simple-app application, even after we make changes and redeploy.</p>\n<h3 id=\"deploying-configuration-changes\" style=\"position:relative;\"><a href=\"#deploying-configuration-changes\" aria-label=\"deploying configuration changes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deploying configuration changes</h3>\n<p>Let's make a change to the application configuration to simulate a common occurrence in a development workflow. A simple observable change we can make is to change the value of the HELLO_MSG environment variable in <a href=\"https://github.com/carvel-dev/simple-app-on-kubernetes/blob/develop/config-step-1-minimal/config.yml\" target=\"_blank\" rel=\"noopener noreferrer\">config-step-1-minimal/config.yml</a>:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 626px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 22.941176470588236%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABHElEQVR42l2P2U7CUBRF+f9v8MVIUERARcYCQqkioxo0EUEGQUTmQim9LbpsUoPRh5WznvbexyW0FeKtw7r7wqrdtL2LZvvS9k2vjf7aYtF8Qhv0EdYWwzQdhIMQ4g8uMR1h5iU6Z4fUTw5QM2GGsQDPfjcf8QD9iI+Gb5++nGBrGpi6hrXR+RQb+67tEON/4JivogyVK4eSsvN5OsogespYCqErKabJCNNUmFEiRCt4xCyXtAt0hPm71DVRhxRaUSo9aUe5l6Dal8g8BokUPcSrXpT6Oeman2jJw0XezXFmj9iNF3W5wDItDMNZ6pquZ5Tnt9xptT/crx+4fi8gdxSyXYXSpEJ+WCTdyJBuZsm2c1zWZdSV6gT+vP4NADtm2E01WO8AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"alt text\" title=\"\" src=\"/static/0d16a6b8266dc46deaad6fd887b5b158/af590/change.png\" srcset=\"/static/0d16a6b8266dc46deaad6fd887b5b158/04472/change.png 170w,\n/static/0d16a6b8266dc46deaad6fd887b5b158/9f933/change.png 340w,\n/static/0d16a6b8266dc46deaad6fd887b5b158/af590/change.png 626w\" sizes=\"(max-width: 626px) 100vw, 626px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>and <strong>re-run kapp deploy command</strong>.</p>\n<p>The output should highlight several kapp features:</p>\n<p>kapp detected a single change to simple-app Deployment by comparing given local configuration against the live cluster copy</p>\n<p>kapp showed changes in a git-style diff via <code class=\"language-text\">--diff-changes</code> flag</p>\n<p>since simple-app Service was not changed in any way, it was not \"touched\" during the apply changes phase at all</p>\n<p>kapp waited for Pods associated with a Deployment to converge to their ready state before exiting successfully</p>\n<p>To double check that our change applied, go ahead and refresh your browser window with our deployed application.</p>\n<p>Given that kapp does not care where application configuration comes from, one can use it with any other tools that produce Kubernetes configuration, for example, Helm's template command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm template my-chart <span class=\"token parameter variable\">--values</span> values.yml <span class=\"token operator\">|</span> kapp deploy <span class=\"token parameter variable\">-a</span> my-app -f- <span class=\"token parameter variable\">--yes</span></code></pre></div>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<p><br><br></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time, „Å§„Å•„Åè üáµüá∏üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}