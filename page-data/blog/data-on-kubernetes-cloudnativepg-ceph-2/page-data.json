{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/data-on-kubernetes-cloudnativepg-ceph-2/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Data persistence: running PostgreSQL on Amazon EKS</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🎆 Introduction</h2>\n<p><a href=\"https://www.postgresql.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PostgreSQL</a>, the well-known open-source <a href=\"https://en.wikipedia.org/wiki/Relational_database\" target=\"_blank\" rel=\"noopener noreferrer\">relational database</a>, is a popular choice for applications demanding scalability, reliability, and ease of management.</p>\n<p>But how can we deploy and run PostgreSQL in a Kubernetes environment to handle the stored data? 🤔</p>\n<p>In this blog post, we'll explore how to combine <a href=\"https://github.com/cloudnative-pg/cloudnative-pg\" target=\"_blank\" rel=\"noopener noreferrer\">CloudNative-PG</a> (a PostgreSQL operator) and <a href=\"https://rook.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ceph Rook</a> (a storage orchestrator) to create a PostgreSQL cluster that scales easily, recovers from failures, and ensures data persistence — all within an <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Kubernetes Service EKS cluster</a>.</p>\n<h2 id=\"-databases-on-k8seks\" style=\"position:relative;\"><a href=\"#-databases-on-k8seks\" aria-label=\" databases on k8seks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>💥 Databases on K8S/EKS</h2>\n<p>Running databases and query engines on Kubernetes can offer benefits, but it's important to consider the trade-offs.</p>\n<h3 id=\"️-benefits-of-running-databases-on-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-benefits-of-running-databases-on-kubernetes\" aria-label=\"️ benefits of running databases on kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>☑️ Benefits of Running Databases on Kubernetes</h3>\n<ul>\n<li><strong>Automated deployment</strong>: Kubernetes simplifies deploying databases by automating the process and it aligns with DevOps practices and infrastructure as code.</li>\n<li><strong>Scaling</strong>: Kubernetes allows automatic scaling based on workload demands.</li>\n<li><strong>Rolling updates</strong>: It supports seamless updates without downtime.</li>\n<li><strong>Self-healing</strong>: Kubernetes ensures high availability by automatically recovering from failures.</li>\n<li><strong>Portability</strong>: Kubernetes provides a consistent environment across different clusters and cloud providers.</li>\n<li><strong>Cost-effectiveness</strong>: Kubernetes optimizes resource utilization.</li>\n</ul>\n<h3 id=\"️-challenges-and-considerations\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-challenges-and-considerations\" aria-label=\"️ challenges and considerations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>☑️ Challenges and Considerations</h3>\n<ul>\n<li><strong>Statefulness</strong>: Databases are stateful, and managing state in a transient environment (like Kubernetes pods) can be complex.</li>\n<li><strong>Transient pods</strong>: Pods can restart or fail over, affecting database availability.</li>\n<li><strong>Abstractions</strong>: Containerization introduces abstractions that impact database-specific tasks (e.g., backups, scaling).</li>\n<li><strong>DIY on VM</strong>: Full control but more operational overhead.</li>\n<li><a href=\"https://cloud.google.com/blog/products/databases/to-run-or-not-to-run-a-database-on-kubernetes-what-to-consider\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes: Closer to full control, but still requires careful planning</a>.</li>\n</ul>\n<h3 id=\"️-popular-databases-on-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-popular-databases-on-kubernetes\" aria-label=\"️ popular databases on kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>☑️ Popular Databases on Kubernetes</h3>\n<ul>\n<li><strong>Apache Cassandra</strong>: Scalable, distributed NoSQL database.</li>\n<li><strong>PostgreSQL</strong>: Supports Kubernetes through operators (e.g., cnpg).</li>\n<li><strong>MySQL, MongoDB, and others</strong>.</li>\n</ul>\n<blockquote>\n<p><a href=\"https://sealos.io/blog/to-run-or-not-to-run-a-database-on-kubernetes\" target=\"_blank\" rel=\"noopener noreferrer\">Many teams successfully run databases on Kubernetes in production, but it requires thoughtful planning and understanding of trade-offs</a>.</p>\n</blockquote>\n<h2 id=\"-deploying-cloudnative-pg-with-ceph-rook-on-aws-eks\" style=\"position:relative;\"><a href=\"#-deploying-cloudnative-pg-with-ceph-rook-on-aws-eks\" aria-label=\" deploying cloudnative pg with ceph rook on aws eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🚀 Deploying CloudNative-PG with Ceph Rook on AWS EKS</h2>\n<p><strong>CloudNativePG</strong> is an open-source <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\" target=\"_blank\" rel=\"noopener noreferrer\">Level 5 operator</a> designed to manage PostgreSQL workloads on any Kubernetes cluster. It simplifies the deployment, management, and recovery of PostgreSQL instances within Kubernetes. Here are some key features:</p>\n<ul>\n<li><strong>High availability</strong>: CloudNativePG covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication.</li>\n<li><strong>Automated failover</strong>: It automates tasks that a human operator would perform, including failover.</li>\n<li><strong>Data persistence</strong>: Unlike relying on stateful sets, CloudNativePG manages persistent volume claims for storing PGDATA.</li>\n<li><strong>Declarative configuration</strong>: It directly integrates with the Kubernetes API server, eliminating the need for an external failover management tool.</li>\n<li><strong>Cloud Native</strong>: Built on DevOps concepts like immutable infrastructure and declarative configuration, it leverages Kubernetes for self-healing, high availability, and more.</li>\n<li><strong>Security &#x26; TLS</strong>: Supports security contexts, encrypted TLS connections, and client authentication.</li>\n<li><strong>Monitoring</strong>: Includes a Prometheus exporter for metrics and transparent log integration.</li>\n<li><strong>Advanced architectures</strong>: Extend the architecture with PgBouncer or create disaster recovery clusters.</li>\n</ul>\n<blockquote>\n<p>CloudNativePG was originally developed by <a href=\"https://www.enterprisedb.com/\" target=\"_blank\" rel=\"noopener noreferrer\">EDB</a>, released under the Apache License 2.0, and submitted to CNCF Sandbox in April 2022.</p>\n</blockquote>\n<p><strong>Rook</strong> is an open-source cloud-native storage orchestrator designed for Kubernetes. Its purpose is to provide a platform, framework, and support for <a href=\"https://docs.ceph.com/en/latest/rados/\" target=\"_blank\" rel=\"noopener noreferrer\">Ceph storage</a> to seamlessly integrate with Kubernetes. Here are some key points about Rook:</p>\n<ul>\n<li><strong>Ceph integration</strong>: Rook enables Ceph, a distributed storage system that offers file, block, and object storage, to be deployed and managed within Kubernetes clusters.</li>\n<li><strong>Automation and management</strong>: Rook automates the deployment, configuration, provisioning, scaling, upgrading, and monitoring of Ceph. It ensures self-managing, self-scaling, and self-healing storage services.</li>\n<li><strong>Stability and compatibility</strong>: The status of the Ceph storage provider in Rook is stable. Upgrades between versions maintain backward compatibility.</li>\n<li><strong>CNCF project</strong>: Rook is hosted by the Cloud Native Computing Foundation (CNCF) as a graduated-level project.</li>\n</ul>\n<p>Now let's start the deployment process.</p>\n<h2 id=\"hands-on-demo\" style=\"position:relative;\"><a href=\"#hands-on-demo\" aria-label=\"hands on demo permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Hands-on Demo</h2>\n<p>First, you can get the code and clone the repo:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone git@github.com:seifrajhi/data-on-eks.git</code></pre></div>\n<p>After that, run the <code class=\"language-text\">deploy.sh</code> script which deploys an <strong>EKS cluster</strong> to the <code class=\"language-text\">eu-west-1</code> region.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">cd</span> distributed-databases/cloudnative-postgres/\n\n<span class=\"token function\">chmod</span> +x install.sh\n\n./install.sh</code></pre></div>\n<p>Once the script ends, you can run the below command to update the local kubeconfig so we can access the Kubernetes cluster:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">aws eks update-kubeconfig <span class=\"token parameter variable\">--name</span> cnpg-on-eks <span class=\"token parameter variable\">--region</span> eu-west-1</code></pre></div>\n<p>Next, you can verify if all the pods are running:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get pods <span class=\"token parameter variable\">-n</span><span class=\"token operator\">=</span>cnpg-system\nNAME                                          READY   STATUS    RESTARTS   AGE\ncnpg-on-eks-cloudnative-pg-412d5a5fd8-amuz   <span class=\"token number\">1</span>/1     Running   <span class=\"token number\">0</span>          11m</code></pre></div>\n<h3 id=\"️-setup-rook-for-storage\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-setup-rook-for-storage\" aria-label=\"️ setup rook for storage permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🏗️ Setup Rook for Storage</h3>\n<p>Rook has published the following Helm charts for the Ceph storage provider:</p>\n<ul>\n<li><a href=\"https://rook.io/docs/rook/latest-release/Helm-Charts/operator-chart/\" target=\"_blank\" rel=\"noopener noreferrer\">Rook Ceph Operator</a>: Starts the Ceph Operator, which will watch for Ceph CRs (custom resources).</li>\n<li><a href=\"https://rook.io/docs/rook/latest-release/Helm-Charts/ceph-cluster-chart/\" target=\"_blank\" rel=\"noopener noreferrer\">Rook Ceph Cluster</a>: Creates Ceph CRs that the operator will use to configure the cluster.</li>\n</ul>\n<h4 id=\"-rook-ceph-operator\" style=\"position:relative;\"><a href=\"#-rook-ceph-operator\" aria-label=\" rook ceph operator permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📦 Rook Ceph Operator</h4>\n<p>Installs Rook to create, configure, and manage Ceph clusters on Kubernetes.</p>\n<p>This chart bootstraps a <a href=\"https://github.com/rook/rook\" target=\"_blank\" rel=\"noopener noreferrer\">rook-ceph-operator</a> deployment on a Kubernetes cluster using the Helm package manager.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> rook-release https://charts.rook.io/release\nhelm <span class=\"token function\">install</span> --create-namespace <span class=\"token parameter variable\">--namespace</span> rook-ceph rook-ceph rook-release/rook-ceph <span class=\"token parameter variable\">-f</span> values.yaml</code></pre></div>\n<p>For customized settings, you can check the <a href=\"https://github.com/rook/rook/tree/release-1.14/deploy/charts/rook-ceph/values.yaml\" target=\"_blank\" rel=\"noopener noreferrer\">values.yaml</a> file.</p>\n<h4 id=\"️-ceph-cluster\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-ceph-cluster\" aria-label=\"️ ceph cluster permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🏗️ Ceph Cluster</h4>\n<p>Creates Rook resources to configure a Ceph cluster using the Helm package manager.</p>\n<p>Before installing, review the <code class=\"language-text\">values.yaml</code> to confirm if the default settings need to be updated.</p>\n<ul>\n<li>If the operator was installed in a namespace other than <code class=\"language-text\">rook-ceph</code>, the namespace must be set in the <code class=\"language-text\">operatorNamespace</code> variable.</li>\n<li>Set the desired settings in the <code class=\"language-text\">cephClusterSpec</code>. <a href=\"https://github.com/rook/rook/tree/release-1.14/deploy/charts/rook-ceph-cluster/values.yaml\" target=\"_blank\" rel=\"noopener noreferrer\">The defaults</a> are only an example and not likely to apply to your cluster.</li>\n<li>The monitoring section should be removed from the <code class=\"language-text\">cephClusterSpec</code>, as it is specified separately in the Helm settings.</li>\n<li>The default values for <code class=\"language-text\">cephBlockPools</code>, <code class=\"language-text\">cephFileSystems</code>, and <code class=\"language-text\">CephObjectStores</code> will create one of each, and their corresponding storage classes.</li>\n</ul>\n<p>All Ceph components now have default values for the pod resources. The resources may need to be adjusted in production clusters depending on the load. The resources can also be disabled if Ceph should not be limited (e.g., test clusters).</p>\n<p>The below command assumes you have first created a customized <code class=\"language-text\">values.yaml</code> file:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token function\">install</span> --create-namespace <span class=\"token parameter variable\">--namespace</span> rook-ceph rook-ceph-cluster <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">operatorNamespace</span><span class=\"token operator\">=</span>rook-ceph rook-release/rook-ceph-cluster <span class=\"token parameter variable\">-f</span> values.yaml</code></pre></div>\n<h4 id=\"-provision-storage\" style=\"position:relative;\"><a href=\"#-provision-storage\" aria-label=\" provision storage permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📦 Provision Storage</h4>\n<p>Before Rook can provision storage, a <a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes\" target=\"_blank\" rel=\"noopener noreferrer\">StorageClass</a> and <a href=\"https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/\" target=\"_blank\" rel=\"noopener noreferrer\">CephBlockPool</a> CR need to be created. This will allow Kubernetes to interoperate with Rook when provisioning persistent volumes.</p>\n<p>Save this StorageClass definition as <strong>storageclass.yaml</strong>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> ceph.rook.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> CephBlockPool\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> replicapool\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph <span class=\"token comment\"># namespace:cluster</span>\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">failureDomain</span><span class=\"token punctuation\">:</span> host\n  <span class=\"token key atrule\">replicated</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">size</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n    <span class=\"token key atrule\">requireSafeReplicaSize</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> storage.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> StorageClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph<span class=\"token punctuation\">-</span>block\n<span class=\"token key atrule\">provisioner</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph.rbd.csi.ceph.com <span class=\"token comment\"># csi-provisioner-name</span>\n<span class=\"token key atrule\">parameters</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">clusterID</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph \n  <span class=\"token key atrule\">pool</span><span class=\"token punctuation\">:</span> replicapool\n  <span class=\"token key atrule\">imageFormat</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"2\"</span>\n  <span class=\"token key atrule\">imageFeatures</span><span class=\"token punctuation\">:</span> layering\n  <span class=\"token key atrule\">csi.storage.k8s.io/provisioner-secret-name</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>csi<span class=\"token punctuation\">-</span>rbd<span class=\"token punctuation\">-</span>provisioner\n  <span class=\"token key atrule\">csi.storage.k8s.io/provisioner-secret-namespace</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph <span class=\"token comment\"># namespace:cluster</span>\n  <span class=\"token key atrule\">csi.storage.k8s.io/controller-expand-secret-name</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>csi<span class=\"token punctuation\">-</span>rbd<span class=\"token punctuation\">-</span>provisioner\n  <span class=\"token key atrule\">csi.storage.k8s.io/controller-expand-secret-namespace</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph <span class=\"token comment\"># namespace:cluster</span>\n  <span class=\"token key atrule\">csi.storage.k8s.io/node-stage-secret-name</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>csi<span class=\"token punctuation\">-</span>rbd<span class=\"token punctuation\">-</span>node\n  <span class=\"token key atrule\">csi.storage.k8s.io/node-stage-secret-namespace</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph <span class=\"token comment\"># namespace:cluster</span>\n  <span class=\"token key atrule\">csi.storage.k8s.io/fstype</span><span class=\"token punctuation\">:</span> ext4\n<span class=\"token key atrule\">allowVolumeExpansion</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n<span class=\"token key atrule\">reclaimPolicy</span><span class=\"token punctuation\">:</span> Delete</code></pre></div>\n<p>Then apply it</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> storageclass.yaml</code></pre></div>\n<h3 id=\"-deploy-a-postgresql-cluster\" style=\"position:relative;\"><a href=\"#-deploy-a-postgresql-cluster\" aria-label=\" deploy a postgresql cluster permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🚀 Deploy a PostgreSQL Cluster</h3>\n<p>As with any other deployment in Kubernetes, to deploy a PostgreSQL cluster you need to apply a configuration file that defines your desired Cluster. The CloudNativePG operator offers two types of bootstrapping a new database:</p>\n<ol>\n<li><strong>Bootstrap an empty cluster</strong></li>\n<li><strong>Bootstrap from another cluster</strong></li>\n</ol>\n<p>In this demo, we are going to create a new empty database cluster using <code class=\"language-text\">initdb</code> flags. We will use the template below by modifying the IAM role for IRSA configuration and the S3 bucket for the backup restore process and WAL archiving.</p>\n<p>The Terraform script has already created these resources. Use the Terraform output to extract these parameters:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">terraform output\ndemo_backup_irsa <span class=\"token operator\">=</span> <span class=\"token string\">\"arn:aws:iam::&lt;your_account_id>:role/cnpg-on-eks-prod-irsa\"</span>\ndemo_s3_bucket <span class=\"token operator\">=</span> <span class=\"token string\">\"xyz-cnpg-demo-bucket\"</span>\nconfigure_kubectl <span class=\"token operator\">=</span> <span class=\"token string\">\"aws eks --region eu-west-1 update-kubeconfig --name cnpg-on-eks\"</span></code></pre></div>\n<ul>\n<li><strong>IRSA Configuration</strong>: <code class=\"language-text\">arn:aws:iam::&lt;your_account_id>:role/cnpg-on-eks-prod-irsa</code></li>\n<li><strong>S3 Bucket</strong>: <code class=\"language-text\">xyz-cnpg-demo-bucket</code></li>\n</ul>\n<p>For more information on configuring IAM roles for service accounts (IRSA), refer to the <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS documentation</a>.</p>\n<p>For details on setting up S3 buckets for backups, check the <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS S3 documentation</a>.</p>\n<p>Now, we are ready to deploy a CloudNativePG database cluster.</p>\n<p><strong>demo-cnpg-cluster.yaml</strong>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> postgresql.cnpg.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Cluster\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> demo<span class=\"token punctuation\">-</span>cnpg\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> demo\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Cluster Demo for DoEKS\"</span>\n  <span class=\"token comment\"># Choose your PostGres Database Version</span>\n  <span class=\"token key atrule\">imageName</span><span class=\"token punctuation\">:</span> ghcr.io/cloudnative<span class=\"token punctuation\">-</span>pg/postgresql<span class=\"token punctuation\">:</span><span class=\"token number\">15.2</span>\n  <span class=\"token comment\"># Number of Replicas</span>\n  <span class=\"token key atrule\">instances</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n  <span class=\"token key atrule\">startDelay</span><span class=\"token punctuation\">:</span> <span class=\"token number\">300</span>\n  <span class=\"token key atrule\">stopDelay</span><span class=\"token punctuation\">:</span> <span class=\"token number\">300</span>\n  <span class=\"token key atrule\">replicationSlots</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">highAvailability</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">enabled</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">updateInterval</span><span class=\"token punctuation\">:</span> <span class=\"token number\">300</span>\n  <span class=\"token key atrule\">primaryUpdateStrategy</span><span class=\"token punctuation\">:</span> unsupervised\n  <span class=\"token key atrule\">serviceAccountTemplate</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># For backup and restore, we use IRSA for demo tool.</span>\n    <span class=\"token comment\"># You will find this IAM role on terraform outputs.</span>\n    <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">eks.amazonaws.com/role-arn</span><span class=\"token punctuation\">:</span> arn<span class=\"token punctuation\">:</span>aws<span class=\"token punctuation\">:</span>iam<span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span>&lt;&lt;account_id<span class=\"token punctuation\">></span><span class=\"token punctuation\">></span><span class=\"token punctuation\">:</span>role/cnpg<span class=\"token punctuation\">-</span>on<span class=\"token punctuation\">-</span>eks<span class=\"token punctuation\">-</span>prod<span class=\"token punctuation\">-</span>irsa <span class=\"token comment\">#1</span>\n  <span class=\"token key atrule\">postgresql</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">parameters</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">shared_buffers</span><span class=\"token punctuation\">:</span> 256MB\n      <span class=\"token key atrule\">pg_stat_statements.max</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'10000'</span>\n      <span class=\"token key atrule\">pg_stat_statements.track</span><span class=\"token punctuation\">:</span> all\n      <span class=\"token key atrule\">auto_explain.log_min_duration</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'10s'</span>\n    <span class=\"token key atrule\">pg_hba</span><span class=\"token punctuation\">:</span>\n      <span class=\"token comment\"># - hostssl app all all cert</span>\n      <span class=\"token punctuation\">-</span> host app app all password\n  <span class=\"token key atrule\">logLevel</span><span class=\"token punctuation\">:</span> debug\n  <span class=\"token key atrule\">storage</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">storageClass</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph<span class=\"token punctuation\">-</span>block\n    <span class=\"token key atrule\">size</span><span class=\"token punctuation\">:</span> 1Gi\n  <span class=\"token key atrule\">walStorage</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">storageClass</span><span class=\"token punctuation\">:</span> rook<span class=\"token punctuation\">-</span>ceph<span class=\"token punctuation\">-</span>block\n    <span class=\"token key atrule\">size</span><span class=\"token punctuation\">:</span> 1Gi\n  <span class=\"token key atrule\">monitoring</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">enablePodMonitor</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token key atrule\">bootstrap</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">initdb</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># Deploying a new cluster</span>\n      <span class=\"token key atrule\">database</span><span class=\"token punctuation\">:</span> WorldDB\n      <span class=\"token key atrule\">owner</span><span class=\"token punctuation\">:</span> app\n      <span class=\"token key atrule\">secret</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> app<span class=\"token punctuation\">-</span>auth\n  <span class=\"token key atrule\">backup</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">barmanObjectStore</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># For backup, we S3 bucket to store data.</span>\n    <span class=\"token comment\"># On this Blueprint, we create an S3 check the terraform output for it.</span>\n      <span class=\"token key atrule\">destinationPath</span><span class=\"token punctuation\">:</span> s3<span class=\"token punctuation\">:</span>//&lt;your<span class=\"token punctuation\">-</span>s3<span class=\"token punctuation\">-</span>demo<span class=\"token punctuation\">-</span>bucket<span class=\"token punctuation\">></span> <span class=\"token comment\">#2</span>\n      <span class=\"token key atrule\">s3Credentials</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">inheritFromIAMRole</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n      <span class=\"token key atrule\">wal</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">compression</span><span class=\"token punctuation\">:</span> gzip\n        <span class=\"token key atrule\">maxParallel</span><span class=\"token punctuation\">:</span> <span class=\"token number\">8</span>\n    <span class=\"token key atrule\">retentionPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"30d\"</span>\n\n  <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span> <span class=\"token comment\"># m5large: m5xlarge 2vCPU, 8GI RAM</span>\n    <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"512Mi\"</span>\n      <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1\"</span>\n    <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1Gi\"</span>\n      <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"2\"</span>\n\n  <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">enablePodAntiAffinity</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> failure<span class=\"token punctuation\">-</span>domain.beta.kubernetes.io/zone\n\n  <span class=\"token key atrule\">nodeMaintenanceWindow</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">inProgress</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n    <span class=\"token key atrule\">reusePVC</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span></code></pre></div>\n<p>Once updated, you can apply your template.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> demo-cnpg-cluster.yaml</code></pre></div>\n<p>Now you can check Cluster status is by using cloudnative-pg kubectl plugin offered by the CloudNativePG community.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl cnpg status demo-cnpg\nCluster Summary\nName:               demo-cnpg\nNamespace:          demo\nSystem ID:          <span class=\"token number\">7214866198623563798</span>\nPostgreSQL Image:   ghcr.io/cloudnative-pg/postgresql:15.2\nPrimary instance:   demo-cnpg-1\nStatus:             Cluster <span class=\"token keyword\">in</span> healthy state\nInstances:          <span class=\"token number\">3</span>\nReady instances:    <span class=\"token number\">3</span>\nCurrent Write LSN:  <span class=\"token number\">0</span>/6000000 <span class=\"token punctuation\">(</span>Timeline: <span class=\"token number\">1</span> - WAL File: 000000010000000000000005<span class=\"token punctuation\">)</span>\nCertificates Status\nCertificate Name  Expiration Date                Days Left Until Expiration\n----------------  ---------------                --------------------------\ndemo-cnpg-ca           <span class=\"token number\">2024</span>-07-14 <span class=\"token number\">14</span>:40:27 +0000 UTC  <span class=\"token number\">89.96</span>\ndemo-cnpg-replication  <span class=\"token number\">2024</span>-07-14 <span class=\"token number\">14</span>:40:27 +0000 UTC  <span class=\"token number\">89.96</span>\ndemo-cnpg-server       <span class=\"token number\">2024</span>-07-14 <span class=\"token number\">14</span>:40:27 +0000 UTC  <span class=\"token number\">89.96</span>\n\nContinuous Backup status\nFirst Point of Recoverability:  Not Available\nWorking WAL archiving:          OK\nWALs waiting to be archived:    <span class=\"token number\">0</span>\nLast Archived WAL:              000000010000000000000005   @   <span class=\"token number\">2024</span>-07-01T14:52:09.24307Z\nLast Failed WAL:                -\n\nStreaming Replication status\nReplication Slots Enabled\nName    Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\n----    --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\ndemo-cnpg-2  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000   00:00:00   00:00:00   00:00:00    streaming  async       <span class=\"token number\">0</span>              active\ndemo-cnpg-3  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000  <span class=\"token number\">0</span>/6000000   00:00:00   00:00:00   00:00:00    streaming  async       <span class=\"token number\">0</span>              active\n\nUnmanaged Replication Slot Status\nNo unmanaged replication slots found\n\nInstances status\nName    Database Size  Current LSN  Replication role  Status  QoS         Manager Version  Node\n----    -------------  -----------  ----------------  ------  ---         ---------------  ----\ndemo-cnpg-1  <span class=\"token number\">29</span> MB          <span class=\"token number\">0</span>/6000000    Primary           OK      BestEffort  <span class=\"token number\">1.19</span>.0           ip-10-1-10-111.eu-west-1.compute.internal\ndemo-cnpg-2  <span class=\"token number\">29</span> MB          <span class=\"token number\">0</span>/6000000    Standby <span class=\"token punctuation\">(</span>async<span class=\"token punctuation\">)</span>   OK      BestEffort  <span class=\"token number\">1.19</span>.0           ip-10-1-12-144.eu-west-1.compute.internal\ndemo-cnpg-3  <span class=\"token number\">29</span> MB          <span class=\"token number\">0</span>/6000000    Standby <span class=\"token punctuation\">(</span>async<span class=\"token punctuation\">)</span>   OK      BestEffort  <span class=\"token number\">1.19</span>.0           ip-10-1-11-78.eu-west-1.compute.internal</code></pre></div>\n<p>And there are more to discover like backup, monitoring, etc.</p>\n<h2 id=\"key-takeaways\" style=\"position:relative;\"><a href=\"#key-takeaways\" aria-label=\"key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Key takeaways</h2>\n<p>When running PostgreSQL on Amazon EKS, it's important to ensure data persistence in Kubernetes. Tools like CloudNativePG help manage highly available PostgreSQL clusters with native streaming replication, ensuring data durability and seamless failover.</p>\n<p><strong>Stay tuned for next blogs in this series🎉</strong></p>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://dok.community/blog/the-future-of-data-on-kubernetes\" target=\"_blank\" rel=\"noopener noreferrer\">https://dok.community/blog/the-future-of-data-on-kubernetes</a></li>\n<li><a href=\"https://www.reddit.com/r/kubernetes/comments/1dp062w/are_you_running_stateful_data_workloads_or/?sort=old\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.reddit.com/r/kubernetes/comments/1dp062w/are_you_running_stateful_data_workloads_or/?sort=old</a></li>\n<li>\n<iframe width=\"100%\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/99uSJXkKpeI?rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres</a></li>\n<li><a href=\"https://sealos.io/blog/to-run-or-not-to-run-a-database-on-kubernetes\" target=\"_blank\" rel=\"noopener noreferrer\">https://sealos.io/blog/to-run-or-not-to-run-a-database-on-kubernetes</a></li>\n<li><a href=\"https://rook.io/docs/rook/latest-release/Getting-Started/intro/\" target=\"_blank\" rel=\"noopener noreferrer\">https://rook.io/docs/rook/latest-release/Getting-Started/intro/</a></li>\n<li><a href=\"https://cloudnative-pg.io/documentation/1.22/\" target=\"_blank\" rel=\"noopener noreferrer\">https://cloudnative-pg.io/documentation/1.22/</a></li>\n</ul>\n<p><strong>Thank You 🖤</strong></p>\n<br>\n<p><strong><em>Until next time, つづく 🎉</em></strong></p>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <strong><em>Until next time 🎉</em></strong></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":9,"rawMarkdownBody":"\n> **Data persistence: running PostgreSQL on Amazon EKS**\n\n## 🎆 Introduction\n\n[PostgreSQL](https://www.postgresql.org/), the well-known open-source [relational database](https://en.wikipedia.org/wiki/Relational_database), is a popular choice for applications demanding scalability, reliability, and ease of management.\n\nBut how can we deploy and run PostgreSQL in a Kubernetes environment to handle the stored data? 🤔\n\nIn this blog post, we'll explore how to combine [CloudNative-PG](https://github.com/cloudnative-pg/cloudnative-pg) (a PostgreSQL operator) and [Ceph Rook](https://rook.io/) (a storage orchestrator) to create a PostgreSQL cluster that scales easily, recovers from failures, and ensures data persistence — all within an [Amazon Elastic Kubernetes Service EKS cluster](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html).\n\n## 💥 Databases on K8S/EKS\n\nRunning databases and query engines on Kubernetes can offer benefits, but it's important to consider the trade-offs.\n\n### ☑️ Benefits of Running Databases on Kubernetes\n\n- **Automated deployment**: Kubernetes simplifies deploying databases by automating the process and it aligns with DevOps practices and infrastructure as code.\n- **Scaling**: Kubernetes allows automatic scaling based on workload demands.\n- **Rolling updates**: It supports seamless updates without downtime.\n- **Self-healing**: Kubernetes ensures high availability by automatically recovering from failures.\n- **Portability**: Kubernetes provides a consistent environment across different clusters and cloud providers.\n- **Cost-effectiveness**: Kubernetes optimizes resource utilization.\n\n### ☑️ Challenges and Considerations\n\n- **Statefulness**: Databases are stateful, and managing state in a transient environment (like Kubernetes pods) can be complex.\n- **Transient pods**: Pods can restart or fail over, affecting database availability.\n- **Abstractions**: Containerization introduces abstractions that impact database-specific tasks (e.g., backups, scaling).\n- **DIY on VM**: Full control but more operational overhead.\n- [Kubernetes: Closer to full control, but still requires careful planning](https://cloud.google.com/blog/products/databases/to-run-or-not-to-run-a-database-on-kubernetes-what-to-consider).\n\n### ☑️ Popular Databases on Kubernetes\n\n- **Apache Cassandra**: Scalable, distributed NoSQL database.\n- **PostgreSQL**: Supports Kubernetes through operators (e.g., cnpg).\n- **MySQL, MongoDB, and others**.\n\n> [Many teams successfully run databases on Kubernetes in production, but it requires thoughtful planning and understanding of trade-offs](https://sealos.io/blog/to-run-or-not-to-run-a-database-on-kubernetes).\n\n## 🚀 Deploying CloudNative-PG with Ceph Rook on AWS EKS\n\n**CloudNativePG** is an open-source [Level 5 operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) designed to manage PostgreSQL workloads on any Kubernetes cluster. It simplifies the deployment, management, and recovery of PostgreSQL instances within Kubernetes. Here are some key features:\n\n- **High availability**: CloudNativePG covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication.\n- **Automated failover**: It automates tasks that a human operator would perform, including failover.\n- **Data persistence**: Unlike relying on stateful sets, CloudNativePG manages persistent volume claims for storing PGDATA.\n- **Declarative configuration**: It directly integrates with the Kubernetes API server, eliminating the need for an external failover management tool.\n- **Cloud Native**: Built on DevOps concepts like immutable infrastructure and declarative configuration, it leverages Kubernetes for self-healing, high availability, and more.\n- **Security & TLS**: Supports security contexts, encrypted TLS connections, and client authentication.\n- **Monitoring**: Includes a Prometheus exporter for metrics and transparent log integration.\n- **Advanced architectures**: Extend the architecture with PgBouncer or create disaster recovery clusters.\n\n> CloudNativePG was originally developed by [EDB](https://www.enterprisedb.com/), released under the Apache License 2.0, and submitted to CNCF Sandbox in April 2022.\n\n**Rook** is an open-source cloud-native storage orchestrator designed for Kubernetes. Its purpose is to provide a platform, framework, and support for [Ceph storage](https://docs.ceph.com/en/latest/rados/) to seamlessly integrate with Kubernetes. Here are some key points about Rook:\n\n- **Ceph integration**: Rook enables Ceph, a distributed storage system that offers file, block, and object storage, to be deployed and managed within Kubernetes clusters.\n- **Automation and management**: Rook automates the deployment, configuration, provisioning, scaling, upgrading, and monitoring of Ceph. It ensures self-managing, self-scaling, and self-healing storage services.\n- **Stability and compatibility**: The status of the Ceph storage provider in Rook is stable. Upgrades between versions maintain backward compatibility.\n- **CNCF project**: Rook is hosted by the Cloud Native Computing Foundation (CNCF) as a graduated-level project.\n\nNow let's start the deployment process.\n\n##  Hands-on Demo\n\nFirst, you can get the code and clone the repo:\n\n```shell\ngit clone git@github.com:seifrajhi/data-on-eks.git\n```\n\nAfter that, run the `deploy.sh` script which deploys an **EKS cluster** to the `eu-west-1` region.\n\n```shell\ncd distributed-databases/cloudnative-postgres/\n\nchmod +x install.sh\n\n./install.sh\n```\n\nOnce the script ends, you can run the below command to update the local kubeconfig so we can access the Kubernetes cluster:\n\n```shell\naws eks update-kubeconfig --name cnpg-on-eks --region eu-west-1\n```\n\nNext, you can verify if all the pods are running:\n\n```shell\nkubectl get pods -n=cnpg-system\nNAME                                          READY   STATUS    RESTARTS   AGE\ncnpg-on-eks-cloudnative-pg-412d5a5fd8-amuz   1/1     Running   0          11m\n```\n\n### 🏗️ Setup Rook for Storage\n\nRook has published the following Helm charts for the Ceph storage provider:\n\n- [Rook Ceph Operator](https://rook.io/docs/rook/latest-release/Helm-Charts/operator-chart/): Starts the Ceph Operator, which will watch for Ceph CRs (custom resources).\n- [Rook Ceph Cluster](https://rook.io/docs/rook/latest-release/Helm-Charts/ceph-cluster-chart/): Creates Ceph CRs that the operator will use to configure the cluster.\n\n#### 📦 Rook Ceph Operator\n\nInstalls Rook to create, configure, and manage Ceph clusters on Kubernetes.\n\nThis chart bootstraps a [rook-ceph-operator](https://github.com/rook/rook) deployment on a Kubernetes cluster using the Helm package manager.\n\n```shell\nhelm repo add rook-release https://charts.rook.io/release\nhelm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f values.yaml\n```\n\nFor customized settings, you can check the [values.yaml](https://github.com/rook/rook/tree/release-1.14/deploy/charts/rook-ceph/values.yaml) file.\n\n#### 🏗️ Ceph Cluster\n\nCreates Rook resources to configure a Ceph cluster using the Helm package manager.\n\nBefore installing, review the `values.yaml` to confirm if the default settings need to be updated.\n\n- If the operator was installed in a namespace other than `rook-ceph`, the namespace must be set in the `operatorNamespace` variable.\n- Set the desired settings in the `cephClusterSpec`. [The defaults](https://github.com/rook/rook/tree/release-1.14/deploy/charts/rook-ceph-cluster/values.yaml) are only an example and not likely to apply to your cluster.\n- The monitoring section should be removed from the `cephClusterSpec`, as it is specified separately in the Helm settings.\n- The default values for `cephBlockPools`, `cephFileSystems`, and `CephObjectStores` will create one of each, and their corresponding storage classes.\n\nAll Ceph components now have default values for the pod resources. The resources may need to be adjusted in production clusters depending on the load. The resources can also be disabled if Ceph should not be limited (e.g., test clusters).\n\nThe below command assumes you have first created a customized `values.yaml` file:\n\n```shell\nhelm install --create-namespace --namespace rook-ceph rook-ceph-cluster \\\n    --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster -f values.yaml\n```\n\n#### 📦 Provision Storage\n\nBefore Rook can provision storage, a [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes) and [CephBlockPool](https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/) CR need to be created. This will allow Kubernetes to interoperate with Rook when provisioning persistent volumes.\n\nSave this StorageClass definition as **storageclass.yaml**:\n\n```yaml\n---\napiVersion: ceph.rook.io/v1\nkind: CephBlockPool\nmetadata:\n  name: replicapool\n  namespace: rook-ceph # namespace:cluster\nspec:\n  failureDomain: host\n  replicated:\n    size: 3\n    requireSafeReplicaSize: true\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-ceph-block\nprovisioner: rook-ceph.rbd.csi.ceph.com # csi-provisioner-name\nparameters:\n  clusterID: rook-ceph \n  pool: replicapool\n  imageFormat: \"2\"\n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster\n  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster\n  csi.storage.k8s.io/fstype: ext4\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n```\n\nThen apply it\n\n```shell\nkubectl apply -f storageclass.yaml\n```\n### 🚀 Deploy a PostgreSQL Cluster\n\nAs with any other deployment in Kubernetes, to deploy a PostgreSQL cluster you need to apply a configuration file that defines your desired Cluster. The CloudNativePG operator offers two types of bootstrapping a new database:\n\n1. **Bootstrap an empty cluster**\n2. **Bootstrap from another cluster**\n\nIn this demo, we are going to create a new empty database cluster using `initdb` flags. We will use the template below by modifying the IAM role for IRSA configuration and the S3 bucket for the backup restore process and WAL archiving.\n\nThe Terraform script has already created these resources. Use the Terraform output to extract these parameters:\n\n```shell\nterraform output\ndemo_backup_irsa = \"arn:aws:iam::<your_account_id>:role/cnpg-on-eks-prod-irsa\"\ndemo_s3_bucket = \"xyz-cnpg-demo-bucket\"\nconfigure_kubectl = \"aws eks --region eu-west-1 update-kubeconfig --name cnpg-on-eks\"\n```\n\n- **IRSA Configuration**: `arn:aws:iam::<your_account_id>:role/cnpg-on-eks-prod-irsa`\n- **S3 Bucket**: `xyz-cnpg-demo-bucket`\n\nFor more information on configuring IAM roles for service accounts (IRSA), refer to the [AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html).\n\nFor details on setting up S3 buckets for backups, check the [AWS S3 documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html).\n\nNow, we are ready to deploy a CloudNativePG database cluster.\n\n**demo-cnpg-cluster.yaml**:\n\n```yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: demo-cnpg\n  namespace: demo\nspec:\n  description: \"Cluster Demo for DoEKS\"\n  # Choose your PostGres Database Version\n  imageName: ghcr.io/cloudnative-pg/postgresql:15.2\n  # Number of Replicas\n  instances: 3\n  startDelay: 300\n  stopDelay: 300\n  replicationSlots:\n    highAvailability:\n      enabled: true\n    updateInterval: 300\n  primaryUpdateStrategy: unsupervised\n  serviceAccountTemplate:\n    # For backup and restore, we use IRSA for demo tool.\n    # You will find this IAM role on terraform outputs.\n    metadata:\n      annotations:\n        eks.amazonaws.com/role-arn: arn:aws:iam::<<account_id>>:role/cnpg-on-eks-prod-irsa #1\n  postgresql:\n    parameters:\n      shared_buffers: 256MB\n      pg_stat_statements.max: '10000'\n      pg_stat_statements.track: all\n      auto_explain.log_min_duration: '10s'\n    pg_hba:\n      # - hostssl app all all cert\n      - host app app all password\n  logLevel: debug\n  storage:\n    storageClass: rook-ceph-block\n    size: 1Gi\n  walStorage:\n    storageClass: rook-ceph-block\n    size: 1Gi\n  monitoring:\n    enablePodMonitor: true\n  bootstrap:\n    initdb: # Deploying a new cluster\n      database: WorldDB\n      owner: app\n      secret:\n        name: app-auth\n  backup:\n    barmanObjectStore:\n    # For backup, we S3 bucket to store data.\n    # On this Blueprint, we create an S3 check the terraform output for it.\n      destinationPath: s3://<your-s3-demo-bucket> #2\n      s3Credentials:\n        inheritFromIAMRole: true\n      wal:\n        compression: gzip\n        maxParallel: 8\n    retentionPolicy: \"30d\"\n\n  resources: # m5large: m5xlarge 2vCPU, 8GI RAM\n    requests:\n      memory: \"512Mi\"\n      cpu: \"1\"\n    limits:\n      memory: \"1Gi\"\n      cpu: \"2\"\n\n  affinity:\n    enablePodAntiAffinity: true\n    topologyKey: failure-domain.beta.kubernetes.io/zone\n\n  nodeMaintenanceWindow:\n    inProgress: false\n    reusePVC: false\n```\n\nOnce updated, you can apply your template.\n\n```shell\nkubectl apply -f demo-cnpg-cluster.yaml\n```\n\nNow you can check Cluster status is by using cloudnative-pg kubectl plugin offered by the CloudNativePG community.\n\n```shell\n$ kubectl cnpg status demo-cnpg\nCluster Summary\nName:               demo-cnpg\nNamespace:          demo\nSystem ID:          7214866198623563798\nPostgreSQL Image:   ghcr.io/cloudnative-pg/postgresql:15.2\nPrimary instance:   demo-cnpg-1\nStatus:             Cluster in healthy state\nInstances:          3\nReady instances:    3\nCurrent Write LSN:  0/6000000 (Timeline: 1 - WAL File: 000000010000000000000005)\nCertificates Status\nCertificate Name  Expiration Date                Days Left Until Expiration\n----------------  ---------------                --------------------------\ndemo-cnpg-ca           2024-07-14 14:40:27 +0000 UTC  89.96\ndemo-cnpg-replication  2024-07-14 14:40:27 +0000 UTC  89.96\ndemo-cnpg-server       2024-07-14 14:40:27 +0000 UTC  89.96\n\nContinuous Backup status\nFirst Point of Recoverability:  Not Available\nWorking WAL archiving:          OK\nWALs waiting to be archived:    0\nLast Archived WAL:              000000010000000000000005   @   2024-07-01T14:52:09.24307Z\nLast Failed WAL:                -\n\nStreaming Replication status\nReplication Slots Enabled\nName    Sent LSN   Write LSN  Flush LSN  Replay LSN  Write Lag  Flush Lag  Replay Lag  State      Sync State  Sync Priority  Replication Slot\n----    --------   ---------  ---------  ----------  ---------  ---------  ----------  -----      ----------  -------------  ----------------\ndemo-cnpg-2  0/6000000  0/6000000  0/6000000  0/6000000   00:00:00   00:00:00   00:00:00    streaming  async       0              active\ndemo-cnpg-3  0/6000000  0/6000000  0/6000000  0/6000000   00:00:00   00:00:00   00:00:00    streaming  async       0              active\n\nUnmanaged Replication Slot Status\nNo unmanaged replication slots found\n\nInstances status\nName    Database Size  Current LSN  Replication role  Status  QoS         Manager Version  Node\n----    -------------  -----------  ----------------  ------  ---         ---------------  ----\ndemo-cnpg-1  29 MB          0/6000000    Primary           OK      BestEffort  1.19.0           ip-10-1-10-111.eu-west-1.compute.internal\ndemo-cnpg-2  29 MB          0/6000000    Standby (async)   OK      BestEffort  1.19.0           ip-10-1-12-144.eu-west-1.compute.internal\ndemo-cnpg-3  29 MB          0/6000000    Standby (async)   OK      BestEffort  1.19.0           ip-10-1-11-78.eu-west-1.compute.internal\n```\n\nAnd there are more to discover like backup, monitoring, etc.\n\n## Key takeaways\n\nWhen running PostgreSQL on Amazon EKS, it's important to ensure data persistence in Kubernetes. Tools like CloudNativePG help manage highly available PostgreSQL clusters with native streaming replication, ensuring data durability and seamless failover.\n\n**Stay tuned for next blogs in this series🎉**\n\n**References:**\n\n- https://dok.community/blog/the-future-of-data-on-kubernetes\n- https://www.reddit.com/r/kubernetes/comments/1dp062w/are_you_running_stateful_data_workloads_or/?sort=old\n- https://www.youtube.com/watch?v=99uSJXkKpeI\n- https://awslabs.github.io/data-on-eks/docs/blueprints/distributed-databases/cloudnative-postgres\n- https://sealos.io/blog/to-run-or-not-to-run-a-database-on-kubernetes\n- https://rook.io/docs/rook/latest-release/Getting-Started/intro/\n- https://cloudnative-pg.io/documentation/1.22/\n\n**Thank You 🖤**\n\n<br>\n\n**_Until next time, つづく 🎉_**\n\n> 💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  **_Until next time 🎉_**\n\n🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**♻️ LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**♻️ X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ✌🏻**\n\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n\n**📅 Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1256},"frontmatter":{"id":"85258584758902d46b7ccad8","path":"/blog/data-on-kubernetes-cloudnativepg-ceph-2/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Data on Kubernetes: Part 2 - Deploying Databases in K8s with PostgreSQL, CloudNative-PG, and Ceph Rook on Amazon EKS","keywords":["Ceph Rook","Kubernetes","PostgreSQL","AWS EKS","CloudNative-PG"],"excerpt":"EKS, Databases, Cloud Native DB, CloudNative-PG, PostgreSQL, Ceph Rook","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC5klEQVR42i2T20/TZxjHe+sf4F/gjUu8MMa4ZBdL9NbEay+MJnLjWKIiBhEHApsFEUTogVPbYS2lLT0xoIMW2pV2gOF8KMeonFRCjbIN29KWfvb0t128eX/vk+/heb/P+1OZJw54ap9AM7BKe3CbtpFtWoc30f6+RnP/irLrBtdpCbyj0TOPIfwBU2SP2u4J9ENvMI5+EM4WttkUtrk0KsdChgqDn3sNdh619vHEEqFGwI+NASo6Bnmg8VL1a5Di5w4e6vuUet4oj6l3zfBYuPlatwja84Km0T2qO4OKUKm2VyHnQaVar3QxTnGjg/KOIZr6llF3hSnVeaixiaFhiF/MISpNw8KN0hrYFMGMCEb2+anNR0mzE2P4vQiGKGly/t+hX6k/sYSptY3JTYYUgZ/NYao6Q0rHRXVdYvKbxLWDfV4EXbEc5okvco012T8rGer9b+kc21dyqxZimRDLWvqlyyUlP+Nofn1UvnWDb2gRvH3u6L8Oe5ay9G3BgKx8nk45O2PH4pbGvYJiovGt0RnZkXNWMFl6FrN4Y1l6BeddzuFcPqZ76RibcFTGkU+UVkeobJ7EOp2USR3hkNWzkMYxf4R7KYNnJUdjKM2r10ncsTTWmRRtr/9BM/4XpumEcJK4/jwQwRSqivpJvjlxm3OnijGE4rjE1TyZpMi6T63vAMt0mkutWW62f+XG/W1e9H/FNHvIVc86hb5NCvy7ROv/YOtiFdGnQVRldVFOnyzk/NkiGt0beFfBOJbgun6XSk+cqkCWKx1Z4h8TdNs/UaiO07WS5AfJ6MeBTUrC+4yrA+xefsaUehjVtQIz587c4cK3t3lmX1Ry64j+TYP/C22jB9QFU3zXlEMXynDXmkLtOqRrIcGt/rdc965THtyjfXibl8YxdL4NVOX6CI+0IR688FNjnZOh5LBMJZSHaplKKTnWjKT5XnfMLbcMLZbBMnmIdmQHXXAXQzROg/xBas8c9Z55/gVW96hTD1rWiwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/03e3b164103eddf566043988de083244/e5567/dok2.png","srcSet":"/static/03e3b164103eddf566043988de083244/ba276/dok2.png 750w,\n/static/03e3b164103eddf566043988de083244/f0fea/dok2.png 1080w,\n/static/03e3b164103eddf566043988de083244/dc894/dok2.png 1366w,\n/static/03e3b164103eddf566043988de083244/e5567/dok2.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/03e3b164103eddf566043988de083244/5436b/dok2.webp 750w,\n/static/03e3b164103eddf566043988de083244/25d47/dok2.webp 1080w,\n/static/03e3b164103eddf566043988de083244/906fd/dok2.webp 1366w,\n/static/03e3b164103eddf566043988de083244/bbc11/dok2.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.55375}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/data-on-kubernetes-ai-ml-5/","title":"Data on Kubernetes: Part 5 - Making AI/ML simpler","date":"2024-10-25 13:06:00"},"excerpt":"Using Kubernetes to deploy and scale AI/ML models efficiently 📈 📚 Introduction Welcome back to our 'Data on Kubernetes' series. In this…","html":"<blockquote>\n<p><strong>Using Kubernetes to deploy and scale AI/ML models efficiently 📈</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📚 Introduction</h2>\n<p>Welcome back to our 'Data on Kubernetes' series. In this fifth part, we're focusing on AI and machine learning (ML). We'll discuss how Kubernetes is transforming the use of <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a> software.</p>\n<p>The more we rely on <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a>, the more complex it becomes to manage these systems. Kubernetes offers a helping hand with features that allow AI/ML software to automatically adjust their size, update without stopping, and repair themselves. This keeps AI/ML systems always on and working smoothly.</p>\n<p>In this blog, we'll discover the tools <a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, and their integration with Kubernetes to simplify the process, all within the AWS cloud ecosystem.</p>\n<h2 id=\"-kubernetes-making-aiml-easier\" style=\"position:relative;\"><a href=\"#-kubernetes-making-aiml-easier\" aria-label=\" kubernetes making aiml easier permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🚀 Kubernetes: Making AI/ML Easier</h2>\n<p>When we use AI and ML, things can get pretty tricky. It's exciting but can also be a bit overwhelming with all the pieces you need to keep track of and there's a lot to manage. That's exactly why Kubernetes comes in — it's like having a smart assistant that helps keep everything in order.</p>\n<p>Kubernetes is great because it can fix itself. If something goes wrong with an AI program, Kubernetes doesn't wait around; it jumps right in to fix the problem.</p>\n<p>This means if an AI program stops working, Kubernetes doesn't just stand by. It jumps into action, fixing things up or starting fresh so that there's no downtime. This means you can rely on your AI and ML to work non-stop.</p>\n<p>But that's not all. Kubernetes is also smart about using resources. Sometimes AI needs more resources to work on big tasks, and sometimes it needs less.</p>\n<p>Kubernetes is smart enough to adjust on the fly, giving them more resources when they're busy and scaling back when they're not. This way, you're not wasting any energy or money, and everything runs super efficiently.</p>\n<h3 id=\"️-ray-and-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-ray-and-kubernetes\" aria-label=\"️ ray and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🛠️ Ray and Kubernetes</h3>\n<p>When working with AI and ML tasks, we require tools that can manage extensive workloads efficiently.</p>\n<p><a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> is such a tool, designed for <a href=\"https://www.python.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Python</a>, a widely-used programming language. It simplifies the execution of large-scale projects on multiple computers simultaneously.</p>\n<p>Ray serves as a comprehensive toolkit for deep learning, an AI branch that equips computers with cognitive abilities.</p>\n<p><a href=\"https://ray-project.github.io/q4-2021-docs-hackathon/0.4/ray-overview/what-and-why-ray/\" target=\"_blank\" rel=\"noopener noreferrer\">Developed by experts at UC Berkeley</a>, Ray's objective is to streamline large computing projects. It consists of Ray Core, the primary component for distributing tasks across computers, and Ray Ecosystem, a suite of specialized tools for various purposes, including project optimization and decision-making algorithms.</p>\n<p>While similar to Dask, a tool for executing Python code concurrently in different locations and processing multiple data-related tasks simultaneously, Ray stands out.</p>\n<p>It doesn't merely replicate existing tools like NumPy and Pandas; it's engineered to be versatile, catering to a wide array of projects beyond data science. Its adaptability has led major projects such as spaCy, Hugging Face, and XGBoost to incorporate Ray, enhancing their efficiency and intelligence.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAChElEQVR42mNgYGAQio+PMsjMzNcEsrmAmB2IOczszWRMdE2UzFSkZUyUlWX1xMW5geJsQMwDxAIMMhacDAwynPX19TJd9SUSxgwMrGD5/3/+ZN24dHrt6Yu3Th179Iiz9Pp/KQaGJN77axVnPTpgHciQ9F9Ktnm5snLVCiOZK/+FbMTYndmZGBJmhq6S6xROktq9Z8/RPXuOrNxz+1vVyRc/ihiePHn2euLlj1Yu847p6m18rhjcttrHKS5P78ia7OorF9bXfv32Y+fNV+/2zDn/9EDKyf8J9s7OHmYiPLahfnPU45ymS5+/eKPg8amydRuXlyxvnrCohQEMzOr5GIwjRRiSN4oDecxaWlpCTR3T27dv37P51fWzjy/cuf9q4em7jzceuxZ/bMOcpqubZueJWxaLWViAvM3A8GQVw+T5jca3/IPSZjHsPnN77e4zN1ftP3dnxY7Tt9btPXtn9Z5z99bsPXNz9dKjN9ZN2HV5/ab9pzZsO3Vzw65jl9fvOnx2345jF/duO3pp7ebDF9cA8frNh6+s2Xnq1obNhy9PYrj9/v//a6/+/Lv++u//G6///L/8/Me/Mw8+/7rx7Ou/g3c+fl184e33Pecf/jn36OPfk7ef/zh6/en3k7fffjt1583v03ff/gdhIP/v3Y///198/PUmw/Slm1VBeNbyrWr989crLN+82+Hk8ePdbZuOuGnverrNaO/Tyoe9kXFTmuaGFMx53lM752rN3Llz9yxfvbFh2a6zUos37FWft2YXWP+UJVvkGWAAGP1MIPrA8eOa9y8ebYw/8dyb8+D3hZyHf2WvW6DtZldxxDJ9yueOrElPc+bMmLx69ep1Gcj6YAAASclQlNbtCaIAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ray\" title=\"\" src=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png\" srcset=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/04472/ray.png 170w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/9f933/ray.png 340w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png 680w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b12f7/ray.png 1020w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b5a09/ray.png 1360w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/2cefc/ray.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead\">Source</a></div>\n<p>The diagram above shows that at a high level, the Ray ecosystem consists of:</p>\n<ul>\n<li>The core Ray system</li>\n<li>Scalable libraries for machine learning (both native and third party)</li>\n<li>Tools for <a href=\"https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208\" target=\"_blank\" rel=\"noopener noreferrer\">launching clusters on any cluster or cloud provider</a></li>\n</ul>\n<p>KubeRay amplifies Ray's capabilities by integrating it with Kubernetes, a platform that orchestrates numerous computers working in unison. This integration combines Ray's user-friendly Python interface with Kubernetes' robust and dependable framework.</p>\n<p>The result is a great system capable of developing, operating, and maintaining large-scale projects easily. It's akin to an advanced system that scales with your requirements, self-repairs, and autonomously performs maintenance and updates.</p>\n<p>The use of Kubernetes is advantageous for Ray. It acts as a command center for computers, ensuring smooth operation and task management. Kubernetes excels with both tools, adeptly managing numerous tasks without confusion.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABdElEQVR42lWR2Y6bQBBF+f9PykNe82ZrNmuc8Tjx2INNQ4MBs/V2Ug0ZK2npiKu69O1SVeKcw3l/x4fAZAxKZeR5LhRkSjGO4+z9+29kHAfS9JNCa4L4SZBAxFhwM0FQVUMhoefPE3VVIsW7fyc4+n7g8HFC62IJzDrH+WYXOs+ld2S953gT/ddTQ6z7uZ4PC1mkt0wuoK4D4zQRT3KUkJ+7PW+/P8hG2Mtrh/RC2kqQrmYKXfKr7DnmJanKhYJzrtnXBus8qhloTJCOpcPX2vFSDGzKiafSzvpFj6yynh+7s5DylLU8aMNKjazVcOe5NNJT4PH9JiNadLKrLc+VhF6FSgKvntcWHiTk++Mb39Zb1qJjLT6+bb0Q2NSeXeOw1nLJFO2tW2YYt+zjducNg201Q6O51IHVwRCMDH2T0rU129PE/lRTlTlN284B8V48UcesJIr/cAZnDaPM5NrZWXd1L19LI0vpBos109zZ150Y+qX/APzJZbQoEulJAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay\" title=\"\" src=\"/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png\" srcset=\"/static/a78dd992dd6340928c4606a616df4d61/04472/kuberay.png 170w,\n/static/a78dd992dd6340928c4606a616df4d61/9f933/kuberay.png 340w,\n/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png 680w,\n/static/a78dd992dd6340928c4606a616df4d61/b12f7/kuberay.png 1020w,\n/static/a78dd992dd6340928c4606a616df4d61/b5a09/kuberay.png 1360w,\n/static/a78dd992dd6340928c4606a616df4d61/29007/kuberay.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\">Source</a></div>\n<h3 id=\"-pytorch-and-kubernetes\" style=\"position:relative;\"><a href=\"#-pytorch-and-kubernetes\" aria-label=\" pytorch and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🔥 PyTorch and Kubernetes</h3>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> is an open-source machine learning library for Python, widely used for applications such as natural language processing, computer vision, and deep learning.</p>\n<p>What makes PyTorch special is its ease of use, flexibility, and dynamic computational graph, which allows for quick prototyping and experimentation. Researchers like it because it's designed to work well with Python and makes deep learning straightforward.</p>\n<h4 id=\"pytorch-and-tensorflow\" style=\"position:relative;\"><a href=\"#pytorch-and-tensorflow\" aria-label=\"pytorch and tensorflow permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch and TensorFlow</h4>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> and <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a> are both strong in deep learning, but they're good at different things. TensorFlow, made by Google, is great for getting big machine learning models ready for use and for training them on many computers at once.</p>\n<p>PyTorch is more about being easy to work with and changing things as you go, which is really helpful when you're still figuring things out.</p>\n<h4 id=\"pytorch-with-ray-and-kuberay\" style=\"position:relative;\"><a href=\"#pytorch-with-ray-and-kuberay\" aria-label=\"pytorch with ray and kuberay permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch with Ray and KubeRay</h4>\n<p>When PyTorch works together with Ray or KubeRay, it gets even better at deep learning jobs. Ray and KubeRay help spread out the work over many computers, making things faster and more reliable.</p>\n<p>This integration facilitates the distribution of computational tasks across multiple nodes, enabling faster processing times and resilience against individual node failures.</p>\n<p>They also make better use of cloud services, saving money. This combination means developers can spend more time being creative with their machine learning models, while the tough tech stuff is taken care of.</p>\n<h4 id=\"pytorch-meets-kubernetes\" style=\"position:relative;\"><a href=\"#pytorch-meets-kubernetes\" aria-label=\"pytorch meets kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch meets Kubernetes</h4>\n<p>By using PyTorch with Kubernetes (also known as k8s), you get all the perks of Kubernetes' smart way of managing containers. Kubernetes helps set up, grow, and manage containers over lots of computers.</p>\n<p>This means PyTorch applications can handle big, complicated jobs better, grow more easily, and keep running smoothly. Developers can then put more energy into making their models, without worrying too much about the tech behind it.</p>\n<h2 id=\"️-hands-on-ray-on-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-hands-on-ray-on-kubernetes\" aria-label=\"️ hands on ray on kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🛠️ Hands-on: Ray on Kubernetes</h2>\n<p>Our setup consists of a Kubernetes cluster in Amazon EKS, which hosts our AI applications. The recommended method for installing Ray onto a Kubernetes cluster is through KubeRay, a Kubernetes Operator. The KubeRay Operator allows you to create and manage Ray clusters in a Kubernetes-native way by defining clusters as a custom resource called a RayCluster.</p>\n<p>The installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob, and RayService as documented <a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n<p>This RayCluster resource describes the desired state of a Ray cluster. The KubeRay Operator manages the cluster lifecycle, autoscaling, and monitoring functions.</p>\n<p>Hence, we use the KubeRay Operator for our Ray cluster installation.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 63.52941176470588%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABYUlEQVR42pWSi46CQAxF+f8/JBEkJoSHyhsVedQ5XcvK6ia7k1zHdNrTzh08Ebk53f+raZpU8zzfWc/44LmfWf6w8jyX/X4vURTpnqapHI9HybJM6rqW2425ZNkAXTc9qKpKk9q2FZoTL8tSLpeLXK9XjQM7n89yOp2kaZrPQBJJ2u12EgSBTgK4KAo9W5ZlMzVwmrzE34FMM46jiuloYNOgrutkGAa9ahzHkiSJnhN7A/Z9r12ZChjFeGUwfATAVFyTPEDucWzKbyABAAZB+MMUNKKQnRv8XJsrO/pMBxJ5DIrwjB3RgMkQNpBrltCEm5gc4wvIFRBTGcxeHQtohIADZJFHE/xj5ytYgXiBJwbEcF45DEMtMOM5N884831fDoeDAqlZgbwuQKagk02CaEbMPh97fWoAcTOaUb8CzUNk/zEaWdxEjBx7KIB81M/axXMJhbOkepU7XPXb2ae4Y5UPzzXwRaCIkHEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay operator\" title=\"\" src=\"/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png\" srcset=\"/static/0fe3a262cf7937c74bc1df905770e678/04472/kuberay-op.png 170w,\n/static/0fe3a262cf7937c74bc1df905770e678/9f933/kuberay-op.png 340w,\n/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png 680w,\n/static/0fe3a262cf7937c74bc1df905770e678/b12f7/kuberay-op.png 1020w,\n/static/0fe3a262cf7937c74bc1df905770e678/c61d0/kuberay-op.png 1145w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/sage-ai/demystifying-the-process-of-building-a-ray-cluster-110c67914a99\">Source</a></div>\n<p>KubeRay Operator managing the Ray Cluster lifecycle. Now let us deploy the infrastructure. Start by cloning the repo and change the working directory.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/data-on-eks\n<span class=\"token builtin class-name\">cd</span> data-on-eks/ai-ml/ray/terraform</code></pre></div>\n<p>Next, we can use the shell script <code class=\"language-text\">install.sh</code> to run the <code class=\"language-text\">terraform init</code> and <code class=\"language-text\">apply</code> commands.</p>\n<p>Update <code class=\"language-text\">variables.tf</code> to change the region. Also, we can update any other input variables or make any other changes to the terraform template.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">./install.sh</code></pre></div>\n<p>And now, we can run the PyTorch benchmark.</p>\n<p>We deploy a Ray Cluster with its own configuration for Karpenter workers. Different jobs can have different requirements for Ray Cluster such as a different version of Ray libraries or EC2 instance configuration such as making use of Spot market or GPU instances.</p>\n<p>To deploy the Ray cluster run below commands:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">cd</span> examples/pytorch\nterraform init\nterraform plan\nterraform apply -auto-approve</code></pre></div>\n<p>Once running, we can forward the port for the server:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl port-forward service/pytorch-kuberay-head-svc <span class=\"token parameter variable\">-n</span> pytorch <span class=\"token number\">8266</span>:8265</code></pre></div>\n<p>We can then submit the job for PyTorch benchmark workload:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">python job/pytorch_submit.py</code></pre></div>\n<p>You can open <a href=\"http://localhost:8266\" target=\"_blank\" rel=\"noopener noreferrer\">http://localhost:8266</a> to monitor the progress of the PyTorch benchmark.</p>\n<p>The <code class=\"language-text\">pytorch_submit.py</code> script is a benchmarking for evaluating the performance of training and tuning a PyTorch model on a distributed system using Ray.</p>\n<p>It measures the time taken to train a model and to find the best hyperparameters through tuning, providing insights into the efficiency of distributed machine learning workflows.</p>\n<h2 id=\"-key-takeaways\" style=\"position:relative;\"><a href=\"#-key-takeaways\" aria-label=\" key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🔚 Key Takeaways</h2>\n<p>Kubernetes makes working with AI/ML simpler. It scales AI/ML models easily and keeps them running smoothly. With Kubernetes, Ray, and PyTorch work better in the cloud, making AI/ML systems more reliable and easier to manage.</p>\n<br>\n<br>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <strong><em>Until next time 🎉</em></strong></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📻🧡 References:</strong></p>\n<ul>\n<li><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://docs.ray.io/en/latest/cluster/kubernetes/index.html</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws\" target=\"_blank\" rel=\"noopener noreferrer\">https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay\" target=\"_blank\" rel=\"noopener noreferrer\">https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay</a></li>\n</ul>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/unsung-kubernetes-features-reliable-resilient/","title":"Unsung Kubernetes Features that Keep Kubernetes Clusters Reliable and Resilient","date":"2024-10-24 22:36:00"},"excerpt":"Explore the lesser-known features that keep Kubernetes high available ⭐️ Introduction Ensuring high availability and reliability in…","html":"<blockquote>\n<p><strong>Explore the lesser-known features that keep Kubernetes high available</strong></p>\n</blockquote>\n<h2 id=\"️-introduction\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-introduction\" aria-label=\"️ introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>⭐️ Introduction</h2>\n<p>Ensuring high availability and reliability in Kubernetes clusters can be a challenge. While some features like Deployments and Services are well-known for their ability to maintain availability, there are several lesser-known features that play an important role in keeping Kubernetes clusters running smoothly. In this article, we'll talk about some of these unsung heroes, including <strong>Pod Topology Spread Constraints</strong>, <strong>pod-deletion-cost</strong>, <strong>maxUnavailable</strong>, <strong>Pod Disruption Budgets</strong>, <strong>PriorityClasses</strong>, and <strong>Pod Anti-Affinity</strong>.</p>\n<h2 id=\"pod-topology-spread-constraints\" style=\"position:relative;\"><a href=\"#pod-topology-spread-constraints\" aria-label=\"pod topology spread constraints permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Pod Topology Spread Constraints</h2>\n<p>Pod Topology Spread Constraints in Kubernetes allow for fine-grained control over the distribution of pods across failure domains, such as regions and zones, to achieve high availability and efficient resource utilization. These constraints use labels to enforce specific distribution patterns, ensuring that workloads are evenly distributed and separated by topology. By preventing single points of failure, they enhance fault-tolerance, improve performance, and optimize resource utilization.</p>\n<p>For example, the <code class=\"language-text\">maxSkew</code> parameter defines the degree to which pods may be unevenly distributed, while the <code class=\"language-text\">minDomains</code> parameter forces spreading pods over a minimum number of domains. These constraints complement other Kubernetes scheduling policies, such as Pod affinity/anti-affinity, Node Selector, Node Affinity, and Node Taints, and are an important asset in deploying and running efficient and highly available workloads in complex, distributed environments.</p>\n<p>Here's an example of a pod topology spread constraint:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> example<span class=\"token punctuation\">-</span>pod\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">topologySpreadConstraints</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">maxSkew</span><span class=\"token punctuation\">:</span> &lt;integer<span class=\"token punctuation\">></span>\n            <span class=\"token key atrule\">minDomains</span><span class=\"token punctuation\">:</span> &lt;integer<span class=\"token punctuation\">></span> <span class=\"token comment\"># optional</span>\n            <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> &lt;string<span class=\"token punctuation\">></span>\n            <span class=\"token key atrule\">whenUnsatisfiable</span><span class=\"token punctuation\">:</span> &lt;string<span class=\"token punctuation\">></span>\n            <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span> &lt;object<span class=\"token punctuation\">></span>\n            <span class=\"token key atrule\">matchLabelKeys</span><span class=\"token punctuation\">:</span> &lt;list<span class=\"token punctuation\">></span> <span class=\"token comment\"># optional</span>\n            <span class=\"token key atrule\">nodeAffinityPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>Honor<span class=\"token punctuation\">|</span>Ignore<span class=\"token punctuation\">]</span> <span class=\"token comment\"># optional</span>\n            <span class=\"token key atrule\">nodeTaintsPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>Honor<span class=\"token punctuation\">|</span>Ignore<span class=\"token punctuation\">]</span> <span class=\"token comment\"># optional</span></code></pre></div>\n<p>You can find a full explanation of each element in the <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#spread-constraint-definition\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes documentation</a>. For now, let's just briefly outline the obligatory fields:</p>\n<ul>\n<li><strong>maxSkew</strong>: The degree to which your Pods can be distributed unevenly across all zones. Its value must be more than zero.</li>\n<li><strong>topologyKey</strong>: The key of node labels. Nodes with the same label and values belong to the same topology. Each topology instance is a domain to which the scheduler tries to assign a balanced number of pods.</li>\n<li><strong>whenUnsatisfiable</strong>: Lets you decide what to do with a Pod when it doesn't satisfy your spread constraint:\n<ol>\n<li><code class=\"language-text\">DoNotSchedule</code> instructs the scheduler not to schedule it.</li>\n<li><code class=\"language-text\">ScheduleAnyway</code> tells the scheduler to schedule it and prioritize the nodes minimizing the skew.</li>\n</ol>\n</li>\n<li><strong>labelSelector</strong>: Allows finding matching Pods. The number of Pods in their corresponding topology domain is based on the Pods matching the label selector.</li>\n</ul>\n<h2 id=\"pod-deletion-cost\" style=\"position:relative;\"><a href=\"#pod-deletion-cost\" aria-label=\"pod deletion cost permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Pod Deletion Cost</h2>\n<p>Pod deletion cost is an annotation in Kubernetes that allows users to set a preference regarding which pods to remove first when downscaling a ReplicaSet. By using the <code class=\"language-text\">controller.kubernetes.io/pod-deletion-cost</code> annotation, users can influence the order in which pods are deleted. The annotation should be set on the pod, and the range is <code class=\"language-text\">[-2147483648, 2147483647]</code>.</p>\n<p>It represents the cost of deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion cost are preferred to be deleted before pods with higher deletion costs. If the annotation is not specified, the implicit value is 0.</p>\n<p>This feature is useful for influencing the downscaling order of pods and can be particularly beneficial in scenarios where specific pods need to be retained for longer periods during downscaling. The annotation interacts with the ReplicaSet controller to determine the order of pod deletion based on the set deletion cost values. It provides a way to customize the pod deletion behavior, ensuring that pods are removed in the desired order when downscaling a ReplicaSet.</p>\n<p>Here's an example of a YAML manifest for a Kubernetes Pod with the <code class=\"language-text\">controller.kubernetes.io/pod-deletion-cost</code> annotation:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> example<span class=\"token punctuation\">-</span>pod\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> example\n    <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">controller.kubernetes.io/pod-deletion-cost</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"100\"</span>\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">ports</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">containerPort</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span></code></pre></div>\n<p>In this example, we have a Pod named \"example-pod\" with the label \"app: example\" running an Nginx container. The <code class=\"language-text\">controller.kubernetes.io/pod-deletion-cost</code> annotation is set to \"100\" to indicate the deletion cost for this Pod. When scaling down, Pods with lower deletion costs will be preferred for deletion first.</p>\n<h2 id=\"maxunavailable-parameter\" style=\"position:relative;\"><a href=\"#maxunavailable-parameter\" aria-label=\"maxunavailable parameter permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>maxUnavailable Parameter</h2>\n<p><code class=\"language-text\">maxUnavailable</code> is a parameter in Kubernetes that specifies the maximum number of pods that can be unavailable during a rolling update. It is used in conjunction with the <code class=\"language-text\">maxSurge</code> parameter, which specifies the maximum number of new pods that can be created at a time. Together, these parameters control the rate at which pods are updated during a rolling update.</p>\n<p>Here's an example of how to use <code class=\"language-text\">maxUnavailable</code> in a Kubernetes Deployment:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>deployment\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>app\n    <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>app\n        <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>container\n                <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>image\n                <span class=\"token key atrule\">ports</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">containerPort</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n    <span class=\"token key atrule\">strategy</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> RollingUpdate\n        <span class=\"token key atrule\">rollingUpdate</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">maxSurge</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n            <span class=\"token key atrule\">maxUnavailable</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span></code></pre></div>\n<p>In this example, the <code class=\"language-text\">maxSurge</code> is set to 1, which means that only one new pod can be created at a time. The <code class=\"language-text\">maxUnavailable</code> is set to 0, which means that no old pods can be deleted at a time.</p>\n<p>Keep in mind that <code class=\"language-text\">maxUnavailable</code> can only be used to control the eviction of pods that have an associated controller managing them. In the case of a Deployment, this means that the controller is the ReplicaSet. If you want to control the eviction of pods in a StatefulSet, you can use the <code class=\"language-text\">maxUnavailable</code> parameter in the StatefulSet's rolling update strategy.</p>\n<h2 id=\"pod-disruption-budgets-pdb\" style=\"position:relative;\"><a href=\"#pod-disruption-budgets-pdb\" aria-label=\"pod disruption budgets pdb permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Pod Disruption Budgets (PDB)</h2>\n<p>PDBs are a Kubernetes feature that allows users to declare the minimum or maximum number of unavailable pods for an application during disruptions. PDBs help ensure the high availability of applications by setting constraints on the number of pods that can be disrupted at a time. Disruptions can be voluntary, such as maintenance operations or node scaling, or involuntary, such as hardware failures or system crashes.</p>\n<h3 id=\"three-steps-to-creating-a-pdb\" style=\"position:relative;\"><a href=\"#three-steps-to-creating-a-pdb\" aria-label=\"three steps to creating a pdb permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Three Steps to Creating a PDB:</h3>\n<ol>\n<li>\n<p><strong>Determine the Minimum Number of Instances</strong>: Assess your application's needs and identify the minimum number of instances (<code class=\"language-text\">minAvailable</code>) that must be operational during disruptions.</p>\n</li>\n<li>\n<p><strong>Create a YAML File</strong>: Use a text editor to create a YAML file that specifies the PDB object. Here's an example YAML file:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> ```yaml\n apiVersion: policy/v1\n kind: PodDisruptionBudget\n metadata:\n     name: my-pdb\n spec:\n     minAvailable: 2\n     selector:\n         matchLabels:\n             app: my-app\n ```\n\n In this example, the `minAvailable` field is set to 2, indicating that at least two instances of the specified pods should be available at any given time. The `selector` field identifies the target pods based on the label selector `app: my-app`.</code></pre></div>\n</li>\n<li>\n<p><strong>Apply the YAML File</strong>: Use the <code class=\"language-text\">kubectl apply</code> command to apply the YAML file and create the PDB within your Kubernetes cluster:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> ```sh\n kubectl apply -f pdb.yaml\n ```</code></pre></div>\n</li>\n</ol>\n<h2 id=\"kubernetes-pod-priority--preemption\" style=\"position:relative;\"><a href=\"#kubernetes-pod-priority--preemption\" aria-label=\"kubernetes pod priority  preemption permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Kubernetes Pod Priority &#x26; Preemption</h2>\n<p><code class=\"language-text\">PriorityClass</code> is a cluster-wide API object in Kubernetes and part of the <code class=\"language-text\">scheduling.k8s.io/v1</code> API group. It contains a mapping of the <code class=\"language-text\">PriorityClass</code> name (defined in <code class=\"language-text\">.metadata.name</code>) and an integer value (defined in <code class=\"language-text\">.value</code>). This represents the value that the scheduler uses to determine Pod's relative priority.</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption\" target=\"_blank\" rel=\"noopener noreferrer\">Pod preemption</a> is a Kubernetes feature that allows the cluster to preempt pods (removing an existing Pod in favor of a new Pod) on the basis of priority. <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority\" target=\"_blank\" rel=\"noopener noreferrer\">Pod priority</a> indicates the importance of a pod relative to other pods while scheduling. If there aren't enough resources to run all the current pods, the scheduler tries to evict lower-priority pods over high-priority ones.</p>\n<p>Here's an example. Next, create some environment-specific <code class=\"language-text\">PriorityClasses</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> scheduling.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PriorityClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> dev<span class=\"token punctuation\">-</span>pc\n<span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1000000</span>\n<span class=\"token key atrule\">globalDefault</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n<span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">></span><span class=\"token punctuation\">-</span>\n    (Optional) This priority class should only be used for all development pods.\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> scheduling.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PriorityClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> preprod<span class=\"token punctuation\">-</span>pc\n<span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2000000</span>\n<span class=\"token key atrule\">globalDefault</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n<span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">></span><span class=\"token punctuation\">-</span>\n    (Optional) This priority class should only be used for all preprod pods.\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> scheduling.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PriorityClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> prod<span class=\"token punctuation\">-</span>pc\n<span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4000000</span>\n<span class=\"token key atrule\">globalDefault</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n<span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">></span><span class=\"token punctuation\">-</span>\n    (Optional) This priority class should only be used for all prod pods.</code></pre></div>\n<p>Use <code class=\"language-text\">kubectl create -f &lt;FILE.YAML></code> command to create a pc and <code class=\"language-text\">kubectl get pc</code> to check its status.</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">$ kubectl get pc\nNAME                      VALUE        GLOBAL-DEFAULT   AGE\ndev-pc                    <span class=\"token number\">1000000</span>      <span class=\"token boolean\">false</span>            3m13s\npreprod-pc                <span class=\"token number\">2000000</span>      <span class=\"token boolean\">false</span>            2m3s\nprod-pc                   <span class=\"token number\">4000000</span>      <span class=\"token boolean\">false</span>            7s\nsystem-cluster-critical   <span class=\"token number\">2000000000</span>   <span class=\"token boolean\">false</span>            82m\nsystem-node-critical      <span class=\"token number\">2000001000</span>   <span class=\"token boolean\">false</span>            82m</code></pre></div>\n<p>Now, let's say there are 3 application pods: one for prod, one for preprod, and one for development. Below are three sample YAML manifest files for each of those.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token punctuation\">---</span>\n<span class=\"token comment\"># development</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> dev<span class=\"token punctuation\">-</span>nginx\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> dev\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> dev<span class=\"token punctuation\">-</span>nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">priorityClassName</span><span class=\"token punctuation\">:</span> dev<span class=\"token punctuation\">-</span>pc\n<span class=\"token punctuation\">---</span>\n<span class=\"token comment\"># preproduction</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> preprod<span class=\"token punctuation\">-</span>nginx\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> preprod\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> preprod<span class=\"token punctuation\">-</span>nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">priorityClassName</span><span class=\"token punctuation\">:</span> preprod<span class=\"token punctuation\">-</span>pc\n<span class=\"token punctuation\">---</span>\n<span class=\"token comment\"># production</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> prod<span class=\"token punctuation\">-</span>nginx\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> prod\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> prod<span class=\"token punctuation\">-</span>nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">priorityClassName</span><span class=\"token punctuation\">:</span> prod<span class=\"token punctuation\">-</span>pc</code></pre></div>\n<p>You can create these pods with the <code class=\"language-text\">kubectl create -f &lt;FILE.yaml></code> command, and then check their status using the <code class=\"language-text\">kubectl get pods</code> command. You can see if they are up and look ready to serve traffic:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">$ kubectl get pods --show-labels\nNAME            READY   STATUS    RESTARTS   AGE   LABELS\ndev-nginx       <span class=\"token number\">1</span>/1     Running   <span class=\"token number\">0</span>          55s   <span class=\"token assign-left variable\">env</span><span class=\"token operator\">=</span>dev\npreprod-nginx   <span class=\"token number\">1</span>/1     Running   <span class=\"token number\">0</span>          55s   <span class=\"token assign-left variable\">env</span><span class=\"token operator\">=</span>preprod\nprod-nginx      <span class=\"token number\">0</span>/1     Pending   <span class=\"token number\">0</span>          55s   <span class=\"token assign-left variable\">env</span><span class=\"token operator\">=</span>prod</code></pre></div>\n<p>The diagram below shows exactly how it works with the help of an exampl</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 124.70588235294117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAACIklEQVR42qVV2XLiMBD0/39TfoCHTfEQ7hXmMmXANjfG8tHrHjKJAwS8hapU2GNNT6unhRy8MPKiAIoMbx8x5pFlBM4rgKfTCdPpFAMzhjEuoih6DdBai/V6DW82w2QywWaz+QYsSvq73Q6z8uNyuUQcxxJ7NJIkkZwgCCTncDh8A7Ka7/toNptot9tSLcuyS7E7U0lcF/0CTNNUNOj3+3BdF8fjUWJpanEumbBgorN812LX48eWCUBN9vu9JFiblGAWf7wco3WOdy+DCXP0g7xsrsU9RR42Jc9zYWO8CFM/xMdfD13Xg5mtkGcpCtQAJAjZXX4vzzaJsdtusFr4CFZLHA/7X0k42hB2lfN8Psv7Rb9UAClHVhZg/NlwuJhNaDQaaLVa0ozqUNDFYoHRaITtditFuYNfGZIZfURP0f1qBSbxGwHpTxYkIItQW9TRMPm0CIGZzI4TlCDqOxbid5XjBlAXchKMCXJGBwMMh0PZqjLimvl8LuZn/FpX5975rG7567R8xqRBZazasB+ATOIZpH5siDLkqaGunGEY1uqwABKk1+uh0+nIFlQvzytN3O3K1tgQxqtMH9qGNiAwddKuqmWq2tVieE9DPSlq7v8ZN4AKQqbGGJHj2uxPAclIG6PmVkBqy291t+3o3UBvMVn/C9Uedbt7Y2y1T1U3ArJhL2nIQYa8dDjH47FMXgm1bFN90cVkRTPzjqEUfFYdnzH8BzpJpcsmGvYGAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"class\" title=\"\" src=\"/static/7ac02e7be6040d03a013650befced831/c5bb3/priority.png\" srcset=\"/static/7ac02e7be6040d03a013650befced831/04472/priority.png 170w,\n/static/7ac02e7be6040d03a013650befced831/9f933/priority.png 340w,\n/static/7ac02e7be6040d03a013650befced831/c5bb3/priority.png 680w,\n/static/7ac02e7be6040d03a013650befced831/b12f7/priority.png 1020w,\n/static/7ac02e7be6040d03a013650befced831/1be7e/priority.png 1059w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"inter-pod-anti-affinity\" style=\"position:relative;\"><a href=\"#inter-pod-anti-affinity\" aria-label=\"inter pod anti affinity permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Inter-pod Anti-affinity</h2>\n<p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods already running on that node, instead of the node labels. Inter-pod affinity and anti-affinity rules take the form \"this Pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more Pods that meet rule Y\", where X is a topology domain like node, rack, cloud provider zone or region, or similar and Y is the rule Kubernetes tries to satisfy.</p>\n<p>You express these rules (Y) as <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selector\" target=\"_blank\" rel=\"noopener noreferrer\">label selectors</a> with an optional associated list of namespaces. Pods are namespaced objects in Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors for Pod labels should specify the namespaces in which Kubernetes should look for those labels. You express the topology domain (X) using a <code class=\"language-text\">topologyKey</code>, which is the key for the node label that the system uses to denote the domain.</p>\n<p>Here's what the pod spec looks like:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">podAntiAffinity</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> app\n                        <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                        <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> web\n                <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> kubernetes.io/hostname\n            <span class=\"token key atrule\">preferredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">weight</span><span class=\"token punctuation\">:</span> <span class=\"token number\">100</span>\n                <span class=\"token key atrule\">podAffinityTerm</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> app\n                            <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                            <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                            <span class=\"token punctuation\">-</span> backend\n                    <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> kubernetes.io/hostname</code></pre></div>\n<p>Let's dive into the inter-pod anti-affinity structure:</p>\n<ul>\n<li><strong>requiredDuringSchedulingIgnoredDuringExecution</strong>: It means that conditions must be satisfied to pod be scheduled (hard requirement).</li>\n<li><strong>preferredDuringSchedulingIgnoredDuringExecution</strong>: If a condition can be satisfied, it will be satisfied. But if not, it will be ignored (soft requirement).</li>\n<li><strong>podAffinityTerm</strong>: The pod affinity term defines which pods we select with a label selector and which node topology key we target.</li>\n</ul>\n<p>A soft requirement has <code class=\"language-text\">podAffinityTerm</code> as separate property with an additional weight parameter that defines which term is more important. A hard requirement has an affinity term as a root list item object. For the hard affinity rule, all affinity terms and all expressions should be satisfied for the pod to be scheduled.</p>\n<h2 id=\"wrap-up\" style=\"position:relative;\"><a href=\"#wrap-up\" aria-label=\"wrap up permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Wrap Up</h2>\n<p>In this blog post, we explore the lesser-known features of Kubernetes that play an important role in keeping Kubernetes clusters running smoothly. We discuss <strong>Pod Topology Spread Constraints</strong>, <strong>pod-deletion-cost</strong>, <strong>maxUnavailable</strong>, <strong>Pod Disruption Budgets</strong>, and <strong>PriorityClasses</strong>. These features help ensure high availability and reliability in Kubernetes clusters, making them essential for managing Kubernetes workloads effectively.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 64px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD8ElEQVR42pWUC0xbZRTHL2DUGJMlRpeZ6OIr0fggLnO+jeHRFQota2np7YunmoUYTTAaTdxq1Jmo8f2YDveIC3GbYxQHDA1zBBgPIWwtlEkfYHkVJrS9bWlLe7/797vUmk7ZmCc5uffmu9/v+59zvnMY5l9mNjOZ4vP3uq03L7351APpa4A5E2ZzJrOe4ZgmC0CG6OK3d2f2xmh7SW+oRx0NNssbQgcLNAu12zYloUyG6OtDmeRPZxjmmkizvAWz5Ui4jHzUbgJv1SHRq/GGmor3TZsfvS0Fvixs6dD2F7gW+ZlAq6I7fLKkH3aW4I9aHq5qfmWMjXsHWCHqMBLMVyH2s3Joqubxmy4LDezPKYNNB8yUE/KLEsKvpcCkgYf7RSJMvw24TfyKXUu8/VoSuWCIYaoCXENhXSpV/wFyR2R74DSAG1DHfZ/nC0vUY91qAR4TwdRuCNPvAhM7SWxMTzirjsd0BZZPKg6nirSGQqmOt7LgRlg+0FQM32d58H8tQbRTJWCynMDzMiETrwhwVBI49HHMVCLcVmJJAmkxVwuaBl549v4bIx07LsRFlTYtCfykoNB8ULUIN8sFfkRD4NZTN4q+CryiQtGC9fmq2IAGwVGWUKVC4FQJ/Hsl8H1C1e6TYrlZIaz0agRi18XhMCL4Q+Eucd+ehzfd4mmXf+kbZesvuayruTwsfUuwsRSqTXAjWoHrVyNwtAi+L6jaj3KTqfhGwpPvZfSwvFWFB7T31WFQh/hiTcLWWvhIGjRZfl+D9ENQaGxcL1BogrNScJcKgcZi+A9I4ftKIoiHhDtVQH1utunBjU94v5UAC9WYH9LuTo86I6V0cW9uTej0Dh9cBoTsNPxRlqdwwg2XITioAXe+jISsWvg7lYMdDds/Hn/naT8c5Yi4jENrtqD49Jsfu8N3rOi7UJcqjHED4g69EB7TwW/TwndevI+0SBerAe55QluTj1oUSEyWY8CSFvalfZ1MwZ/v59zLHZW9F2hXjIV6SmPDJwoTK2LHOEyw7c9rPGuRtWHxOUQ7VHHMVmJumK1Yu6/pREm/ErTPszpf2ypxt8mD7qaiRvcHz5SireA6urTZ3aWcEmGYrYJ3WP/SlQcGVTpq1lwrvrOKe7aIYcVdxn82VWrvvn2uTz0NT8UqcOGcrmrdKZSajxs2MHfO9GuWidN4ztOtlPT9WJDt6VOfxUIN4DQlyIQJI6dLnmSuxlJz8tTB3CNI1ILMVsZ5J+2aqQoBTmMMF2uwnKxy1tUB/56Vt951/ea+E7Ie2uPAfLVA5qoEsdq0UBFnjyqH+Z+Wmn03fLprm7L7uPSNPovs9d9aZK8OtRY+lErPXzG7r/RlSAwMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"hat\" title=\"\" src=\"/static/3f09f9fba9b337c48fbfa766a63fec33/6b377/hat.png\" srcset=\"/static/3f09f9fba9b337c48fbfa766a63fec33/6b377/hat.png 64w\" sizes=\"(max-width: 64px) 100vw, 64px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<br>\n<p><strong><em>Until next time, つづく 🎉</em></strong></p>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <strong><em>Until next time 🎉</em></strong></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}