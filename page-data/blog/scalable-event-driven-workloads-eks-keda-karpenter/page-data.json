{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/scalable-event-driven-workloads-eks-keda-karpenter/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Use the power of Kubernetes for efficient and responsive application scaling.</strong></p>\n</blockquote>\n<p>Nowadays, businesses need to handle large amounts of data and events efficiently. This is where event-driven workflows can help. By using Amazon Elastic Kubernetes Service (EKS), <a href=\"https://keda.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">KEDA</a> (Kubernetes Event-Driven Autoscaling), and <a href=\"https://karpenter.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">Karpenter</a>, you can create a scalable and cost-effective solution for managing your workloads.</p>\n<p>Amazon EKS provides a managed Kubernetes service, making it easier to run Kubernetes without needing to manage the control plane. KEDA helps in scaling your applications based on the number of events, ensuring that your resources are used efficiently. Karpenter, on the other hand, is an open-source Kubernetes cluster autoscaler that helps in optimizing the cost and performance of your workloads.</p>\n<p>In this blog post, we will explore how to set up and run an event-driven workflow using these tools.</p>\n<h2 id=\"understanding-keda\" style=\"position:relative;\"><a href=\"#understanding-keda\" aria-label=\"understanding keda permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Understanding KEDA</h2>\n<p><a href=\"https://keda.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">KEDA</a> is a Kubernetes-based autoscaler that dynamically adjusts the number of pods in your cluster based on the number of events needing to be processed. It is a lightweight, single-purpose component that integrates seamlessly with any Kubernetes cluster.</p>\n<p>KEDA works alongside standard Kubernetes components like the <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Horizontal Pod Autoscaler (HPA)</a> without overwriting or duplicating their functionality.</p>\n<p>KEDA provides multiple scalers that detect whether deployments should be active, scaled in, or scaled out. These scalers support various event sources, including AWS services like <strong>SQS</strong>, <strong>CloudWatch</strong>, and <strong>DynamoDB</strong>, as well as <strong>GCP</strong>, <strong>Azure events</strong>, and more.</p>\n<p>By defining autoscaling policies through Custom Resource Definitions (CRDs), the KEDA operator manages the scaling of Kubernetes objects based on these policies. This ensures that your applications scale precisely according to the event load, optimizing resource usage and reducing costs.</p>\n<h2 id=\"exploring-karpenter\" style=\"position:relative;\"><a href=\"#exploring-karpenter\" aria-label=\"exploring karpenter permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Exploring Karpenter</h2>\n<p><a href=\"https://karpenter.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">Karpenter</a> is a high-performance Kubernetes cluster autoscaler that dynamically provisions worker nodes to meet the resource demands of unscheduled pods.</p>\n<p>Unlike traditional autoscalers, Karpenter uses a groupless architecture, allowing it to select the most appropriate instance types based on the specific needs of your workloads. It continuously evaluates the resource requirements of pending pods and other scheduling constraints, such as node selectors, affinities, and tolerations, to provision the ideal compute capacity.</p>\n<p>Karpenter integrates directly with the Amazon EC2 fleet API, bypassing the need for nodes and EC2 auto scaling groups. This direct provisioning significantly reduces the time required to scale up or down, from minutes to milliseconds. Karpenter also allows you to set quotas on CPU and memory for your EKS cluster, ensuring you only pay for the resources you actually use.</p>\n<p>By intercepting requests to Kubernetes admission controllers, Karpenter can dynamically adjust the number of nodes in your cluster, scaling up when demand increases and scaling down when there are excess resources. This flexibility helps maintain optimal performance and cost-efficiency for your Kubernetes workloads.</p>\n<h2 id=\"hands-on-walkthrough\" style=\"position:relative;\"><a href=\"#hands-on-walkthrough\" aria-label=\"hands on walkthrough permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Hands-On Walkthrough</h2>\n<p>This post is a proof-of-concept implementation that uses Kubernetes to execute code in response to events, such as API requests. The workflow is driven by KEDA, which scales Kubernetes pods based on incoming events like SQS messages. When KEDA scales out pods that remain in a pending state, Karpenter steps in, using provisioners to decide whether to scale out additional nodes.</p>\n<p>By integrating KEDA and Karpenter with Amazon EKS, we can easily build event-driven workflows that orchestrate jobs running on Kubernetes with AWS services, such as Amazon SQS, with minimal code. All AWS resources, Kubernetes manifests, and Kubernetes add-ons are managed and installed using Terraform.</p>\n<p>We will be bootstrapping the components with <a href=\"https://github.com/aws-ia/terraform-aws-eks-blueprints-addons/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">EKS blueprints addons</a>.</p>\n<h3 id=\"architecture-overview\" style=\"position:relative;\"><a href=\"#architecture-overview\" aria-label=\"architecture overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Architecture Overview</h3>\n<p>In this hands-on lab, we will explore an application deployment architecture that uses Kubernetes for container orchestration, AWS SQS for message queuing, KEDA for scaling pods, and Karpenter for scaling nodes.</p>\n<p>Our architecture consists of the following components:</p>\n<ol>\n<li><strong>Application Deployment:</strong> The application is deployed in a Kubernetes cluster, running as a set of pods. These pods handle incoming requests and perform the necessary processing.</li>\n<li><strong>AWS SQS:</strong> Amazon Simple Queue Service (SQS) is used to queue messages that need to be processed by the application. This ensures that messages are handled asynchronously and can be processed as resources become available.</li>\n<li><strong>KEDA:</strong> Kubernetes Event-Driven Autoscaling (KEDA) is responsible for scaling the number of pods based on the number of messages in the SQS queue. When messages are posted to the SQS queue, KEDA scales up the pods to handle the increased load.</li>\n<li><strong>Karpenter:</strong> If the current cluster cannot handle the increased load even after scaling the pods, Karpenter comes into play. Karpenter is an open-source Kubernetes cluster autoscaler that creates additional nodes to accommodate the increased number of pods.</li>\n</ol>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 124.11764705882354%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3UlEQVR42r2Vy27TQBSG+yy8AI/Bu7BiCWILQmLDDhZA2VAhFalCIG6tyqVU0AoKUkGVEApJfBl7PLYTey62E9s/x2lSNWkqKgpY+uXMeOab/5w5M1nAX34W/jtwMBgiy3JUdX06YD0GfNv7js0PH9GxHJRledD/W2AzsCwrNMMbTRy12hZerr9DGPfQdE2+j1RVRxZYmDQlhfWTcwitESh1oKYdGgOhNLiU+/2S1IyrAK8okVFajjjURQ6eMgTzJBl44sLvu9QmSQ4ufoCvX0Xw+S70ccBmspDelEJF0g3UpQVd6iPlIXznC9zLZxDcPgdT1ScHssiFF3ngMYcfcYT9AGEiyKGL1qsltLdWkA2Pc0ghiRkoYxF8W0J4lEtPwbUzWJZBHGQQcQ1PFNBZMQc4KBDUMUQZHsCa3EVhih6nxZhCEhrs7CZ4sx0jFQbcUfDddBo42XZFO+cur0K83aad9MbhM7heHzY54kyOZLUcOG0HUSAho5Sc9qHMYeA4oYkrsHf2Atj5m7DCDtpeC45oYWcvwsYnhZ9tRSH3EfgtpD0LKGx4vg2f28jy/KjDNEmxe2sZnaevqeZ8BMm+Q8uO6bT04TONRPRx7UGAS4uCoF1cXIxwYyXAYJidPIehSCiHVNieoRA1VrdcPNqwgdzG2lYX219tOu/5Mbvco11O2BRQBAlCPyMZRFxhYCxUeQc1ATPdgZRdmDw/WR3OA+bSQZVZ0ImN6w9DLL7g5DD7c2DRAE0XA23j3nMPjzcZ5fAUDgvlYEjAOqc3hW50l+rwlCE3QJ1auL8W4Ml7H8X8kLPRTSKay4DKptHot0gJlCMiaMw1htqlGrSQSQtXlnzcecYwnFc2ZjCgcxwRVMCNGFjcXAweXD+k89yjE9MD82MEglyHDHHcvD1EvQA5zT0MnLqEh8OyVkrXWpt9GT2WqQ1JKlMrndWp1DS2qquZ+f/kXy+ZVVVVJ9bs3F9483poe5519gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Architecture\" title=\"\" src=\"/static/99b10ffe138fd9f32a12e348f5de6000/c5bb3/Architecture.png\" srcset=\"/static/99b10ffe138fd9f32a12e348f5de6000/04472/Architecture.png 170w,\n/static/99b10ffe138fd9f32a12e348f5de6000/9f933/Architecture.png 340w,\n/static/99b10ffe138fd9f32a12e348f5de6000/c5bb3/Architecture.png 680w,\n/static/99b10ffe138fd9f32a12e348f5de6000/b12f7/Architecture.png 1020w,\n/static/99b10ffe138fd9f32a12e348f5de6000/b5a09/Architecture.png 1360w,\n/static/99b10ffe138fd9f32a12e348f5de6000/0f586/Architecture.png 1498w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"setup-the-infrastructure\" style=\"position:relative;\"><a href=\"#setup-the-infrastructure\" aria-label=\"setup the infrastructure permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Setup the Infrastructure</h3>\n<p>In order to create our infrastructure, we will use the <a href=\"https://github.com/aws-ia/terraform-aws-eks-blueprints-addons\" target=\"_blank\" rel=\"noopener noreferrer\">terraform-aws-eks-blueprints-addons</a> module.</p>\n<p>Follow the steps to set up an Amazon EKS cluster, and Karpenter on the Amazon EKS cluster.</p>\n<p>Clone the repository to your local machine using the following command:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/eks-keda-karpenter-autoscaling.git</code></pre></div>\n<p>Navigate to the repository's directory:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">cd</span> eks-keda-karpenter-autoscaling/aws-blueprints-iac</code></pre></div>\n<p>Now, run the following commands to initialize, plan, and apply the Terraform configuration with automatic approval:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">terraform init\nterraform plan\nterraform apply -auto-approve</code></pre></div>\n<p>This will create a VPC, EKS cluster, Karpenter, and Fargate profile, thanks to the <code class=\"language-text\">enable_karpenter</code> set to <code class=\"language-text\">true</code>.</p>\n<p>Then provision the Karpenter <code class=\"language-text\">EC2NodeClass</code> and <code class=\"language-text\">NodePool</code> resources which provide Karpenter the necessary configurations to provision EC2 resources:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">kubectl apply <span class=\"token parameter variable\">-f</span> karpenter/karpenter.yaml</code></pre></div>\n<p>Once the Karpenter resources are in place, Karpenter will provision the necessary EC2 resources to satisfy any pending pods in the scheduler's queue.</p>\n<h3 id=\"deploy-keda\" style=\"position:relative;\"><a href=\"#deploy-keda\" aria-label=\"deploy keda permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deploy KEDA</h3>\n<p>We will use Helm to deploy KEDA. Run the below commands to get the values file:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">helm repo <span class=\"token function\">add</span> kedacore https://kedacore.github.io/charts \nhelm repo update\nhelm show values kedacore/keda <span class=\"token operator\">></span> values.yaml</code></pre></div>\n<p>Before installing the release, we need to update the values. Here is what you need to add:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">serviceAccount</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span> \n        <span class=\"token key atrule\">eks.amazonaws.com/role-arn</span><span class=\"token punctuation\">:</span> &lt;POD_ROLE_ARN<span class=\"token punctuation\">></span></code></pre></div>\n<p>Now to deploy KEDA, you need to run:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">helm <span class=\"token function\">install</span> keda kedacore/keda <span class=\"token parameter variable\">--values</span> values.yaml <span class=\"token parameter variable\">--namespace</span> keda</code></pre></div>\n<h2 id=\"keda-and-karpenter-autoscaling-in-action\" style=\"position:relative;\"><a href=\"#keda-and-karpenter-autoscaling-in-action\" aria-label=\"keda and karpenter autoscaling in action permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>KEDA and Karpenter Autoscaling in Action</h2>\n<p>With KEDA, we are going to scale the deployment replicas to zero by using an empty AWS SQS queue. Then, we will feed that queue to scale up and down the number of replicas.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">kubectl create ns keda-karpenter-scaling\nkubectl config set-context <span class=\"token parameter variable\">--current</span> <span class=\"token parameter variable\">--namespace</span><span class=\"token operator\">=</span>keda\nkubectl create deployment nginx-deployment <span class=\"token parameter variable\">--image</span> nginx <span class=\"token parameter variable\">--replicas</span><span class=\"token operator\">=</span><span class=\"token number\">2</span> <span class=\"token parameter variable\">--requests</span><span class=\"token operator\">=</span>cpu<span class=\"token operator\">=</span><span class=\"token number\">1</span>,memory<span class=\"token operator\">=</span>3Gi</code></pre></div>\n<p>Running these commands will deploy 2 pods with 1 vCPU and 3 GiB of memory each, requiring one node per pod. Karpenter should create two nodes. If you set 3 replicas, based on the current provisioner configuration, Karpenter will create 2 nodes and leave one pod pending.</p>\n<p>To create an SQS queue, run:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws sqs create-queue --queue-name keda-karpenter-scaling</code></pre></div>\n<p>Once the queue is created, deploy the KEDA scaled object and the trigger authentication:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">cat &lt;&lt;EOF <span class=\"token punctuation\">|</span> kubectl create <span class=\"token punctuation\">-</span>f <span class=\"token punctuation\">-</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> keda.sh/v1alpha1 \n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ScaledObject \n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span> \n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>sqs<span class=\"token punctuation\">-</span>queue<span class=\"token punctuation\">-</span>scaledobject \n    <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> keda\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span> \n    <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span> \n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx<span class=\"token punctuation\">-</span>deployment \n        <span class=\"token key atrule\">minReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span>  <span class=\"token comment\"># We don't want pods if the queue is empty </span>\n        <span class=\"token key atrule\">maxReplicaCount</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>  <span class=\"token comment\"># We don't want more than 2 replicas </span>\n        <span class=\"token key atrule\">pollingInterval</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span> <span class=\"token comment\"># Frequency for metrics (in seconds) </span>\n        <span class=\"token key atrule\">cooldownPeriod</span><span class=\"token punctuation\">:</span> <span class=\"token number\">25</span>  <span class=\"token comment\"># Wait time for downscale (in seconds) </span>\n<span class=\"token key atrule\">triggers</span><span class=\"token punctuation\">:</span> \n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>sqs<span class=\"token punctuation\">-</span>queue \n        <span class=\"token key atrule\">authenticationRef</span><span class=\"token punctuation\">:</span> \n            <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> keda<span class=\"token punctuation\">-</span>aws<span class=\"token punctuation\">-</span>credentials \n        <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span> \n            <span class=\"token key atrule\">queueURL</span><span class=\"token punctuation\">:</span> https<span class=\"token punctuation\">:</span>//sqs.eu<span class=\"token punctuation\">-</span>west<span class=\"token punctuation\">-</span>1.amazonaws.com/$AWS_ACCOUNT_ID/keda<span class=\"token punctuation\">-</span>karpenter<span class=\"token punctuation\">-</span>scaling\n            <span class=\"token key atrule\">queueLength</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1\"</span> \n            <span class=\"token key atrule\">awsRegion</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"eu-west-1\"</span> \n            <span class=\"token key atrule\">identityOwner</span><span class=\"token punctuation\">:</span> operator\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> keda.sh/v1alpha1 \n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> TriggerAuthentication \n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span> \n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> keda<span class=\"token punctuation\">-</span>aws<span class=\"token punctuation\">-</span>credentials \n    <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> keda\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span> \n    <span class=\"token key atrule\">podIdentity</span><span class=\"token punctuation\">:</span> \n        <span class=\"token key atrule\">provider</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>eks\nEOF</code></pre></div>\n<p>After deploying the KEDA configuration, since the queue is empty, the nginx deployment should scale down to 0 because the minReplicaCount is set to 0.</p>\n<p>With two nodes without any resources running, Karpenter will downscale the number of nodes from 2 to 0.</p>\n<p>The queue length is set to 1, meaning that with (n) messages, we'll have (n) pods, but (n) will be less than maxReplicaCount. For example, with 2 messages in the queue, we'll have 2 pods. With 3 messages in the queue, we'll still have 2 pods due to the Karpenter quota.</p>\n<p>If the queueLength is set to 2, with 1 or 2 messages in the queue, we'll have 1 pod, and with 3‚Äì4 or more messages, we'll have 2 pods because the maxReplicaCount is still set to 2. To test this, you can send messages using:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token keyword\">for</span> <span class=\"token for-or-select variable\">i</span> <span class=\"token keyword\">in</span> <span class=\"token number\">1</span><span class=\"token punctuation\">..</span><span class=\"token number\">2</span>\n<span class=\"token keyword\">do</span>\n    aws sqs send-message <span class=\"token punctuation\">\\</span>\n    --queue-url <span class=\"token variable\"><span class=\"token variable\">$(</span>aws sqs get-queue-url --queue-name keda-karpenter-scaling<span class=\"token variable\">)</span></span> <span class=\"token punctuation\">\\</span>\n    --message-body <span class=\"token string\">\"Keda and Karpenter demo\"</span>\n<span class=\"token keyword\">done</span></code></pre></div>\n<p>This should trigger the KEDA scaled object and create two pods. These two pods will be pending due to the lack of space in the cluster. Karpenter will detect these pending pods, create two nodes, and schedule the pods on the new nodes.</p>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h2>\n<p>In this post, we showed how using KEDA and Karpenter on Amazon EKS makes running event-driven workloads easier and more efficient. KEDA scales your applications based on events, while Karpenter quickly adds the necessary nodes. This combination ensures your system is responsive and cost-effective.</p>\n<p>By integrating these tools with Kubernetes, EKS, AWS, and SQS, you can manage your workloads better, optimize resources, and improve performance. This setup helps businesses handle event-driven tasks smoothly and efficiently.</p>\n<p>The end ‚úåüèª</p>\n<h2 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>References</h2>\n<ul>\n<li><a href=\"https://medium.com/@ledevedeccorentin/run-a-scalable-event-driven-workflow-with-amazon-eks-keda-and-karpenter-558fce1766ec\" target=\"_blank\" rel=\"noopener noreferrer\">Run a scalable event-driven workflow with Amazon EKS, KEDA, and Karpenter</a></li>\n<li><a href=\"https://aws.amazon.com/fr/blogs/containers/scalable-and-cost-effective-event-driven-workloads-with-keda-and-karpenter-on-amazon-eks/\" target=\"_blank\" rel=\"noopener noreferrer\">Scalable and cost-effective event-driven workloads with KEDA and Karpenter on Amazon EKS</a></li>\n<li><a href=\"https://blog.devgenius.io/run-event-driven-workflows-with-amazon-eks-blueprints-keda-and-karpenter-80e325426b4a\" target=\"_blank\" rel=\"noopener noreferrer\">Run event-driven workflows with Amazon EKS Blueprints, KEDA, and Karpenter</a></li>\n<li><a href=\"https://keda.sh/docs/2.14/concepts/scaling-deployments/\" target=\"_blank\" rel=\"noopener noreferrer\">KEDA Documentation</a></li>\n<li><a href=\"https://aws-ia.github.io/terraform-aws-eks-blueprints/\" target=\"_blank\" rel=\"noopener noreferrer\">Terraform AWS EKS Blueprints</a></li>\n<li><a href=\"https://aws-ia.github.io/terraform-aws-eks-blueprints-addons/main/\" target=\"_blank\" rel=\"noopener noreferrer\">Terraform AWS EKS Blueprints Addons</a></li>\n</ul>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":7,"rawMarkdownBody":"\n> **Use the power of Kubernetes for efficient and responsive application scaling.**\n\nNowadays, businesses need to handle large amounts of data and events efficiently. This is where event-driven workflows can help. By using Amazon Elastic Kubernetes Service (EKS), [KEDA](https://keda.sh/) (Kubernetes Event-Driven Autoscaling), and [Karpenter](https://karpenter.sh/), you can create a scalable and cost-effective solution for managing your workloads.\n\nAmazon EKS provides a managed Kubernetes service, making it easier to run Kubernetes without needing to manage the control plane. KEDA helps in scaling your applications based on the number of events, ensuring that your resources are used efficiently. Karpenter, on the other hand, is an open-source Kubernetes cluster autoscaler that helps in optimizing the cost and performance of your workloads.\n\nIn this blog post, we will explore how to set up and run an event-driven workflow using these tools.\n\n## Understanding KEDA\n\n[KEDA](https://keda.sh/) is a Kubernetes-based autoscaler that dynamically adjusts the number of pods in your cluster based on the number of events needing to be processed. It is a lightweight, single-purpose component that integrates seamlessly with any Kubernetes cluster.\n\nKEDA works alongside standard Kubernetes components like the [Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) without overwriting or duplicating their functionality.\n\nKEDA provides multiple scalers that detect whether deployments should be active, scaled in, or scaled out. These scalers support various event sources, including AWS services like **SQS**, **CloudWatch**, and **DynamoDB**, as well as **GCP**, **Azure events**, and more.\n\nBy defining autoscaling policies through Custom Resource Definitions (CRDs), the KEDA operator manages the scaling of Kubernetes objects based on these policies. This ensures that your applications scale precisely according to the event load, optimizing resource usage and reducing costs.\n\n## Exploring Karpenter\n\n[Karpenter](https://karpenter.sh/) is a high-performance Kubernetes cluster autoscaler that dynamically provisions worker nodes to meet the resource demands of unscheduled pods.\n\nUnlike traditional autoscalers, Karpenter uses a groupless architecture, allowing it to select the most appropriate instance types based on the specific needs of your workloads. It continuously evaluates the resource requirements of pending pods and other scheduling constraints, such as node selectors, affinities, and tolerations, to provision the ideal compute capacity.\n\nKarpenter integrates directly with the Amazon EC2 fleet API, bypassing the need for nodes and EC2 auto scaling groups. This direct provisioning significantly reduces the time required to scale up or down, from minutes to milliseconds. Karpenter also allows you to set quotas on CPU and memory for your EKS cluster, ensuring you only pay for the resources you actually use.\n\nBy intercepting requests to Kubernetes admission controllers, Karpenter can dynamically adjust the number of nodes in your cluster, scaling up when demand increases and scaling down when there are excess resources. This flexibility helps maintain optimal performance and cost-efficiency for your Kubernetes workloads.\n\n## Hands-On Walkthrough\n\nThis post is a proof-of-concept implementation that uses Kubernetes to execute code in response to events, such as API requests. The workflow is driven by KEDA, which scales Kubernetes pods based on incoming events like SQS messages. When KEDA scales out pods that remain in a pending state, Karpenter steps in, using provisioners to decide whether to scale out additional nodes.\n\nBy integrating KEDA and Karpenter with Amazon EKS, we can easily build event-driven workflows that orchestrate jobs running on Kubernetes with AWS services, such as Amazon SQS, with minimal code. All AWS resources, Kubernetes manifests, and Kubernetes add-ons are managed and installed using Terraform.\n\nWe will be bootstrapping the components with [EKS blueprints addons](https://github.com/aws-ia/terraform-aws-eks-blueprints-addons/tree/main).\n\n### Architecture Overview\n\nIn this hands-on lab, we will explore an application deployment architecture that uses Kubernetes for container orchestration, AWS SQS for message queuing, KEDA for scaling pods, and Karpenter for scaling nodes.\n\nOur architecture consists of the following components:\n\n1. **Application Deployment:** The application is deployed in a Kubernetes cluster, running as a set of pods. These pods handle incoming requests and perform the necessary processing.\n2. **AWS SQS:** Amazon Simple Queue Service (SQS) is used to queue messages that need to be processed by the application. This ensures that messages are handled asynchronously and can be processed as resources become available.\n3. **KEDA:** Kubernetes Event-Driven Autoscaling (KEDA) is responsible for scaling the number of pods based on the number of messages in the SQS queue. When messages are posted to the SQS queue, KEDA scales up the pods to handle the increased load.\n4. **Karpenter:** If the current cluster cannot handle the increased load even after scaling the pods, Karpenter comes into play. Karpenter is an open-source Kubernetes cluster autoscaler that creates additional nodes to accommodate the increased number of pods.\n\n![Architecture](./Architecture.png)\n\n### Setup the Infrastructure\n\nIn order to create our infrastructure, we will use the [terraform-aws-eks-blueprints-addons](https://github.com/aws-ia/terraform-aws-eks-blueprints-addons) module.\n\nFollow the steps to set up an Amazon EKS cluster, and Karpenter on the Amazon EKS cluster.\n\nClone the repository to your local machine using the following command:\n\n```bash\ngit clone https://github.com/seifrajhi/eks-keda-karpenter-autoscaling.git\n```\n\nNavigate to the repository's directory:\n\n```bash\ncd eks-keda-karpenter-autoscaling/aws-blueprints-iac\n```\n\nNow, run the following commands to initialize, plan, and apply the Terraform configuration with automatic approval:\n\n```bash\nterraform init\nterraform plan\nterraform apply -auto-approve\n```\n\nThis will create a VPC, EKS cluster, Karpenter, and Fargate profile, thanks to the `enable_karpenter` set to `true`.\n\nThen provision the Karpenter `EC2NodeClass` and `NodePool` resources which provide Karpenter the necessary configurations to provision EC2 resources:\n\n```bash\nkubectl apply -f karpenter/karpenter.yaml\n```\n\nOnce the Karpenter resources are in place, Karpenter will provision the necessary EC2 resources to satisfy any pending pods in the scheduler's queue.\n\n### Deploy KEDA\n\nWe will use Helm to deploy KEDA. Run the below commands to get the values file:\n\n```bash\nhelm repo add kedacore https://kedacore.github.io/charts \nhelm repo update\nhelm show values kedacore/keda > values.yaml\n```\n\nBefore installing the release, we need to update the values. Here is what you need to add:\n\n```yaml\nserviceAccount:\n    annotations: \n        eks.amazonaws.com/role-arn: <POD_ROLE_ARN>\n```\n\nNow to deploy KEDA, you need to run:\n\n```bash\nhelm install keda kedacore/keda --values values.yaml --namespace keda\n```\n\n## KEDA and Karpenter Autoscaling in Action\n\nWith KEDA, we are going to scale the deployment replicas to zero by using an empty AWS SQS queue. Then, we will feed that queue to scale up and down the number of replicas.\n\n```bash\nkubectl create ns keda-karpenter-scaling\nkubectl config set-context --current --namespace=keda\nkubectl create deployment nginx-deployment --image nginx --replicas=2 --requests=cpu=1,memory=3Gi\n```\n\nRunning these commands will deploy 2 pods with 1 vCPU and 3 GiB of memory each, requiring one node per pod. Karpenter should create two nodes. If you set 3 replicas, based on the current provisioner configuration, Karpenter will create 2 nodes and leave one pod pending.\n\nTo create an SQS queue, run:\n\n```bash\naws sqs create-queue --queue-name keda-karpenter-scaling\n```\n\nOnce the queue is created, deploy the KEDA scaled object and the trigger authentication:\n\n```yaml\ncat <<EOF | kubectl create -f -\napiVersion: keda.sh/v1alpha1 \nkind: ScaledObject \nmetadata: \n    name: aws-sqs-queue-scaledobject \n    namespace: keda\nspec: \n    scaleTargetRef: \n        name: nginx-deployment \n        minReplicaCount: 0  # We don't want pods if the queue is empty \n        maxReplicaCount: 2  # We don't want more than 2 replicas \n        pollingInterval: 10 # Frequency for metrics (in seconds) \n        cooldownPeriod: 25  # Wait time for downscale (in seconds) \ntriggers: \n    - type: aws-sqs-queue \n        authenticationRef: \n            name: keda-aws-credentials \n        metadata: \n            queueURL: https://sqs.eu-west-1.amazonaws.com/$AWS_ACCOUNT_ID/keda-karpenter-scaling\n            queueLength: \"1\" \n            awsRegion: \"eu-west-1\" \n            identityOwner: operator\n---\napiVersion: keda.sh/v1alpha1 \nkind: TriggerAuthentication \nmetadata: \n    name: keda-aws-credentials \n    namespace: keda\nspec: \n    podIdentity: \n        provider: aws-eks\nEOF\n```\n\nAfter deploying the KEDA configuration, since the queue is empty, the nginx deployment should scale down to 0 because the minReplicaCount is set to 0.\n\nWith two nodes without any resources running, Karpenter will downscale the number of nodes from 2 to 0.\n\nThe queue length is set to 1, meaning that with (n) messages, we'll have (n) pods, but (n) will be less than maxReplicaCount. For example, with 2 messages in the queue, we'll have 2 pods. With 3 messages in the queue, we'll still have 2 pods due to the Karpenter quota.\n\nIf the queueLength is set to 2, with 1 or 2 messages in the queue, we'll have 1 pod, and with 3‚Äì4 or more messages, we'll have 2 pods because the maxReplicaCount is still set to 2. To test this, you can send messages using:\n\n```bash\nfor i in 1..2\ndo\n    aws sqs send-message \\\n    --queue-url $(aws sqs get-queue-url --queue-name keda-karpenter-scaling) \\\n    --message-body \"Keda and Karpenter demo\"\ndone\n```\n\nThis should trigger the KEDA scaled object and create two pods. These two pods will be pending due to the lack of space in the cluster. Karpenter will detect these pending pods, create two nodes, and schedule the pods on the new nodes.\n\n## üîö Conclusion\n\nIn this post, we showed how using KEDA and Karpenter on Amazon EKS makes running event-driven workloads easier and more efficient. KEDA scales your applications based on events, while Karpenter quickly adds the necessary nodes. This combination ensures your system is responsive and cost-effective.\n\nBy integrating these tools with Kubernetes, EKS, AWS, and SQS, you can manage your workloads better, optimize resources, and improve performance. This setup helps businesses handle event-driven tasks smoothly and efficiently.\n\nThe end ‚úåüèª\n\n## References\n\n- [Run a scalable event-driven workflow with Amazon EKS, KEDA, and Karpenter](https://medium.com/@ledevedeccorentin/run-a-scalable-event-driven-workflow-with-amazon-eks-keda-and-karpenter-558fce1766ec)\n- [Scalable and cost-effective event-driven workloads with KEDA and Karpenter on Amazon EKS](https://aws.amazon.com/fr/blogs/containers/scalable-and-cost-effective-event-driven-workloads-with-keda-and-karpenter-on-amazon-eks/)\n- [Run event-driven workflows with Amazon EKS Blueprints, KEDA, and Karpenter](https://blog.devgenius.io/run-event-driven-workflows-with-amazon-eks-blueprints-keda-and-karpenter-80e325426b4a)\n- [KEDA Documentation](https://keda.sh/docs/2.14/concepts/scaling-deployments/)\n- [Terraform AWS EKS Blueprints](https://aws-ia.github.io/terraform-aws-eks-blueprints/)\n- [Terraform AWS EKS Blueprints Addons](https://aws-ia.github.io/terraform-aws-eks-blueprints-addons/main/)\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1389},"frontmatter":{"id":"579776321ab8a667be34d5da","path":"/blog/scalable-event-driven-workloads-eks-keda-karpenter/","humanDate":"Oct 26, 2024","fullDate":"2024-10-26","title":"Deploy Scalable, Cost-Effective Event-Driven Workloads with Amazon EKS, KEDA, and Karpenter","keywords":["AWS","Kubernetes","EKS","KEDA","Karpenter"],"excerpt":"Learn how to leverage Amazon EKS, KEDA, and Karpenter for scalable and cost-effective event-driven workloads.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC90lEQVR42g2TXUzbVRiHz8f/X4b4MarYLUZ342K89sqFbHyKsu1yiTcmS/RCTYyJyaIxGjM1BgrS1sV1UBjppBms4Bh1kCjT4eL8jMGwxG06JtTKgH4CZVBGH9+LJ2/Oec9583vf8zvKLpYwy1s4a2WcDXALG7jfXsX5aRp7+RfMTBJz9W/MyM+Y6BVM+Hts4Ar2bBb9xSq6N4c+lUF/togK3UHZ/D2c1Xu4ySU8c//hzP+LE+nFbfPjdvfi9J3BXphE9w2j23oxH0TQb0ew/u/QoRl0158Sb6HDy+gTUtQplnHTK9jRMWyPHEx8hb04jnN2ECdxEc9gHHd0Ajv1m6gZxXQNYYITmGOnMW+eRh8NoN+6gAqL0uACyi2BM30NGx8W4jixGHZgACcaxfwgrZ8bxo0OSC6BORnDBISPzmA+EQHvxtEvdaBf7Ud3zqKCotBTKmMoowCdSmGkoJFiemQEffMmelzUfD0pMxyTmZ1HdZ9HB75EfziEbp9Cx7dQ0U1UZAN1ag1lJqfY2dbGrr4+Hg2Hqe7ooCYUwhcM4u3spObTLiEghKg53s7u9/w8csxP9esf43tN4it+Hn65C99RP5VvDKHU+8d53Ovl+SNHqG1sZF9TE882NFDX0kLL4cPUNTfz3MGD1NbXy349B1pf4IDk9u2vo6X1EPWNTdTWNXKouYG9L74jM1zb5Ml8nifSGXZlc/iyWXyZDK5EJWu1WkSnC+i5NJ7f71A1nsRNzKPic+j+W/IYQuAvVPt1VOc/Ypu7ZfbL/FqFZ4SnhKeF+wSzvU31tes8NJ+iMge7f9zkwV9LPJZYp2JiHWe0iI2JQ/oLmEiBip682Ca7je/SOlUzi5jkPOZ2EptaQBflwlaZvTem2TN7gx2zm9R8U6Dq0gp7BjNUnBNjx8TQEfHfSbl7YokHeqQju1DC+0eJylQRdzmLZymHm1/D3pXfIx41a/L6y9vo26Lu8gresRz3D2Wxw1IwmkZ3L6E/X2SH/JadkTz/AyyHIhsSePY3AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/07bd709e3d632c13b2a0a6063652059d/c99e1/istio-hpa-cover.png","srcSet":"/static/07bd709e3d632c13b2a0a6063652059d/02437/istio-hpa-cover.png 750w,\n/static/07bd709e3d632c13b2a0a6063652059d/4584d/istio-hpa-cover.png 1080w,\n/static/07bd709e3d632c13b2a0a6063652059d/e9886/istio-hpa-cover.png 1366w,\n/static/07bd709e3d632c13b2a0a6063652059d/c99e1/istio-hpa-cover.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/07bd709e3d632c13b2a0a6063652059d/06597/istio-hpa-cover.webp 750w,\n/static/07bd709e3d632c13b2a0a6063652059d/8ad81/istio-hpa-cover.webp 1080w,\n/static/07bd709e3d632c13b2a0a6063652059d/95ac8/istio-hpa-cover.webp 1366w,\n/static/07bd709e3d632c13b2a0a6063652059d/1fac0/istio-hpa-cover.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.56375}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/paved-roads-netflix-developers/","title":"The Power of Paved Roads: Netflix's Approach to Empowering Developers with Freedom and Responsibility","date":"2024-10-26 19:34:00"},"excerpt":"Exploring how Netflix balances developer autonomy with structured support ‚ÑπÔ∏è Overview Step onto the paved golden road of freedom and‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/ephemeral-environments-kubernetes-cicd/","title":"Ephemeral Environments for Cost-Effective and Reliable CI/CD Pipelines with Kubernetes üê≥","date":"2024-10-26 17:34:00"},"excerpt":"Build Reliable Software Faster with Ephemeral Environments and Kubernetes üêã üèû Introduction Building high-quality software quickly is‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}