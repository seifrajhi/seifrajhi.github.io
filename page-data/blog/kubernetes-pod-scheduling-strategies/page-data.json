{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/kubernetes-pod-scheduling-strategies/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Pod to Node Scheduling Strategies üê≥</strong></p>\n</blockquote>\n<h2 id=\"-overview\" style=\"position:relative;\"><a href=\"#-overview\" aria-label=\" overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Overview</h2>\n<p>By default, Kubernetes places pods randomly across the available nodes in the cluster. However, there are many scenarios where you may need to control the placement of your pods on specific nodes.</p>\n<p>For example, you may want to place pods that require specific hardware resources (e.g., GPUs) on nodes that have those resources available or avoid placing pods on nodes that are running other critical workloads.</p>\n<p>Kubernetes provides a number of features for controlling pod placement, including: Node selectors, Affinity and anti-affinity rules, Taints and tolerations.</p>\n<p>In this article, we will discuss the different approaches to advanced pod scheduling in Kubernetes and provide examples of how to use them to solve common use cases.</p>\n<h2 id=\"use-cases-for-pod-to-node-scheduling-in-kubernetes\" style=\"position:relative;\"><a href=\"#use-cases-for-pod-to-node-scheduling-in-kubernetes\" aria-label=\"use cases for pod to node scheduling in kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Use Cases for Pod-to-Node Scheduling in Kubernetes</h2>\n<p>In the Kubernetes environment, it is often necessary to customize how pods are scheduled to nodes. Here are some of the most common scenarios where advanced pod scheduling can be beneficial:</p>\n<h3 id=\"-running-pods-on-nodes-with-dedicated-hardware\" style=\"position:relative;\"><a href=\"#-running-pods-on-nodes-with-dedicated-hardware\" aria-label=\" running pods on nodes with dedicated hardware permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Running Pods on Nodes with Dedicated Hardware</h3>\n<p>Some Kubernetes applications may have specific hardware requirements. For example, pods running machine learning jobs may require high-performance GPUs instead of CPUs, while Elasticsearch pods may perform better on SSDs than HDDs. As a result, it is best practice for any resource-aware Kubernetes cluster management strategy to assign pods to the nodes with the appropriate hardware.</p>\n<h3 id=\"-pod-colocation-and-codependency\" style=\"position:relative;\"><a href=\"#-pod-colocation-and-codependency\" aria-label=\" pod colocation and codependency permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>ü§ù Pod Colocation and Codependency</h3>\n<p>In a microservices environment or a tightly coupled application stack, it may be necessary to colocate certain pods on the same machine to improve performance, avoid network latency issues, and prevent connection failures. For example, it is generally recommended to run a web server on the same machine as an in-memory cache service or database.</p>\n<h3 id=\"-data-locality\" style=\"position:relative;\"><a href=\"#-data-locality\" aria-label=\" data locality permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìç Data Locality</h3>\n<p>Data-intensive applications may have similar data locality requirements to the previous use case. To ensure faster reads and better write throughput, these applications may need the databases to be deployed on the same machine as the customer-facing application.</p>\n<h3 id=\"-high-availability-and-fault-tolerance\" style=\"position:relative;\"><a href=\"#-high-availability-and-fault-tolerance\" aria-label=\" high availability and fault tolerance permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ High Availability and Fault Tolerance</h3>\n<p>To make application deployments highly available and fault-tolerant, it is a good practice to run pods on nodes deployed in separate availability zones.</p>\n<h2 id=\"Ô∏è-node-taints-and-pods-tolerations\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-node-taints-and-pods-tolerations\" aria-label=\"Ô∏è node taints and pods tolerations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Node Taints and Pods Tolerations</h2>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\" target=\"_blank\" rel=\"noopener noreferrer\">Taints and tolerations</a> provide a powerful mechanism for controlling the allocation of pods to specific nodes in a Kubernetes cluster. The concept is simple yet effective: a taint sets restrictions on a node, dictating which pods can or cannot be scheduled on it, while a toleration allows a pod to resist the effects of taints and be scheduled on specific nodes.</p>\n<h3 id=\"-taints\" style=\"position:relative;\"><a href=\"#-taints\" aria-label=\" taints permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåü Taints</h3>\n<p>A taint is a key-value pair that specifies a node condition and its effect. The effect can be either <code class=\"language-text\">NoSchedule</code> or <code class=\"language-text\">PreferNoSchedule</code>. A <code class=\"language-text\">NoSchedule</code> taint prevents any pod without a matching toleration from being scheduled on the node. A <code class=\"language-text\">PreferNoSchedule</code> taint signals to the scheduler that it should avoid scheduling pods on the node, but does not prevent it.</p>\n<p>You can use the <code class=\"language-text\">kubectl taint</code> command to taint the nodes:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl taint nodes <span class=\"token operator\">&lt;</span>node-name<span class=\"token operator\">></span> <span class=\"token operator\">&lt;</span>key<span class=\"token operator\">>=</span><span class=\"token operator\">&lt;</span>value<span class=\"token operator\">></span>:<span class=\"token operator\">&lt;</span>taint-effect<span class=\"token operator\">></span></code></pre></div>\n<h3 id=\"-tolerations\" style=\"position:relative;\"><a href=\"#-tolerations\" aria-label=\" tolerations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåü Tolerations</h3>\n<p>A toleration is a key-value pair that specifies a node condition and its effect. The effect can be either <code class=\"language-text\">NoExecute</code> or <code class=\"language-text\">Effect</code>. A <code class=\"language-text\">NoExecute</code> toleration prevents a pod from being evicted from a node with a matching taint. An <code class=\"language-text\">Effect</code> toleration allows a pod to be scheduled on a node with a matching taint, even if the pod does not have a toleration for that taint.</p>\n<p>There are pre-defined 3 effects as follows:</p>\n<ul>\n<li><strong>NoSchedule</strong>: Do not place the pods unless they can tolerate the taint.</li>\n<li><strong>PreferNoSchedule</strong>: Try to avoid scheduling the pods that cannot tolerate the taint. Not guaranteed.</li>\n<li><strong>NoExecute</strong>: By the time taint is enabled on the nodes, the existing pods will be terminated if the pods cannot tolerate the taint.</li>\n</ul>\n<p>For instance, you can create a scenario where only pods with critical services, such as controllers, are allowed to run on a particular node.</p>\n<p>Implementing taints and tolerations is quite simple. First, add a taint to a node that needs to apply some non-standard scheduling behavior. For example:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl taint nodes node01 <span class=\"token assign-left variable\">critical</span><span class=\"token operator\">=</span>true:NoSchedule\n<span class=\"token function\">node</span> <span class=\"token string\">\"node01\"</span> tainted</code></pre></div>\n<p>Creating a taint is only the first part of the configuration. To allow pods to be scheduled on a tainted node, we need to add the toleration:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> taint<span class=\"token punctuation\">-</span>toleration\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> taint<span class=\"token punctuation\">-</span>toleration\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0.8</span>\n                <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 4Gi\n            <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3.0</span>\n                <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 22Gi\n        <span class=\"token key atrule\">tolerations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"critical\"</span>\n            <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Equal\"</span>\n            <span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"true\"</span>\n            <span class=\"token key atrule\">effect</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"NoSchedule\"</span></code></pre></div>\n<p>In this example, I added the toleration for the above taint using the \"Equal\" operator. I could also use the \"Exists\" operator, which will apply toleration to any node matching the key of the taint. However, the value doesn't need to be specified.</p>\n<p>An important fact to remember is that the toleration will not guarantee that the pod will only be placed in the tainted node. If the other nodes are not tainted, the above pod can be placed in those nodes as well as the un-tainted nodes are free to accept any pods.</p>\n<h2 id=\"Ô∏è-choosing-a-node-by-a-pod-code-classlanguage-textnodenamecode-code-classlanguage-textnodeselectorcode-and-code-classlanguage-textnodeaffinitycode\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-choosing-a-node-by-a-pod-code-classlanguage-textnodenamecode-code-classlanguage-textnodeselectorcode-and-code-classlanguage-textnodeaffinitycode\" aria-label=\"Ô∏è choosing a node by a pod code classlanguage textnodenamecode code classlanguage textnodeselectorcode and code classlanguage textnodeaffinitycode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóÇÔ∏è Choosing a Node by a Pod: <code class=\"language-text\">nodeName</code>, <code class=\"language-text\">nodeSelector</code>, and <code class=\"language-text\">nodeAffinity</code></h2>\n<p>Another approach is configuring a Pod in such a way that it chooses which Node to run on. For this, we have <code class=\"language-text\">nodeName</code>, <code class=\"language-text\">nodeSelector</code>, <code class=\"language-text\">nodeAffinity</code>, and <code class=\"language-text\">nodeAntiAffinity</code>. <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/\" target=\"_blank\" rel=\"noopener noreferrer\">See Assign Pods to Nodes</a>.</p>\n<h3 id=\"-code-classlanguage-textnodenamecode\" style=\"position:relative;\"><a href=\"#-code-classlanguage-textnodenamecode\" aria-label=\" code classlanguage textnodenamecode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîπ <code class=\"language-text\">nodeName</code></h3>\n<p>The most straightforward way. Has precedence over all others:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx<span class=\"token punctuation\">:</span>latest\n    <span class=\"token key atrule\">nodeName</span><span class=\"token punctuation\">:</span> host01</code></pre></div>\n<h3 id=\"-code-classlanguage-textnodeselectorcode\" style=\"position:relative;\"><a href=\"#-code-classlanguage-textnodeselectorcode\" aria-label=\" code classlanguage textnodeselectorcode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîπ <code class=\"language-text\">nodeSelector</code></h3>\n<p>In essence, <code class=\"language-text\">nodeSelector</code> is a label-based pod-to-node scheduling method where users assign certain labels to nodes and make sure that the <code class=\"language-text\">nodeSelector</code> field matches those labels.</p>\n<p>For example, let's assume that one of the node labels is <code class=\"language-text\">storage=ssd</code> to indicate the type of storage on the node.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl describe <span class=\"token function\">node</span> <span class=\"token string\">\"host01\"</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">Name</span><span class=\"token punctuation\">:</span> host01\n<span class=\"token key atrule\">Roles</span><span class=\"token punctuation\">:</span> node\n<span class=\"token key atrule\">Labels</span><span class=\"token punctuation\">:</span> critical=true<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">...</span></code></pre></div>\n<p>To schedule pods onto the node with this label, specify the <code class=\"language-text\">nodeSelector</code> field with that label in the Pod manifest.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> dev\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx<span class=\"token punctuation\">:</span>latest\n        <span class=\"token key atrule\">imagePullPolicy</span><span class=\"token punctuation\">:</span> IfNotPresent\n    <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">critical</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>Node selectors are the simplest method of advanced pod scheduling. However, they are not very useful when other rules and conditions should be considered during pod scheduling.</p>\n<h3 id=\"-code-classlanguage-textnodeaffinitycode-and-code-classlanguage-textnodeantiaffinitycode\" style=\"position:relative;\"><a href=\"#-code-classlanguage-textnodeaffinitycode-and-code-classlanguage-textnodeantiaffinitycode\" aria-label=\" code classlanguage textnodeaffinitycode and code classlanguage textnodeantiaffinitycode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîπ <code class=\"language-text\">nodeAffinity</code> and <code class=\"language-text\">nodeAntiAffinity</code></h3>\n<p><code class=\"language-text\">nodeAffinity</code> and <code class=\"language-text\">nodeAntiAffinity</code> operate in the same way as <code class=\"language-text\">nodeSelector</code>, but have more flexible capabilities.</p>\n<p>For example, you can set hard or soft launch limits. For a soft limit, the scheduler will try to launch a Pod on the corresponding Node, and if it cannot, it will launch it on another. Accordingly, if you set a hard limit and the scheduler cannot start the Pod on the selected Node, the Pod will remain in Pending status.</p>\n<p>The hard limit is set in the field <code class=\"language-text\">.spec.affinity.nodeAffinity</code> with the <code class=\"language-text\">requiredDuringSchedulingIgnoredDuringExecution</code>, and the soft limit is set with the <code class=\"language-text\">preferredDuringSchedulingIgnoredDuringExecution</code>.</p>\n<p>In the example below, we use node affinity to place pods on nodes in specific availability zones.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> node<span class=\"token punctuation\">-</span>affinity\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nodeAffinity</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">nodeSelectorTerms</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> kubernetes.io/cp<span class=\"token punctuation\">-</span>az<span class=\"token punctuation\">-</span>name\n                        <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                        <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> cp<span class=\"token punctuation\">-</span>1a\n                        <span class=\"token punctuation\">-</span> cp<span class=\"token punctuation\">-</span>1b\n            <span class=\"token key atrule\">preferredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">weight</span><span class=\"token punctuation\">:</span> <span class=\"token number\">7</span>\n                <span class=\"token key atrule\">preference</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> custom<span class=\"token punctuation\">-</span>key\n                        <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                        <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> custom<span class=\"token punctuation\">-</span>value\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> node<span class=\"token punctuation\">-</span>affinity\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> your<span class=\"token punctuation\">-</span>container<span class=\"token punctuation\">-</span>image</code></pre></div>\n<p>\"Hard\" affinity rules are specified under the <code class=\"language-text\">requiredDuringSchedulingIgnoredDuringExecution</code> field of the <code class=\"language-text\">nodeAffinity</code> section of the pod manifest. In this example, the scheduler is instructed to place the pod only on nodes with the label that has a key <code class=\"language-text\">kubernetes.io/cp-az-name</code> and values <code class=\"language-text\">cp-1a</code> or <code class=\"language-text\">cp-1b</code>.</p>\n<p>To achieve this, the <code class=\"language-text\">In</code> logical operator is used to filter the array of existing label values. Other operators include <code class=\"language-text\">NotIn</code>, <code class=\"language-text\">Exists</code>, <code class=\"language-text\">DoesNotExist</code>, <code class=\"language-text\">Gt</code>, and <code class=\"language-text\">Lt</code>.</p>\n<p>The \"soft\" rule is specified under the <code class=\"language-text\">preferredDuringSchedulingIgnoredDuringExecution</code> field of the spec. In this example, it states that among the nodes that meet \"hard\" criteria, nodes with a label that has a key named <code class=\"language-text\">custom-key</code> and the value <code class=\"language-text\">custom-value</code> are preferred. However, if no such nodes exist, scheduling pods to other candidates that meet the \"hard\" criteria is acceptable.</p>\n<p>It's a good practice to construct node affinity rules incorporating both \"hard\" and \"soft\" rules. Following this \"best-effort\" approach makes deployment scheduling more flexible and predictable.</p>\n<h3 id=\"-code-classlanguage-textpodaffinitycode-and-code-classlanguage-textpodantiaffinitycode\" style=\"position:relative;\"><a href=\"#-code-classlanguage-textpodaffinitycode-and-code-classlanguage-textpodantiaffinitycode\" aria-label=\" code classlanguage textpodaffinitycode and code classlanguage textpodantiaffinitycode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîπ <code class=\"language-text\">podAffinity</code> and <code class=\"language-text\">podAntiAffinity</code></h3>\n<p>Similar to selecting a Node using hard and soft limits, you can adjust Pod Affinity depending on what labels Pods already running on the Node will have. See <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity\" target=\"_blank\" rel=\"noopener noreferrer\">Inter-pod affinity and anti-affinity</a>.</p>\n<p>Inter-pod affinity is defined similarly to node affinity. In this case, the <code class=\"language-text\">podAffinity</code> field of the pod spec is used.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> example<span class=\"token punctuation\">-</span>pod<span class=\"token punctuation\">-</span>affinity\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">podAffinity</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> security\n                        <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                        <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> S1\n                <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> failure<span class=\"token punctuation\">-</span>domain.beta.kubernetes.io/zone\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> pod<span class=\"token punctuation\">-</span>affinity\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> your<span class=\"token punctuation\">-</span>container</code></pre></div>\n<p>Similar to node affinity, pod affinity supports match expressions and logical operators. In this case, they are applied to the label selector of the pods running on a particular node. If the specified expression matches the pod label of the target pod, a new pod is collocated with the target pod on the same machine.</p>\n<p>It's possible to repel pods from each other via the pod anti-affinity feature. As mentioned above, one of the best practices in Kubernetes is to avoid a single point of failure by spreading pods across different availability zones. Similar behavior can be configured in the anti-affinity part of the pod spec. For pod anti-affinity, two pods are needed:</p>\n<p>The first pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> s1\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">security</span><span class=\"token punctuation\">:</span> s1\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> c1\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> first<span class=\"token punctuation\">-</span>image</code></pre></div>\n<p>Note that the first pod has the label <code class=\"language-text\">security: s1</code>.</p>\n<p>The second pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> s2\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">podAntiAffinity</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> security\n                        <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                        <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> s1\n                <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> kubernetes.io/hostname\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> pod<span class=\"token punctuation\">-</span>anti<span class=\"token punctuation\">-</span>affinity\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> second<span class=\"token punctuation\">-</span>image</code></pre></div>\n<p>The second pod refers to the label selector <code class=\"language-text\">security: s1</code> under the <code class=\"language-text\">spec.affinity.podAntiAffinity</code>. As a consequence, this pod won't be scheduled to a node that already hosts any pods with the label <code class=\"language-text\">security: s1</code>.</p>\n<h3 id=\"-code-classlanguage-texttopologyspreadconstraintscode\" style=\"position:relative;\"><a href=\"#-code-classlanguage-texttopologyspreadconstraintscode\" aria-label=\" code classlanguage texttopologyspreadconstraintscode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîπ <code class=\"language-text\">topologySpreadConstraints</code></h3>\n<p>First, imagine a cluster of twenty nodes. You want to run a workload that automatically scales its replica number. It can scale anywhere from two to twenty Pods, and you want to run those replicas on as many separate nodes as possible. This approach helps to minimize the risk of a node failure affecting the workload.</p>\n<p>Then let's think about an application with fifteen replicas running on three nodes in the same Availability Zone, with five Pods on each node. You've mitigated the node failure risk, but clients interacting with the workload come from three distinct zones‚Ää‚Äî‚Ääand traffic spanning different AZs results in higher latency and network costs.</p>\n<p>You can reduce them by distributing Pods across nodes in different AZs and routing clients to the instances inside the relevant zone. Deploying the workload in multiple zones in addition to running it on several nodes further decreases the risk of a failure affecting your Pods.</p>\n<p>Normally, you'd want to distribute workloads evenly across every failure domain. You can configure that with pod topology constraints‚Ää‚Äî‚Ääand to do so, use the <code class=\"language-text\">spec.topologySpreadConstraints</code> field.</p>\n<h4 id=\"how-pod-topology-spread-constraints-work\" style=\"position:relative;\"><a href=\"#how-pod-topology-spread-constraints-work\" aria-label=\"how pod topology spread constraints work permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How pod topology spread constraints work</h4>\n<p>Here's an example of a pod topology spread constraint:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> example<span class=\"token punctuation\">-</span>pod\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">topologySpreadConstraints</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">maxSkew</span><span class=\"token punctuation\">:</span> &lt;integer<span class=\"token punctuation\">></span>\n        <span class=\"token key atrule\">minDomains</span><span class=\"token punctuation\">:</span> &lt;integer<span class=\"token punctuation\">></span> <span class=\"token comment\"># optional</span>\n        <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> &lt;string<span class=\"token punctuation\">></span>\n        <span class=\"token key atrule\">whenUnsatisfiable</span><span class=\"token punctuation\">:</span> &lt;string<span class=\"token punctuation\">></span>\n        <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span> &lt;object<span class=\"token punctuation\">></span>\n        <span class=\"token key atrule\">matchLabelKeys</span><span class=\"token punctuation\">:</span> &lt;list<span class=\"token punctuation\">></span> <span class=\"token comment\"># optional</span>\n        <span class=\"token key atrule\">nodeAffinityPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>Honor<span class=\"token punctuation\">|</span>Ignore<span class=\"token punctuation\">]</span> <span class=\"token comment\"># optional</span>\n        <span class=\"token key atrule\">nodeTaintsPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>Honor<span class=\"token punctuation\">|</span>Ignore<span class=\"token punctuation\">]</span> <span class=\"token comment\"># optional</span></code></pre></div>\n<p>You can find a full explanation of each element in the <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes documentation</a>. For now, let's briefly outline the obligatory fields:</p>\n<ul>\n<li><strong><code class=\"language-text\">maxSkew</code></strong>: The degree to which your Pods can be distributed unevenly across all zones. Its value must be more than zero.</li>\n<li><strong><code class=\"language-text\">topologyKey</code></strong>: The key of node labels. Nodes with the same label and values belong to the same topology. Each topology instance is a domain to which the scheduler tries to assign a balanced number of pods.</li>\n<li><strong><code class=\"language-text\">whenUnsatisfiable</code></strong>: Lets you decide what to do with a Pod when it doesn't satisfy your spread constraint:\n<ul>\n<li><code class=\"language-text\">DoNotSchedule</code> instructs the scheduler not to schedule it.</li>\n<li><code class=\"language-text\">ScheduleAnyway</code> tells the scheduler to schedule it and prioritize the nodes minimizing the skew.</li>\n</ul>\n</li>\n<li><strong><code class=\"language-text\">labelSelector</code></strong>: Allows finding matching Pods. The number of Pods in their corresponding topology domain is based on the Pods matching the label selector.</li>\n</ul>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìë Conclusion</h2>\n<p>Advanced pod scheduling in Kubernetes is a powerful tool that can be used to improve the performance, availability, and resilience of your containerized applications. By understanding the different features available and how to use them, you can control the placement of your pods to meet the specific needs of your application.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":9,"rawMarkdownBody":"\n> **Pod to Node Scheduling Strategies üê≥**\n\n## üìå Overview\n\nBy default, Kubernetes places pods randomly across the available nodes in the cluster. However, there are many scenarios where you may need to control the placement of your pods on specific nodes.\n\nFor example, you may want to place pods that require specific hardware resources (e.g., GPUs) on nodes that have those resources available or avoid placing pods on nodes that are running other critical workloads.\n\nKubernetes provides a number of features for controlling pod placement, including: Node selectors, Affinity and anti-affinity rules, Taints and tolerations.\n\nIn this article, we will discuss the different approaches to advanced pod scheduling in Kubernetes and provide examples of how to use them to solve common use cases.\n\n## Use Cases for Pod-to-Node Scheduling in Kubernetes\n\nIn the Kubernetes environment, it is often necessary to customize how pods are scheduled to nodes. Here are some of the most common scenarios where advanced pod scheduling can be beneficial:\n\n### üöÄ Running Pods on Nodes with Dedicated Hardware\n\nSome Kubernetes applications may have specific hardware requirements. For example, pods running machine learning jobs may require high-performance GPUs instead of CPUs, while Elasticsearch pods may perform better on SSDs than HDDs. As a result, it is best practice for any resource-aware Kubernetes cluster management strategy to assign pods to the nodes with the appropriate hardware.\n\n### ü§ù Pod Colocation and Codependency\n\nIn a microservices environment or a tightly coupled application stack, it may be necessary to colocate certain pods on the same machine to improve performance, avoid network latency issues, and prevent connection failures. For example, it is generally recommended to run a web server on the same machine as an in-memory cache service or database.\n\n### üìç Data Locality\n\nData-intensive applications may have similar data locality requirements to the previous use case. To ensure faster reads and better write throughput, these applications may need the databases to be deployed on the same machine as the customer-facing application.\n\n### üîÑ High Availability and Fault Tolerance\n\nTo make application deployments highly available and fault-tolerant, it is a good practice to run pods on nodes deployed in separate availability zones.\n\n## üõ†Ô∏è Node Taints and Pods Tolerations\n\n[Taints and tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) provide a powerful mechanism for controlling the allocation of pods to specific nodes in a Kubernetes cluster. The concept is simple yet effective: a taint sets restrictions on a node, dictating which pods can or cannot be scheduled on it, while a toleration allows a pod to resist the effects of taints and be scheduled on specific nodes.\n\n### üåü Taints\n\nA taint is a key-value pair that specifies a node condition and its effect. The effect can be either `NoSchedule` or `PreferNoSchedule`. A `NoSchedule` taint prevents any pod without a matching toleration from being scheduled on the node. A `PreferNoSchedule` taint signals to the scheduler that it should avoid scheduling pods on the node, but does not prevent it.\n\nYou can use the `kubectl taint` command to taint the nodes:\n\n```shell\nkubectl taint nodes <node-name> <key>=<value>:<taint-effect>\n```\n\n### üåü Tolerations\n\nA toleration is a key-value pair that specifies a node condition and its effect. The effect can be either `NoExecute` or `Effect`. A `NoExecute` toleration prevents a pod from being evicted from a node with a matching taint. An `Effect` toleration allows a pod to be scheduled on a node with a matching taint, even if the pod does not have a toleration for that taint.\n\nThere are pre-defined 3 effects as follows:\n\n- **NoSchedule**: Do not place the pods unless they can tolerate the taint.\n- **PreferNoSchedule**: Try to avoid scheduling the pods that cannot tolerate the taint. Not guaranteed.\n- **NoExecute**: By the time taint is enabled on the nodes, the existing pods will be terminated if the pods cannot tolerate the taint.\n\nFor instance, you can create a scenario where only pods with critical services, such as controllers, are allowed to run on a particular node.\n\nImplementing taints and tolerations is quite simple. First, add a taint to a node that needs to apply some non-standard scheduling behavior. For example:\n\n```shell\nkubectl taint nodes node01 critical=true:NoSchedule\nnode \"node01\" tainted\n```\n\nCreating a taint is only the first part of the configuration. To allow pods to be scheduled on a tainted node, we need to add the toleration:\n\n```yaml\napiVersion: v1\nmetadata:\n    name: taint-toleration\nspec:\n    containers:\n    - name: taint-toleration\n        image: nginx\n        resources:\n            requests:\n                cpu: 0.8\n                memory: 4Gi\n            limits:\n                cpu: 3.0\n                memory: 22Gi\n        tolerations:\n        - key: \"critical\"\n            operator: \"Equal\"\n            value: \"true\"\n            effect: \"NoSchedule\"\n```\n\nIn this example, I added the toleration for the above taint using the \"Equal\" operator. I could also use the \"Exists\" operator, which will apply toleration to any node matching the key of the taint. However, the value doesn't need to be specified.\n\nAn important fact to remember is that the toleration will not guarantee that the pod will only be placed in the tainted node. If the other nodes are not tainted, the above pod can be placed in those nodes as well as the un-tainted nodes are free to accept any pods.\n\n## üóÇÔ∏è Choosing a Node by a Pod: `nodeName`, `nodeSelector`, and `nodeAffinity`\n\nAnother approach is configuring a Pod in such a way that it chooses which Node to run on. For this, we have `nodeName`, `nodeSelector`, `nodeAffinity`, and `nodeAntiAffinity`. [See Assign Pods to Nodes](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/).\n\n### üîπ `nodeName`\n\nThe most straightforward way. Has precedence over all others:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: nginx\nspec:\n    containers:\n    - name: my-nginx\n        image: nginx:latest\n    nodeName: host01\n```\n\n### üîπ `nodeSelector`\n\nIn essence, `nodeSelector` is a label-based pod-to-node scheduling method where users assign certain labels to nodes and make sure that the `nodeSelector` field matches those labels.\n\nFor example, let's assume that one of the node labels is `storage=ssd` to indicate the type of storage on the node.\n\n```shell\nkubectl describe node \"host01\"\n```\n\n```yaml\nName: host01\nRoles: node\nLabels: critical=true,\n...\n```\n\nTo schedule pods onto the node with this label, specify the `nodeSelector` field with that label in the Pod manifest.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: nginx\n    labels:\n        env: dev\nspec:\n    containers:\n    - name: my-nginx\n        image: nginx:latest\n        imagePullPolicy: IfNotPresent\n    nodeSelector:\n        critical: true\n```\n\nNode selectors are the simplest method of advanced pod scheduling. However, they are not very useful when other rules and conditions should be considered during pod scheduling.\n\n### üîπ `nodeAffinity` and `nodeAntiAffinity`\n\n`nodeAffinity` and `nodeAntiAffinity` operate in the same way as `nodeSelector`, but have more flexible capabilities.\n\nFor example, you can set hard or soft launch limits. For a soft limit, the scheduler will try to launch a Pod on the corresponding Node, and if it cannot, it will launch it on another. Accordingly, if you set a hard limit and the scheduler cannot start the Pod on the selected Node, the Pod will remain in Pending status.\n\nThe hard limit is set in the field `.spec.affinity.nodeAffinity` with the `requiredDuringSchedulingIgnoredDuringExecution`, and the soft limit is set with the `preferredDuringSchedulingIgnoredDuringExecution`.\n\nIn the example below, we use node affinity to place pods on nodes in specific availability zones.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: node-affinity\nspec:\n    affinity:\n        nodeAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                - matchExpressions:\n                    - key: kubernetes.io/cp-az-name\n                        operator: In\n                        values:\n                        - cp-1a\n                        - cp-1b\n            preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 7\n                preference:\n                    matchExpressions:\n                    - key: custom-key\n                        operator: In\n                        values:\n                        - custom-value\n    containers:\n    - name: node-affinity\n        image: your-container-image\n```\n\n\"Hard\" affinity rules are specified under the `requiredDuringSchedulingIgnoredDuringExecution` field of the `nodeAffinity` section of the pod manifest. In this example, the scheduler is instructed to place the pod only on nodes with the label that has a key `kubernetes.io/cp-az-name` and values `cp-1a` or `cp-1b`.\n\nTo achieve this, the `In` logical operator is used to filter the array of existing label values. Other operators include `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`.\n\nThe \"soft\" rule is specified under the `preferredDuringSchedulingIgnoredDuringExecution` field of the spec. In this example, it states that among the nodes that meet \"hard\" criteria, nodes with a label that has a key named `custom-key` and the value `custom-value` are preferred. However, if no such nodes exist, scheduling pods to other candidates that meet the \"hard\" criteria is acceptable.\n\nIt's a good practice to construct node affinity rules incorporating both \"hard\" and \"soft\" rules. Following this \"best-effort\" approach makes deployment scheduling more flexible and predictable.\n\n### üîπ `podAffinity` and `podAntiAffinity`\n\nSimilar to selecting a Node using hard and soft limits, you can adjust Pod Affinity depending on what labels Pods already running on the Node will have. See [Inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity).\n\nInter-pod affinity is defined similarly to node affinity. In this case, the `podAffinity` field of the pod spec is used.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: example-pod-affinity\nspec:\n    affinity:\n        podAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                    matchExpressions:\n                    - key: security\n                        operator: In\n                        values:\n                        - S1\n                topologyKey: failure-domain.beta.kubernetes.io/zone\n    containers:\n    - name: pod-affinity\n        image: your-container\n```\n\nSimilar to node affinity, pod affinity supports match expressions and logical operators. In this case, they are applied to the label selector of the pods running on a particular node. If the specified expression matches the pod label of the target pod, a new pod is collocated with the target pod on the same machine.\n\nIt's possible to repel pods from each other via the pod anti-affinity feature. As mentioned above, one of the best practices in Kubernetes is to avoid a single point of failure by spreading pods across different availability zones. Similar behavior can be configured in the anti-affinity part of the pod spec. For pod anti-affinity, two pods are needed:\n\nThe first pod:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: s1\n    labels:\n        security: s1\nspec:\n    containers:\n    - name: c1\n        image: first-image\n```\n\nNote that the first pod has the label `security: s1`.\n\nThe second pod:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: s2\nspec:\n    affinity:\n        podAntiAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                    matchExpressions:\n                    - key: security\n                        operator: In\n                        values:\n                        - s1\n                topologyKey: kubernetes.io/hostname\n    containers:\n    - name: pod-anti-affinity\n        image: second-image\n```\n\nThe second pod refers to the label selector `security: s1` under the `spec.affinity.podAntiAffinity`. As a consequence, this pod won't be scheduled to a node that already hosts any pods with the label `security: s1`.\n\n### üîπ `topologySpreadConstraints`\n\nFirst, imagine a cluster of twenty nodes. You want to run a workload that automatically scales its replica number. It can scale anywhere from two to twenty Pods, and you want to run those replicas on as many separate nodes as possible. This approach helps to minimize the risk of a node failure affecting the workload.\n\nThen let's think about an application with fifteen replicas running on three nodes in the same Availability Zone, with five Pods on each node. You've mitigated the node failure risk, but clients interacting with the workload come from three distinct zones‚Ää‚Äî‚Ääand traffic spanning different AZs results in higher latency and network costs.\n\nYou can reduce them by distributing Pods across nodes in different AZs and routing clients to the instances inside the relevant zone. Deploying the workload in multiple zones in addition to running it on several nodes further decreases the risk of a failure affecting your Pods.\n\nNormally, you'd want to distribute workloads evenly across every failure domain. You can configure that with pod topology constraints‚Ää‚Äî‚Ääand to do so, use the `spec.topologySpreadConstraints` field.\n\n#### How pod topology spread constraints work\n\nHere's an example of a pod topology spread constraint:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: example-pod\nspec:\n    topologySpreadConstraints:\n    - maxSkew: <integer>\n        minDomains: <integer> # optional\n        topologyKey: <string>\n        whenUnsatisfiable: <string>\n        labelSelector: <object>\n        matchLabelKeys: <list> # optional\n        nodeAffinityPolicy: [Honor|Ignore] # optional\n        nodeTaintsPolicy: [Honor|Ignore] # optional\n```\n\nYou can find a full explanation of each element in the [Kubernetes documentation](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/). For now, let's briefly outline the obligatory fields:\n\n- **`maxSkew`**: The degree to which your Pods can be distributed unevenly across all zones. Its value must be more than zero.\n- **`topologyKey`**: The key of node labels. Nodes with the same label and values belong to the same topology. Each topology instance is a domain to which the scheduler tries to assign a balanced number of pods.\n- **`whenUnsatisfiable`**: Lets you decide what to do with a Pod when it doesn't satisfy your spread constraint:\n    - `DoNotSchedule` instructs the scheduler not to schedule it.\n    - `ScheduleAnyway` tells the scheduler to schedule it and prioritize the nodes minimizing the skew.\n- **`labelSelector`**: Allows finding matching Pods. The number of Pods in their corresponding topology domain is based on the Pods matching the label selector.\n\n## üìë Conclusion\n\nAdvanced pod scheduling in Kubernetes is a powerful tool that can be used to improve the performance, availability, and resilience of your containerized applications. By understanding the different features available and how to use them, you can control the placement of your pods to meet the specific needs of your application.\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1812},"frontmatter":{"id":"db7793ca75cc619408d46e87","path":"/blog/kubernetes-pod-scheduling-strategies/","humanDate":"Oct 26, 2024","fullDate":"2024-10-26","title":"Pod Scheduling in Kubernetes: Control the Placement of Your Pods ‚ò∏Ô∏è","keywords":["Kubernetes","Pod Scheduling","Node Placement","Resource Management"],"excerpt":"Explore effective strategies for pod scheduling in Kubernetes to optimize resource utilization.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAv/EABYBAQEBAAAAAAAAAAAAAAAAAAIAA//aAAwDAQACEAMQAAABfmqbKqHAX//EABoQAAIDAQEAAAAAAAAAAAAAAAEDAAISESH/2gAIAQEAAQUCYfV8BzGr2E1AtP/EABYRAAMAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAwEBPwE1f//EABcRAQEBAQAAAAAAAAAAAAAAAAECEBH/2gAIAQIBAT8BJlO5/8QAGRAAAwEBAQAAAAAAAAAAAAAAARExABBB/9oACAEBAAY/AkYcnfOBUZuHn//EABwQAAEEAwEAAAAAAAAAAAAAABEAARAhMUFRcf/aAAgBAQABPyFsW/RDbHx4h1gpFjQAj//aAAwDAQACAAMAAAAQjA//xAAYEQACAwAAAAAAAAAAAAAAAAAAAREhQf/aAAgBAwEBPxDVOkSf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAgEBPxCwN3CH/8QAHBABAAIDAQEBAAAAAAAAAAAAAQARITFBcVGB/9oACAEBAAE/ENB81hSmh9iQAQbWmyzFa1HPh5FUguWxf0j4YqR33P7ELn//2Q=="},"images":{"fallback":{"src":"/static/e245505a7d40913c4b5cd58e39c395a9/29e1a/schedule-cover.jpg","srcSet":"/static/e245505a7d40913c4b5cd58e39c395a9/29e1a/schedule-cover.jpg 736w","sizes":"100vw"},"sources":[{"srcSet":"/static/e245505a7d40913c4b5cd58e39c395a9/66905/schedule-cover.webp 736w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.7567934782608695}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/ephemeral-environments-kubernetes-cicd/","title":"Ephemeral Environments for Cost-Effective and Reliable CI/CD Pipelines with Kubernetes üê≥","date":"2024-10-26 17:34:00"},"excerpt":"Build Reliable Software Faster with Ephemeral Environments and Kubernetes üêã üèû Introduction Building high-quality software quickly is‚Ä¶","html":"<blockquote>\n<p><strong>Build Reliable Software Faster with Ephemeral Environments and Kubernetes üêã</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèû Introduction</h2>\n<p>Building high-quality software quickly is extremely important nowadays. Testing your app in a real-world environment is key to finding and fixing bugs early on. But setting up your CI/CD pipeline to test in a real-world environment can be tricky and expensive.</p>\n<p>Kubernetes, a popular container orchestration platform, offers a solution through ephemeral environments. With Kubernetes, you can create temporary real-world environments on demand, letting you test and deploy your app without the hassle of managing permanent infrastructure.</p>\n<p>This article digs into how to use Kubernetes to set up ephemeral environments that mirror your real-world environment, ensuring thorough testing and smooth deployment. Learn how to streamline your CI/CD pipeline, improve code quality, and save money with Kubernetes.</p>\n<h2 id=\"-what-is-a-cicd-pipeline\" style=\"position:relative;\"><a href=\"#-what-is-a-cicd-pipeline\" aria-label=\" what is a cicd pipeline permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìò What is a CI/CD Pipeline?</h2>\n<p>A CI/CD pipeline is a streamlined approach to software development that enables developers to rapidly integrate code changes into their applications. This methodology promotes continuous integration, testing, and deployment, resulting in the delivery of high-quality software in a timely manner.</p>\n<h3 id=\"-benefits-of-cicd-pipelines\" style=\"position:relative;\"><a href=\"#-benefits-of-cicd-pipelines\" aria-label=\" benefits of cicd pipelines permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèó Benefits of CI/CD Pipelines</h3>\n<p>CI/CD pipelines offer <a href=\"https://about.gitlab.com/blog/2022/02/15/ten-reasons-why-your-business-needs-ci-cd/\" target=\"_blank\" rel=\"noopener noreferrer\">numerous advantages</a>, including:</p>\n<ul>\n<li><strong>Efficient Development Cycle:</strong> CI/CD pipelines automate repetitive tasks, streamlining the development process and reducing manual effort. Developers can seamlessly integrate code changes, eliminating bottlenecks and accelerating feature delivery.</li>\n<li><strong>Early Bug Detection:</strong> Automated testing incorporated into the CI/CD pipeline facilitates early identification of bugs and issues. This approach minimizes the cost of bug fixes and enhances overall code quality.</li>\n<li><strong>Consistent Test Coverage:</strong> Automated tests are executed with each code commit, ensuring consistent test coverage and continuous improvement of software quality. This proactive approach safeguards against regressions and promotes a reliable product.</li>\n<li><strong>Enhanced Collaboration:</strong> CI/CD pipelines provide a transparent and traceable record of development activities, fostering collaboration among team members. Developers can easily track changes, identify contributors, and maintain a cohesive development process.</li>\n</ul>\n<h2 id=\"Ô∏è-ephemeral-environments-agile-testing-and-seamless-deployment\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-ephemeral-environments-agile-testing-and-seamless-deployment\" aria-label=\"Ô∏è ephemeral environments agile testing and seamless deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>„äôÔ∏è Ephemeral Environments: Agile Testing and Seamless Deployment</h2>\n<p>Ephemeral environments are temporary, isolated spaces that provide developers with a safe and controlled environment to test and deploy microservices without disrupting the production environment. Kubernetes, a popular container orchestration platform, is an ideal tool for creating ephemeral environments due to its ability to manage and scale containerized applications.</p>\n<p>By using custom resources in Kubernetes, developers can define configurations for these environments, including resource requirements, enabling rapid and efficient creation and teardown. This approach facilitates agile development and enables teams to deliver code changes reliably and confidently.</p>\n<h2 id=\"-key-components-of-kubernetes-cicd-pipeline\" style=\"position:relative;\"><a href=\"#-key-components-of-kubernetes-cicd-pipeline\" aria-label=\" key components of kubernetes cicd pipeline permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîë Key Components of Kubernetes CI/CD Pipeline</h2>\n<p>Kubernetes is a powerful platform for modern container-based application deployments. As such, there are many components that should be considered when building an effective CI/CD pipeline with Kubernetes. Several key components contribute to the success of a Kubernetes CI/CD pipeline:</p>\n<ul>\n<li><strong>Containers:</strong> Encapsulated self-contained software units that package code and dependencies, fostering rapid and consistent application deployment across environments.</li>\n<li><strong>Operating Clusters:</strong> Groups of worker nodes that execute containerized applications. Autoscaling capabilities ensure horizontal scalability on demand, handling increased traffic or resource strain.</li>\n<li><strong>Version Control System (VCS):</strong> Facilitates the management of source code changes, enabling developers to push updates seamlessly into shared repositories.</li>\n<li><strong>Configuration Management:</strong> Tracks changes in VCSs, providing insights into code version history. Enables the deployment of updates across networks, simplifying infrastructure management.</li>\n<li><strong>Image Registries:</strong> Centralized repositories for storing container images, streamlining access during CI/CD processes.</li>\n<li><strong>Security Considerations:</strong> Protect sensitive data throughout the entire CI/CD pipeline, from source code repositories to production deployments.</li>\n<li><strong>Continuous Monitoring and Observability:</strong> Leverage tools like <a href=\"https://prometheus.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Prometheus</a> to monitor application performance in real-time, enabling proactive issue detection and resolution.</li>\n</ul>\n<h2 id=\"-cicd-and-kubernetes-best-practices\" style=\"position:relative;\"><a href=\"#-cicd-and-kubernetes-best-practices\" aria-label=\" cicd and kubernetes best practices permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ CI/CD and Kubernetes Best Practices</h2>\n<p>When it comes to setting up a Kubernetes-based CI/CD pipeline, here are some best practices:</p>\n<ol>\n<li>\n<p><strong>Use GitOps:</strong><br>\n<a href=\"https://cloudnativenow.com/topics/gitops-workflows-and-principles-for-kubernetes/\" target=\"_blank\" rel=\"noopener noreferrer\">GitOps</a> makes use of Git version control so that all operations related to deployments are properly tracked and monitored to ensure reliable deployment processes. This helps with the management of configuration files as well as keeping track of all versions deployed, thus improving reliability.</p>\n</li>\n<li>\n<p><strong>Use Helm for Packaging Applications:</strong><br>\n<a href=\"https://helm.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">Helm</a> simplifies package management on Kubernetes by providing packaged applications called charts. It makes it easier for developers to create repeatable deployments while also allowing them to customize their own applications without having to write any additional code or scripts.</p>\n</li>\n<li>\n<p><strong>Follow Security Best Practices:</strong><br>\nSecurity within a Kubernetes environment should never be overlooked, as it's often one of the biggest threats facing modern enterprises when dealing with sensitive data. Implementing Kubernetes security best practices like authentication models and authorization policies helps secure clusters against malicious users attempting unauthorized access or activities on resources managed by Kubernetes clusters.</p>\n</li>\n<li>\n<p><strong>Use Canary/Blue-Green Deployment Patterns:</strong><br>\nBy using these patterns, you can increase the reliability and stability of your production environment while making sure that any potential issues can be identified and addressed without impacting user experience or functionality.</p>\n<ul>\n<li><strong>Canary Deployment:</strong> <a href=\"https://cloud.google.com/blog/topics/developers-practitioners/canary-deployments-using-kubernetes-gateway-api-flagger-and-google-cloud-deploy\" target=\"_blank\" rel=\"noopener noreferrer\">Allows</a> only a small portion of users to access new features, enabling quick rollbacks if the update results in undesirable behavior.</li>\n<li><strong>Blue-Green Deployments:</strong> Allow you to switch traffic between two identical versions of an application so that older services can still run until any major bugs have been dealt with successfully during the testing phase.</li>\n</ul>\n</li>\n<li>\n<p><strong>Avoid Hardcoding Secrets and Configurations in Containers:</strong><br>\nContainer images should not contain confidential information such as passwords, API keys, or tokens. Instead, you should store this sensitive information in an external secret store such as <a href=\"https://aws.amazon.com/secrets-manager/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Secrets Manager</a> or <a href=\"https://www.vaultproject.io/\" target=\"_blank\" rel=\"noopener noreferrer\">HashiCorp Vault</a> and retrieve it during the deployment process using tools like Helm Charts or kubectl. This will ensure that these important credentials are encrypted and kept separate from your container image, which could be shared with other services or publicly exposed if it happens to become compromised.</p>\n</li>\n</ol>\n<h2 id=\"setup-and-hands-on\" style=\"position:relative;\"><a href=\"#setup-and-hands-on\" aria-label=\"setup and hands on permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Setup and Hands-On</h2>\n<p>We'll be showcasing two approaches. The first is to have a shared Kubernetes cluster, where each ephemeral environment is a separate namespace but the underlying cluster is the same. The second is to have a separate cluster for each ephemeral environment. The first approach is more cost-effective, while the second provides more separation between different environments if that is needed for compliance.</p>\n<p>You can find the GitHub Repository for that demo <a href=\"https://github.com/your-repo-link\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n<p>In this demo, we will have a system that consists of <strong>2 microservices</strong> and a single <strong>Mongo database</strong> for storage. We will open a Pull Request (PR) on one of the microservices, and by opening this PR, a new environment should be created on Kubernetes.</p>\n<p>We will use <a href=\"https://www.vcluster.com/\" target=\"_blank\" rel=\"noopener noreferrer\">vcluster</a> for creating and destroying the ephemeral environment. After finishing our testing, we can easily close or merge the pull request, which will trigger another pipeline to destroy the ephemeral environment.</p>\n<p>I have deployed these workloads locally on a Kubernetes cluster. The pipelines are created using GitHub Actions workflows and Kustomize to easily deploy the whole system.</p>\n<p>For development environments, we need to create pipelines to automate the creation of the ephemeral environment for each microservice. Our pipeline should have the following steps:</p>\n<ol>\n<li><strong>Checkout the code</strong></li>\n<li><strong>Build Docker image</strong></li>\n<li><strong>Push the new Docker image</strong></li>\n<li><strong>Update our Kubernetes manifests</strong></li>\n<li><strong>Deploy a new ephemeral environment</strong></li>\n<li><strong>Deploy the whole system on this environment</strong></li>\n<li><strong>Provide access to this environment</strong></li>\n</ol>\n<p>This is not the ultimate pipeline for your development workflow, but these steps are just for demonstration.</p>\n<p>We will use <a href=\"https://www.vcluster.com/\" target=\"_blank\" rel=\"noopener noreferrer\">vcluster</a> to create fully isolated Kubernetes environments. Check the GitHub Action workflow file below:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 201.1764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAoCAIAAABxU02MAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEWElEQVR42o1W227bRhDNNxQFaouXvc3u7IVLUpLlJJJTtElfin5Y46QxUP9bUyQOCjhP7l/0LCUbrsE4WYyIlcTDncuZM3zi/7wJ5x/49Xv3+97+wpVhr//+sr3nNx/95c0Tfvd59fI3MyYZ1iqOKq2KxaXgPG++F8Tq6a/+4jPA13bzE5G2MRlrtdHaGKVkWx+39WLGmqo9/l7mZ/7iuoD1yUtDRpETUjZ13TRN27RtK+ZNyLY6lsOLw8nm5OfQD5xHG5KytuDKaucX/qiO5HA2gS8+i2GntVRklTH4a3/4dCnrMTA+bX7OMaXNNi43cbVJOTt2OXvvLZGRUj4K7p75PKTTXRiWcD4vV6t1//TpmFJA5h4G8AAs+q0xyHZnQ2ec1+TgbVXV3+q2bhufBuNjAeNJWn1TwsrJ+Tm8g8Mu9dwXz6U21aKaOXbWbdXUnHrygTi4gOz5lNha+rrbAINQYXXKebAhuoR8h2HsOPBMxWfAoiEE7INGtUvBSZJTxhrnkIFvAIdkY9bWGutAcmRuv0Hyvg42LmjHSLg0BJILpVupRLFH3Zb9VsuWU8Y5zhJrZbV0WgajWEtdwIWv89ym5ZkVVerHbhhz7lOMKaYucp+7DJbFSOxRCKngfzNDEvwS1s94WPO4lkR7cuGsup26s3SiKJ1aSieaxZG4O1kNO5KN8xF5RsxoBibNRDBEAUODMDZE3llHBm7q8QV6uYD1sAuk0rB0eeBhxSHFYQz90rK3ji1oA/I4prL3hblFDM6KkuzdbuoKJClK5CMKBlUohlCNNhxMKBRw6Bwf2LOoF6Lf3ZVq1y6OQEwwBKxWE090MVJYBvrm8BVlL3ERFfBwB85b0VTIFrpSWy634or74CeVDpsSVR/EBSlc/HCvzsNW6xbawQMaq0O6DEJzStuDKbrbSOW0aI/vgcedkLVmpyzBWiWKibb536oPHfaAJKrfgkmxH9lxF7w3JkzVQmGCZ0iZZ8bP6FE2mrRGnW/B765p9aO3JnQ9bl2msOp89rZz1HnuYowBpQt9lzpvPWno4T1wiflMaU0xG0ZJIkSvqg+rqsq2uZXieRlCV/nliet63y9NzDEF32Wbct9HDJMJJr7YkphMvqS6J7DSWu/ARIdQO7DLkXhcPTHa4rhm6DZoRCClN2SRb8D2kl/fyfCsAIZxA+n0E72Xm1U/ZPALOmK1Iq0ceKpkM3My3G5qXbrKTc+Wk+wJ6DHqLdsGcgBJkGWONQ/rrIetVaIbVx6JcjY6SphVqDDiZ49ioQ1LwqBNmFtlxN6rs9u88qTgZ0gdGpaBR22xZ4Y2IRBkEQMUjQW2S3WPJBO3d0rhXSEo6yTGDaa081C/UuGp2PtUHRj6MOa4xtOgYdEzmAQFi4ExBnATXgJu7XaPN4uj72R3ehADsX4l0Lvd2nIM3Wh8loZb8vNmQ6u0PPllOvnyX/7jH3575c4/ujef7PkVn3/it4/bFSD+8uY/92DYLUZALbkAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"GitHub CI\" title=\"\" src=\"/static/57dd22ebb6ba0635492bdb694b2dd12f/c5bb3/gh-ci.png\" srcset=\"/static/57dd22ebb6ba0635492bdb694b2dd12f/04472/gh-ci.png 170w,\n/static/57dd22ebb6ba0635492bdb694b2dd12f/9f933/gh-ci.png 340w,\n/static/57dd22ebb6ba0635492bdb694b2dd12f/c5bb3/gh-ci.png 680w,\n/static/57dd22ebb6ba0635492bdb694b2dd12f/b12f7/gh-ci.png 1020w,\n/static/57dd22ebb6ba0635492bdb694b2dd12f/2bef9/gh-ci.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>The steps in this pipeline will create an image, push it to our Docker registry, and update GitHub with a new tag. After that, it will deploy a virtual cluster and provide access to it using a normal KUBECONFIG file. You can handle access to the cluster in a different way depending on your way of authorizing your users.</p>\n<p>Now, by running the <code class=\"language-text\">vcluster list</code> command, we will be able to see the newly created ephemeral cluster. The cluster name depends on the GitHub commit SHA and the user ID on GitHub to avoid creating clusters with the same name.</p>\n<p>Now let's check the deployed workloads:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 583px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 18.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAnUlEQVR42k2PWRLDIAxDc5k2O2BDIEvvfy0VuQ2TD42xsTVP3aoZsh3QfMKn/ae4I9TZ3Wu5IPV/dIr37DH5aL1LxfYopwVjFHRzSKAp5WJpRvdsldze/RLwmhzGVax/3i5hQ+8DOprE/WMUmq9WaWxv0pUf/bCIGZKQN9whKXeYakiV0FX35wfrrZCPFoliZFKSzCL/E8l2muEUFV8M1H88bltWIQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Deployed Workloads\" title=\"\" src=\"/static/cb75628c1c4ddb1ecfe46d82318ea01c/9fc4b/deployed.png\" srcset=\"/static/cb75628c1c4ddb1ecfe46d82318ea01c/04472/deployed.png 170w,\n/static/cb75628c1c4ddb1ecfe46d82318ea01c/9f933/deployed.png 340w,\n/static/cb75628c1c4ddb1ecfe46d82318ea01c/9fc4b/deployed.png 583w\" sizes=\"(max-width: 583px) 100vw, 583px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>It's clear that we have the full workloads deployed to the new environment and only for that environment. As development environments are created on demand whenever the developer needs them, it will be better to have an automated job that destroys those development environments every few hours to avoid keeping our main cluster resources busy all the time.</p>\n<p>Now that we have finished our development and we are ready to open a pull request to merge the new changes to the main branch, on PR creation, a new environment will be created only to test the new changes. This environment should be used by the QA and testing teams to accept the new changes/features.</p>\n<p>The PR GitHub workflow will be almost the same as the development environment, but we won't push the new image tag on GitHub; we will deploy it directly on the newly created ephemeral environment.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 192.94117647058823%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAnCAIAAACABf9ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEKklEQVR42pVW227cNhD1NxRonHgl3jlDUiQlrS9xgiZtWvQhSNHPcpOXGPC31Wny2qRA+hc9ktaJ11nbKDGgCO0ecWbOmSH3wsXncP6J3/4d3n4M5//HLj7v8esP619emdMf1PhCjc/kMJm6Nn9jz2V9qk5f8ev3e/zHJde1tdJaIw4eto/2Z3vQHjzALA728XLLVo/Ew++lC3z2J8DvVFrHfvRlMMSWgqFgQzKeXUhKW9EKIeRXk0o0jeQeu05gSTkNa45JKSWVlsuMoZSQANwYUjQryZXPAD67tPXYGsNWK9FiCOyFeXq2YsfYBut8GPrBpeJip41td2N2guF2qD5EROhCNM7BU+Cbpl3GneDXfwnfUZe7o9M4HsdhnXLmEEqJKTGzQyLu2llQDnVMh49j6akMeRzGMZ+c9DHSjnx9C6aQQhkp95aD9SSE2O3yTrAWrecEnieqMYzZtedOsM/4P+fed4VyhYHn1WrV3k/VvLNVGjGH2nMZUj8OQzeOXQh8X7YncBGiAcnWs7YObDG5GL1zdpbkDSzAB1tuG2dDHZAw4wiqNs5rChpoCo6IvCVyi02a3QIjYVpBJNQVZBtS8YFd6hyx40AMsGP24ByzlDfBBc4pY6U2yjo4hkgbRHKl8btiNqHg292wDqnrUkoI2NnkbSaH2Zmpwm4F29g71UJeKZfU5YiBT0TKOeMZYnIc59o2kydyS9vvWt8hDTwcQZtoCULpZhnt1bNdAlhEd8NtrOBhHXzKVEfmENnFwLDAHJlTmOZAPuIFGNDCxGED1lwIVOViQzdVNfLadSBHQ6bWgXmLwK3VxmA99Zn22s4qjcYaCIuQLARZqkMn8+zJe2fIGyIL1wC2IYJLraSguulhtn8cAvXHR3Uc6tjX9cipm+TBoHiSx0yyBxhpmzqk0V/BOo1aS/y+cc0apW7Qc2snmeSJP3I99KmYpfVOqlzk5aZ+fodIVKyWDJXMfe8TorI2aOPRs5V2V7PXmJWdFlI11+QZCgSLtGjvlXNTISGj82hm2y7M7Z01V3CYSo/8oIihzbAxs+gUC4iUrSFrPKL4AqazS18OocQ8HpWKTA9HfZfZQ9jL3M2Gb5HV7JBL1V6vKmTbgNHYQcMEdXmrUHhyPnbkxoxWEwfLAXTdbWgbryEvZVzzpdl/PXeaq2a6S9toImR0rgOKKQc0ceojDwnNwaOqai6R/HLubY6+a25funoSvMmgCp57gsF5ih1aEs5alzIOMKh6lhfB962YJeoZRVMG11WXe+rXlAeEsDC0cVXIDV1gtW1kuDqfW2XN6hF4Yt0mp9lIr1qx2he4GUyXgwdbC9wV9r+TluebwZsP+vQ3NTx3T16G45/j05fu5Fc9/qjGn26xF6p/pp/8zm/e74WLf8P5P/OF6CPNhgWffwr32sXn/wCEncfNJ1F1vQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"GitHub CI Merge\" title=\"\" src=\"/static/6a4d391069dc92cbbb68ee0a3873b8eb/c5bb3/gh-ci-merge.png\" srcset=\"/static/6a4d391069dc92cbbb68ee0a3873b8eb/04472/gh-ci-merge.png 170w,\n/static/6a4d391069dc92cbbb68ee0a3873b8eb/9f933/gh-ci-merge.png 340w,\n/static/6a4d391069dc92cbbb68ee0a3873b8eb/c5bb3/gh-ci-merge.png 680w,\n/static/6a4d391069dc92cbbb68ee0a3873b8eb/b12f7/gh-ci-merge.png 1020w,\n/static/6a4d391069dc92cbbb68ee0a3873b8eb/2bef9/gh-ci-merge.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>We need to create a flow that will destroy the created environment when we merge or close the pull request.</p>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìú Summary</h2>\n<p>Ephemeral environments offer a cost-effective and efficient way for large teams to develop and test software. They eliminate the need to manage and maintain permanent environments, which reduces infrastructure costs and accelerates development cycles. Ephemeral environments also help to improve data integrity by ensuring that pre-production environments always have access to production-like data. However, they may not be the ideal choice for small teams working on independent features.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/data-on-kubernetes-k8ssandra-7/","title":"Data on Kubernetes: Part 7‚Ää-‚ÄäK8ssandra: Bring Apache Cassandra to Kubernetes","date":"2024-10-26 15:06:00"},"excerpt":"K8ssandra: simple Cassandra management on Kubernetes üìà üìö Introduction In this part of my data on Kubernetes series, I will look at how K‚Ä¶","html":"<blockquote>\n<p><strong>K8ssandra: simple Cassandra management on Kubernetes üìà</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>In this part of my data on Kubernetes series, I will look at how <a href=\"https://k8ssandra.io/\" target=\"_blank\" rel=\"noopener noreferrer\">K8ssandra</a> makes it easy to use <a href=\"https://cassandra.apache.org/_/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Cassandra</a> on Kubernetes.</p>\n<p><a href=\"https://k8ssandra.io/\" target=\"_blank\" rel=\"noopener noreferrer\">K8ssandra</a> is a tool that helps you set up and manage Cassandra in a Kubernetes environment. It includes everything you need, like automated operations, monitoring, and backup solutions. This makes it simpler to handle Cassandra clusters.</p>\n<h2 id=\"-apache-cassandra-overview\" style=\"position:relative;\"><a href=\"#-apache-cassandra-overview\" aria-label=\" apache cassandra overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìñ Apache Cassandra Overview</h2>\n<p>Apache Cassandra is a highly scalable, distributed NoSQL database designed to handle large amounts of data across many servers without any single point of failure. It was originally developed at Facebook to power their inbox search feature and later became an open-source project under the Apache Software Foundation.</p>\n<h3 id=\"key-features-and-benefits\" style=\"position:relative;\"><a href=\"#key-features-and-benefits\" aria-label=\"key features and benefits permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Key Features and Benefits</h3>\n<ul>\n<li><strong>Scalability</strong>: Cassandra is designed to scale horizontally by adding more nodes to the cluster. This allows it to handle more data and more requests without any downtime.</li>\n<li><strong>High Availability</strong>: With its peer-to-peer architecture, Cassandra ensures that there is no single point of failure. Data is replicated across multiple nodes, ensuring that it remains available even if some nodes fail.</li>\n<li><strong>Fault Tolerance</strong>: Cassandra's replication strategy ensures that data is copied to multiple nodes. If one node goes down, another node can take over, ensuring continuous availability.</li>\n<li><strong>Flexible Data Model</strong>: Cassandra uses a wide-column store model, which allows for dynamic and flexible schema design. This is particularly useful for applications that require high write throughput.</li>\n<li><strong>Tunable Consistency</strong>: Cassandra offers tunable consistency levels, allowing you to balance between consistency and availability based on your application's needs.</li>\n<li><strong>High Performance</strong>: Designed for high write and read throughput, Cassandra can handle large volumes of data with low latency.</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 99.41176470588235%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE/klEQVR42lWVa1DUVRiHty99siarDzZNt6mxxo/VTI1Okxkx1pijiUYYGSkSmJKoFBBeUhG5uMsdYVcWkLsgu8sCyy6C3JaLBqywuIICG2PibdkLCyzsPv25aPTOvPPO+fD+znPOmfd3RCwLj8ezUGdmXPRcM1GSp+F0VDZFxc3Myg9j2/kizthtuDRZeOyPlprcyyUQLSktphCG7kHEMQVEHEgm6tcUoSaRm6NlOnU31k0irFufXaj2wDdw1WaCe+4JzjJBtxu3QKVRtRERmsKfv2eReCpfEL5IVGiyIKhjOn0vNr+V2INXC/kutl2rsG5+BmdaIHNOK24WoUQe96Ly8Og4h0LEnIrIQpZeyUWZmnxpFbHRUvLkgmDqHuxB7zAZ9y3OeF8chz9iwvc5rBtEuFvKFhnnZhcJXe1dzGTKuN5qIC6mkMQTctISS0g5WyQIysjPq18QtPk+j+PIxzjCPmTC7wUc21cwq0yiqTeTG3fUi0f2TE/jCI9m4usdeM7EM9LeS0JCKeHB5zgensnBwDguyLTMpO1mwku4wx0rmBDucDpwNTZ9CSqDGPGlz8mp9scx9QiRS9+JdfsP2AKCsXzjh2vfQWwt7eTn6zh+JFN46VrujIwzO9rHZHowVoHMFe3NmLEKeXMoktLPOK/0QXLpC7oHKxBNpWVj+zEIx6EIISOxhv6Gwz8Qh6IG89gDXNNTuKYmcUw6cc1f90g/JmMF8vq95NXsIl8bSKEumCzVdhSt0YgmTycwGX0SpzgVZ0ISzqR0bPvCmIxPYlboN94eJaemEZlKi0qtxjIxTZMhgwtqP2o6YlC1HUPTeRZZtR8Fup8ROSJPYN9/GHtYhEAXzkTIQazfBeBQVnPPYkGpN3BGrUdS10WRogrbIysm81Uk5d5I1TvJ1ewRKIM4r/JBJqxFNkHAsmk7kz7+uP33gHBsT7yE2VtD3H/8GGNfH636Dup19dweGsRud/LQYqbheiyXG4OQqzeTWuGNuMyLDMU2RHPyAsiQ8rBWR2tXN9LefgK6ejgq1AdmMx3t7XR1dKDX62lr02O5N0ZRn4XIhocUGO6iNQ3QPlhHc6+E5h4xoqt3x/Ft6+ITXQtrFBpeL1HwdlElrxUrSLnei91qZco1h9M1i8NmRd3/D94FJtbLb7A+d4CNhUN8rxgjTDdO/fAEosphMytzS1lTqmRrXRNBzZ3suNLKW8J6XW0DxnmCuipahBy9ZSREM4ZX/k3CtKNENZgJ1QyzpWSA97N6KLrxANGMMMcbNY28UlyJV20je1o6WVul46WCCk4bBmgtzET6xz5kR/ejzT5LyY377Ci/zRGdmQO1owv1J9UwPmVDWKbmFkevduwub5aq2FLfwjYhvxI2+KD6Cp39BqozYqnMjEMtk6BMP4PhWjsB1XfZkGvky0LTQq7NGUBhsiyOnnvJtsT9Jl4urOC9cjWrii5zrKefropcypJO0lAio7FMTnlqDFfzkinouceneTfZXHyLdTlGEtvvPTUw0RM7nA+JIPpq8WXWKLXoBbryuEhU0nM0XpKjK8xCeT6BothwDH9dY6dqjLUX+kjuFMZyybH+E5y3xCWbVIz8jfSOmZZSGdlRIZQnn+Jy6mkqBLrylJNIo39BeyER9ZAV5dIxPU/t9YnBLv8ClnDHhoQHURZTJRCWSU5QJj6OKiuRpop8hvu7l/X87wfgX5S15Is+QIm+AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"features\" title=\"\" src=\"/static/beede0121234ed9d584966a7da901e5a/0b533/features.png\" srcset=\"/static/beede0121234ed9d584966a7da901e5a/04472/features.png 170w,\n/static/beede0121234ed9d584966a7da901e5a/9f933/features.png 340w,\n/static/beede0121234ed9d584966a7da901e5a/0b533/features.png 500w\" sizes=\"(max-width: 500px) 100vw, 500px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"use-cases\" style=\"position:relative;\"><a href=\"#use-cases\" aria-label=\"use cases permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Use Cases</h3>\n<ul>\n<li><strong>Time-Series Data</strong>: Ideal for applications that need to store and query time-series data, such as IoT sensors and log data.</li>\n<li><strong>Real-Time Analytics</strong>: Used in applications that require real-time data processing and analytics, such as recommendation engines and fraud detection.</li>\n<li><strong>Messaging Systems</strong>: Suitable for high-throughput messaging systems where low latency is crucial.</li>\n<li><strong>E-commerce</strong>: Powers e-commerce platforms that need to handle large volumes of transactions and user data.</li>\n<li><strong>Social Media</strong>: Supports social media applications that require high availability and scalability to manage user interactions and content.</li>\n</ul>\n<h3 id=\"components\" style=\"position:relative;\"><a href=\"#components\" aria-label=\"components permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Components</h3>\n<ul>\n<li><strong>Nodes</strong>: The basic unit of storage in Cassandra. Each node stores a part of the data.</li>\n<li><strong>Clusters</strong>: A collection of nodes that work together. Data is distributed across the nodes in a cluster.</li>\n<li><strong>Keyspaces</strong>: The top-level namespace in Cassandra, similar to a database in relational databases.</li>\n<li><strong>Tables</strong>: Within keyspaces, tables store data in a structured format.</li>\n<li><strong>Commit Log</strong>: A log of all write operations, used for crash recovery.</li>\n<li><strong>SSTables</strong>: Immutable data files that store data on disk.</li>\n<li><strong>Cassandra Query Language (CQL)</strong>: A SQL-like language used to interact with Cassandra.</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 101.17647058823529%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAADQ0lEQVR42mVTaXMaRxDV/69KOVVxFMlJLCEwsYQBaUELiHOvmdmdnZk9Zk/AC0Ic/mCnYonvmYUQh7iqa2v6eNW973WfRAs/WjvRShgT33jtBAvSGFar7T9qneuWeldtlxvDD8nG3RXsalZOuGLTTXISr7xwMw6W82A148KWGV/O3PmEZonOCUo88XDm4108EzXBWlTO+Wo6/ZTmYJJ9Uf2t5v+l82cjeDH4MwhfFPfPajdqqNMR+yxcI8hTokDzv2r+M0pfxpvpSbzmZrJElufyqK8alXqj2e6a1HX8ANIEkNTxOLRZrSHXJHmkI4dHJmYG30w+ZSfJmqP4UdRGmHkqxP2RPVBjSEKbIttDNAwpDYGNeyPcV7iOIou5jAK+zsG7zgtC3TFgSEJyqdN+13M7NDEpwD6kUWQRv0vb5X6r9GDcwrHhUtsGweYAjheMeZFBqxf625/6hVNVLoMU52BAooTQ4Qd08XpY+Hnw/leNj4hLqfENnOTgGNBmCVy+bhVPe/2KuQdDAbapfoeLv/RFqn6phwp1yDFYjD21vCecZGaYmcHSThKLAeyJsUOLCncXDx9xnFnB8djJglIvAWxQIbULRbqCxh1NLZqDSRTbBMvstmDUL9XutR1rjkPIt86CbcfllmJUy+3i7/XShSRddxhEiHBEYxdjudoVQZGqFFugpzlMSLU+7mw5KcBchf7IyLArpoW2j2gUYDI1Ha4CXzFSiCfYozb539juGLpqzbq7HDRLmnVPx/t/3hHG2o50pTSuhoOKNdY9dgQ+SHXzm3L2g/zmx26zpKeYHaRivQo8f9U5f9UqnY3876XaEUYHVXJbUKUiANKeMH9PmCUzqQRFqntjRxo7IuxfqWaIR4odKWRhhulBKrFhT1YUq1SkPgIvszjF/5EKxQtH7LshlgSXTkfv34DeDRZLAm0OaZwSptbt8rn+7ky5LWCuUE+wHaz/OQzRGSICNePhvi9L3Xup22sNTQNoyNVMbgEw6IxEXJYe2s0eUAwEzYNU+T1/1vkWhs8ofjGTrTDxAEF+1SDcgvArikUwT6FkC6P84MU9T8Q9R0+e9zhxZktnthDGduZ8Z/s4y3bu/InNsvE6/RvuycB6hPRZAgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Cassandra components\" title=\"\" src=\"/static/ba33289753141b9941381764017f1208/c5bb3/components.png\" srcset=\"/static/ba33289753141b9941381764017f1208/04472/components.png 170w,\n/static/ba33289753141b9941381764017f1208/9f933/components.png 340w,\n/static/ba33289753141b9941381764017f1208/c5bb3/components.png 680w,\n/static/ba33289753141b9941381764017f1208/b12f7/components.png 1020w,\n/static/ba33289753141b9941381764017f1208/99f37/components.png 1100w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"why-choose-cassandra\" style=\"position:relative;\"><a href=\"#why-choose-cassandra\" aria-label=\"why choose cassandra permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Why Choose Cassandra?</h3>\n<ul>\n<li><strong>Scalability</strong>: Cassandra's ability to scale horizontally without downtime makes it ideal for growing applications.</li>\n<li><strong>High Availability</strong>: Its peer-to-peer architecture ensures no single point of failure, providing continuous availability.</li>\n<li><strong>Performance</strong>: Optimized for high write and read throughput, making it suitable for applications with heavy data loads.</li>\n<li><strong>Flexibility</strong>: The wide-column store model allows for flexible schema design, accommodating various data types and structures.</li>\n<li><strong>Community and Support</strong>: As an open-source project, Cassandra has a large and active community, providing extensive resources and support.</li>\n</ul>\n<blockquote>\n<p>Some alternatives to Apache Cassandra are <a href=\"https://www.scylladb.com/\" target=\"_blank\" rel=\"noopener noreferrer\">ScyllaDB</a>, which is a high-performance, low-latency NoSQL database designed as a drop-in replacement for Cassandra, and <a href=\"https://aws.amazon.com/keyspaces/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Keyspaces</a>, which is a scalable, highly available, and managed Apache Cassandra‚Äìcompatible database service.</p>\n</blockquote>\n<h2 id=\"-k8ssandra-overview\" style=\"position:relative;\"><a href=\"#-k8ssandra-overview\" aria-label=\" k8ssandra overview permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåü K8ssandra Overview</h2>\n<p>K8ssandra is an open-source distribution of Apache Cassandra that is optimized for Kubernetes. It includes everything you need to run Cassandra in a Kubernetes environment, such as automated operations, monitoring, and backup solutions. K8ssandra simplifies the deployment and management of Cassandra clusters on Kubernetes, making it easier to achieve scalability and high availability.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 108.8235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFTklEQVR42m1TWW8aVxSe9/S9+QFR24fE7VOTp75UTVxXatSqcVIr6qK2T5WqKEraqm2UBSXx2jitl9gOXrANmM3GgGEwm1nGBDDgBTwODjYQQ/FKhmEbYE7vjO0qTno13z3nfN+9Zz7N3IuJRKJjPT09pzQazUmPx3PS5XKh6Dq5E/fwkatfRhBx2ZSHjy/zBEGcwnH8GObz+c6l02mgaZqpVCpl9JQLpXI5miyUS0yZrw/H/2kMw0sMy7IQiURqMW6C/cFWqyyfKJw5+L59G8aJHF9zNLsvgTlYgFsjGdD7Cv9p3F5uWllZqeManjvoV+F0DgpnvnJjOFNtVVJ8Xdnfx+kVkYmu/ti5UxXidPVwPexrsLq6+vERh4cuklt5mHRTsLlbOBAOLKCxtVcAa5BGMc8bY191aDKZ3gyHVy6FQmT9AS6QIe/5TMxxwWEz8xxJkvXx+NpnyMHnTqfl4vOIo95oNtYHFkL1C0gLLJD1i+HwJa4X1qdpfF9sbyH0ZK9tJiGym6JCc+9U15l+0/yYzEMSw7aAWRvaCI54IhcDo6rrpM4cCOuc1qJ/eSbrWeKR84ZslHuJ8E3gpzErOVDry5ihUR2rPsBj4N7SMfp5+XlnshQTzgLclBWY2V0AQzT1CyytCNcWq9Akpioe+zbAszT0qzLQLXvBVqJ7kLDN1mG+2MOzQpcZrveWC1e6c+Vht572r/XVSXx7kU4tDX8IX9DNBgZssdjV8pPgo7ZRCq6JC3lBX6pEOmKlNk2u9E0nVSTwOBSeWGuRQ1HtU9YG9yaXqw+mSQgXcORw4lM/xazLlwBuKfLMIvqHpnj6Z4gkhJl1gMaB7UrEnwF4vgNK7Q4Mq7ZZeJ6FtdmFOkwy/fdb5hVhOx5oapny328zhLuaeiZb31H71n5TugPtY1ZHk9oX6X5snvsgiVu/2LQ4uhIaZfOmeaZ1w0y0rqomWtfU6pY4PtM+P6F/G+uQWU88npq7p7DP3zGSaYEu/EKgW0o+nFpK3tWGUnemwinBbCJ125VIt02EUy0yxCnJLYE8nBYokWbZ2BFYUnt3lMv/3Ouzzp3AtEuRWj8F0G3Ls6P+Ciygo+fdA/DTAAF0UYIIg14AEcJ8FgHxwQMsIm3YBzDiA3a5DDAVWq3DzOHwWbGPhp86dwtfNW8WpXNU0ZkqFC2xfNGFosSXLV7p3i5ebkwXR70Uz3Eat0bqzxavPtoufnl/syBCF2FmefkcZkAOnZsAN8Q0++sIDfYkC150TNzoVHCRq2/LcnBtKAu2DZZ3z2kepDlSAAJFDq4OUKw9DWDkHIoswdOmRNY7Foi75ME4gceyhDZKEToE7TOKMMazhMS3TsgC63zOcYcajmppIEaMza258DjlHbDOn8FkzTePm/5quxwfaWvIj/c0PJX0v4ZnsiEer/GSoYYtmaxhQyFpIMXCy9aO5uNYUNRXW9LLwC3G2YDKBWDTAFi1R2Hh8Cqvg4pFDRQuBhqXsuAwwIp0qA7LKB/VRjV2+K43V23oyrPusRkWzEq2gqtZwMdZ1IilRwZZenSQzzmugqtYMOlZr6KF/aHnE/Z34aVqwSiDhHR0v2FMNwNfd1LshT+3YU5uQ28fh+o0cjQ9ybtjFBIoT8r2XZomoTI9gdYYwS6/Cx8+fA++7f6IzZvkkFRI67DFwcdnYVoFLpGmNN2vZgBXMEW9iikdgDGMM5vjYmYLgcs5rqhXMmXDBLOlHWGMoluMS9xUKhlUQKLPh1k7Ot5YlArfBXVzDUw11yyIB2tIcd8RLAzu4wgn7qzZlctrQD2H9s3WRFEPrte/eeBgKL6TZyoAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"K8ssandra\" title=\"\" src=\"/static/c74c3735ed2a70b2c4249bda916fde70/c5bb3/k8ssandra.png\" srcset=\"/static/c74c3735ed2a70b2c4249bda916fde70/04472/k8ssandra.png 170w,\n/static/c74c3735ed2a70b2c4249bda916fde70/9f933/k8ssandra.png 340w,\n/static/c74c3735ed2a70b2c4249bda916fde70/c5bb3/k8ssandra.png 680w,\n/static/c74c3735ed2a70b2c4249bda916fde70/c0255/k8ssandra.png 920w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"Ô∏è-key-features-and-benefits\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-key-features-and-benefits\" aria-label=\"Ô∏è key features and benefits permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Key Features and Benefits</h3>\n<ul>\n<li><strong>Kubernetes Native</strong>: K8ssandra is designed to run seamlessly on Kubernetes, leveraging Kubernetes' orchestration capabilities to manage Cassandra clusters.</li>\n<li><strong>Automated Operations</strong>: K8ssandra includes tools for automated deployment, scaling, and maintenance of Cassandra clusters, reducing the operational overhead.</li>\n<li><strong>Monitoring and Management</strong>: Integrated with tools like <a href=\"https://prometheus.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Prometheus</a> and <a href=\"https://grafana.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Grafana</a>, K8ssandra provides robust monitoring and management capabilities, allowing you to keep an eye on your Cassandra clusters' health and performance.</li>\n<li><strong>Backup and Restore</strong>: K8ssandra includes backup and restore solutions, ensuring that your data is safe and can be recovered in case of failures.</li>\n<li><strong>Helm Charts</strong>: K8ssandra uses <a href=\"https://helm.sh/\" target=\"_blank\" rel=\"noopener noreferrer\">Helm charts</a> for easy installation and configuration, making it simple to deploy Cassandra clusters on Kubernetes.</li>\n<li><strong>Community Support</strong>: As an open-source project, K8ssandra benefits from a vibrant community that contributes to its development and provides support.</li>\n</ul>\n<h2 id=\"-installing-k8ssandra-on-eks\" style=\"position:relative;\"><a href=\"#-installing-k8ssandra-on-eks\" aria-label=\" installing k8ssandra on eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Installing K8ssandra on EKS</h2>\n<p>K8ssandra Operator may be deployed in one of two modes:</p>\n<ul>\n<li><strong>Control-Plane mode</strong>: The default method of installation. A Control-Plane instance of K8ssandra Operator watches for the creation and changes to K8ssandraCluster custom resources. When Control-Plane is active, Cassandra resources may be created within the local Kubernetes cluster and/or remote Kubernetes clusters (in the case of multi-region deployments). Only one instance should run in Control-Plane mode.</li>\n<li><strong>Data-Plane mode</strong>: Kubernetes clusters acting as remote regions for Cassandra deployments should be run in Data-Plane mode. In this mode, K8ssandra Operator does not directly reconcile K8ssandraCluster resources.</li>\n</ul>\n<h3 id=\"-requirements\" style=\"position:relative;\"><a href=\"#-requirements\" aria-label=\" requirements permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìã Requirements</h3>\n<p>Before you start, ensure you have the following:</p>\n<ul>\n<li><strong>Amazon S3 Bucket</strong>: Backups for K8ssandra are stored within an Amazon Simple Storage Service (S3) Bucket. <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn more about Amazon S3</a>.</li>\n<li><strong>AWS Identity &#x26; Access Management (IAM) Role</strong>: This role is used by the EKS worker nodes to control access to the S3 bucket used for backups. See this <a href=\"https://github.com/k8ssandra/k8ssandra-terraform/blob/main/aws/scripts/policy_document.json#L157-L190\" target=\"_blank\" rel=\"noopener noreferrer\">IAM policy</a> as an example. Note your policy should limit requests to specific buckets and operations. <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\" target=\"_blank\" rel=\"noopener noreferrer\">Learn more about IAM roles</a>.</li>\n<li><strong>kubectl</strong>: Installed and configured to access the cluster. <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\" target=\"_blank\" rel=\"noopener noreferrer\">Install kubectl</a>.</li>\n<li><strong>Helm</strong>: Installed and configured to access the cluster. <a href=\"https://helm.sh/docs/intro/install/\" target=\"_blank\" rel=\"noopener noreferrer\">Install Helm</a>.</li>\n<li><strong>AWS CLI</strong>: Installed and configured. <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\" target=\"_blank\" rel=\"noopener noreferrer\">Install AWS CLI</a>.</li>\n<li><strong>Cert Manager</strong>: Cert Manager provides a common API for management of Transport Layer Security (TLS) certificates. K8ssandra Operator uses this API for certificates used by the various K8ssandra components. <a href=\"https://cert-manager.io/docs/installation/kubernetes/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn more about Cert Manager</a>.</li>\n<li><strong>eksctl</strong>: Installed. <a href=\"https://eksctl.io/getting-started/\" target=\"_blank\" rel=\"noopener noreferrer\">Install eksctl</a>.</li>\n</ul>\n<h3 id=\"Ô∏è-create-the-eks-cluster\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-create-the-eks-cluster\" aria-label=\"Ô∏è create the eks cluster permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Create the EKS Cluster</h3>\n<p>Create an EKS Cluster:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ eksctl create cluster <span class=\"token parameter variable\">--name</span> k8ssandra-cluster <span class=\"token parameter variable\">--version</span> <span class=\"token number\">1.31</span><span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--region</span> eu-west-1 --nodegroup-name standard-workers <span class=\"token punctuation\">\\</span>\n--node-type t3.medium <span class=\"token parameter variable\">--nodes</span> <span class=\"token number\">3</span> --nodes-min <span class=\"token number\">1</span> --nodes-max <span class=\"token number\">4</span> <span class=\"token parameter variable\">--managed</span></code></pre></div>\n<p>This command creates an EKS cluster named <code class=\"language-text\">k8ssandra-cluster</code> in the <code class=\"language-text\">eu-west-1</code> region with Kubernetes version 1.31. It sets up a managed node group with <code class=\"language-text\">t3.medium</code> instances, starting with 3 nodes and scaling between 1 and 4 nodes.</p>\n<p>Verify the Cluster:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ eksctl get cluster <span class=\"token parameter variable\">--name</span> k8ssandra-cluster <span class=\"token parameter variable\">--region</span> eu-west-1</code></pre></div>\n<p>Ensure the cluster is up and running.</p>\n<h3 id=\"Ô∏è-prepare-the-eks-cluster-for-k8ssandra\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-prepare-the-eks-cluster-for-k8ssandra\" aria-label=\"Ô∏è prepare the eks cluster for k8ssandra permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è Prepare the EKS Cluster for K8ssandra</h3>\n<p>We can deploy K8ssandra Operator for namespace-scoped operations (the default), or cluster-scoped operations.</p>\n<ul>\n<li><strong>Namespace-scoped</strong>: Operations are specific only to the identified namespace within a cluster.</li>\n<li><strong>Cluster-scoped</strong>: Operations are global to all namespace(s) in the cluster.</li>\n</ul>\n<p>Add the K8ssandra Helm Repository:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ helm repo <span class=\"token function\">add</span> k8ssandra https://helm.k8ssandra.io/stable \n$ helm repo update</code></pre></div>\n<p>Create a namespace for K8ssandra:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl create namespace k8ssandra</code></pre></div>\n<p>Install K8ssandra:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ helm <span class=\"token function\">install</span> k8ssandra-operator k8ssandra/k8ssandra-operator <span class=\"token parameter variable\">-n</span> k8ssandra</code></pre></div>\n<p>This command installs K8ssandra in the <code class=\"language-text\">k8ssandra</code> namespace.</p>\n<p>Verify the Installation:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl get pods <span class=\"token parameter variable\">-n</span> k8ssandra</code></pre></div>\n<p>Ensure all pods are running and the installation is successful.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 11.176470588235293%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAcElEQVR42h1NSQrAIAzsc8SKHqRaUFFUBI8K9v8/mZIchkkyS67v+1BrRSmFkXNGa43nlBKe54G1FsYY1mKMGGNgzokQAu/k6b1DSolrrYVzDvbeXOCcY+P7vvDeM1OAQKVKKX5AmtYaQgjc98034h+rzENg8yNKaQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"pods\" title=\"\" src=\"/static/ac87c91e0a24e9d6b9e527db29536091/c5bb3/pods.png\" srcset=\"/static/ac87c91e0a24e9d6b9e527db29536091/04472/pods.png 170w,\n/static/ac87c91e0a24e9d6b9e527db29536091/9f933/pods.png 340w,\n/static/ac87c91e0a24e9d6b9e527db29536091/c5bb3/pods.png 680w,\n/static/ac87c91e0a24e9d6b9e527db29536091/b12f7/pods.png 1020w,\n/static/ac87c91e0a24e9d6b9e527db29536091/b5a09/pods.png 1360w,\n/static/ac87c91e0a24e9d6b9e527db29536091/5faa8/pods.png 1444w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-deploying-a-k8ssandracluster-with-medusa-reaper-and-metrics\" style=\"position:relative;\"><a href=\"#-deploying-a-k8ssandracluster-with-medusa-reaper-and-metrics\" aria-label=\" deploying a k8ssandracluster with medusa reaper and metrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üì¶ Deploying a K8ssandraCluster with Medusa, Reaper, and Metrics</h3>\n<p>After installing the K8ssandra operator, we can now deploy a K8ssandraCluster with <a href=\"https://docs.k8ssandra.io/components/medusa/\" target=\"_blank\" rel=\"noopener noreferrer\">Medusa</a>, <a href=\"https://docs.k8ssandra.io/components/reaper/\" target=\"_blank\" rel=\"noopener noreferrer\">Reaper</a>, and <a href=\"https://docs.k8ssandra.io/components/metrics-collector/\" target=\"_blank\" rel=\"noopener noreferrer\">metrics</a> enabled.</p>\n<p>The following YAML configuration includes the necessary components and annotations to ensure proper functionality and permissions.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> eksctl.io/v1alpha5\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ClusterConfig\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">-</span>eks<span class=\"token punctuation\">-</span>demo\n  <span class=\"token key atrule\">region</span><span class=\"token punctuation\">:</span> eu<span class=\"token punctuation\">-</span>west<span class=\"token punctuation\">-</span><span class=\"token number\">1</span>\n<span class=\"token key atrule\">addons</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> aws<span class=\"token punctuation\">-</span>ebs<span class=\"token punctuation\">-</span>csi<span class=\"token punctuation\">-</span>driver\n\n<span class=\"token key atrule\">nodeGroups</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> admin\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> admin\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">-</span>1a\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1a\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c5.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">-</span>1b\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1b\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c5.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">-</span>1c\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1c\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c5.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tidb<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">-</span>1a\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1a\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c7g.xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">-</span>1b\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1b\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c7g.xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">-</span>1c\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1c\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> c7g.xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> pd<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">-</span>1a\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1a\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> r5b.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">-</span>1b\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1b\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> r5b.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">-</span>1c\n    <span class=\"token key atrule\">desiredCapacity</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">privateNetworking</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n    <span class=\"token key atrule\">availabilityZones</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"eu-west-1c\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">instanceType</span><span class=\"token punctuation\">:</span> r5b.2xlarge\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv\n    <span class=\"token key atrule\">taints</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">dedicated</span><span class=\"token punctuation\">:</span> tikv<span class=\"token punctuation\">:</span>NoSchedule\n    <span class=\"token key atrule\">iam</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">withAddonPolicies</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ebs</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>where:</p>\n<p><strong>Medusa</strong></p>\n<p>Medusa is a backup and restore tool for Apache Cassandra. It integrates with Kubernetes to provide automated backups to cloud storage (e.g., S3). Ensure the service account has the necessary role annotation to write to the backup bucket.</p>\n<p><strong>Reaper</strong></p>\n<p>Reaper is a tool for managing and scheduling repairs in Apache Cassandra. It helps maintain the health of your Cassandra cluster by performing regular repairs to prevent data inconsistencies.</p>\n<p>Key features include:</p>\n<ul>\n<li><strong>Auto-scheduling</strong>: Automatically schedules repairs based on defined thresholds and intervals.</li>\n<li><strong>Deployment mode</strong>: Can be deployed per datacenter (PER_DC) for better control.</li>\n<li><strong>Heap size</strong>: Configurable heap size for managing memory usage.</li>\n<li><strong>Keyspace</strong>: Uses a dedicated keyspace (<code class=\"language-text\">reaper_db</code>) for storing repair schedules and state.</li>\n</ul>\n<p><strong>Stargate</strong></p>\n<p><a href=\"https://docs.k8ssandra.io/components/stargate/\" target=\"_blank\" rel=\"noopener noreferrer\">Stargate</a> is an open-source data gateway that provides a unified API layer for accessing Cassandra data. It supports multiple APIs, including REST, GraphQL, and CQL, making it easier to interact with Cassandra.</p>\n<p>Key features include:</p>\n<ul>\n<li><strong>API Gateway</strong>: Provides REST, GraphQL, and CQL APIs for accessing Cassandra data.</li>\n<li><strong>Scalability</strong>: Can be scaled independently of the Cassandra nodes.</li>\n<li><strong>Telemetry</strong>: Supports Prometheus for monitoring and metrics collection.</li>\n</ul>\n<p>Create the K8ssandraCluster with <code class=\"language-text\">kubectl apply</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl apply <span class=\"token parameter variable\">-n</span> k8ssandra <span class=\"token parameter variable\">-f</span> k8sc.yaml</code></pre></div>\n<h3 id=\"-access-reaper\" style=\"position:relative;\"><a href=\"#-access-reaper\" aria-label=\" access reaper permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîß Access Reaper</h3>\n<p><a href=\"http://cassandra-reaper.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Reaper</a> is an interface for managing K8ssandra cluster repairs. Reaper is deployed as part of the K8ssandra Operator installation.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 45.294117647058826%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAACHElEQVR42n3Qy2oTcRQG8HkEUZBaddE2tEGaSw01EtokzSSTZDIJuTRxcmnunUymuRabiU3VBC+gWBQr1IXoWgpecOHCYl3Y+g6dhRvTt/D/OQmFQsEufpzD+c7icKhfxQkcSHrsC1M4SI/gO38eu7Fz2FPrj/iFsw12lkaxl75Kfmau4DN/+Q91q9RCKN+Eb6kGb2IFBncKM34BE/YkLs5GMHozOnTpuA6MmBdP3IiowmSQj5l9R1RUkBFZbsGXroHmK2BCETy7E8GjVQ7dqgsdcQGt4jw2RAfaghXtZRt6VeYUF+nWvDA52D4VK8kI5lbBqRe64lXY2ABaJTdk0Y12icbbJzl8fbeG1w+X8GG7NrReZiALtJo7j9FEFhkY7d4+xYttxMttLGabYJMr0DNJzHAC9N4iDN4CUo3H6Dx9g5jURb27hUbvFYzqfNqVgY7JDqk9MXgKuGYN9ylvqgq3+js6JmI+VIA1XECu1kGkeBth8S48+RZs6RaYwjoc2TYcGRl+8T785ROceI8EKw9g8mX6lGYuSAYmrWEybgkQW1QgfGWDOHJ1Ev+yTRrfPhHdzgvVJtHvPB+afr952l/dx5eYLAp9SmuPYkqlXYhhbC4EjpewtdZDOtsAW5cQkprgShJ8pfJ/sYJIAlIFJo4/ojQW/6HGwikD42ZW0XsSiiOUV2bpqHKdTihGJ6+YnImzuZKHqt9ai3//H7SvbnreQ4+zAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"reaper\" title=\"\" src=\"/static/99343368881581dcf2778846abe57cb2/c5bb3/reaper.png\" srcset=\"/static/99343368881581dcf2778846abe57cb2/04472/reaper.png 170w,\n/static/99343368881581dcf2778846abe57cb2/9f933/reaper.png 340w,\n/static/99343368881581dcf2778846abe57cb2/c5bb3/reaper.png 680w,\n/static/99343368881581dcf2778846abe57cb2/b12f7/reaper.png 1020w,\n/static/99343368881581dcf2778846abe57cb2/b5a09/reaper.png 1360w,\n/static/99343368881581dcf2778846abe57cb2/29007/reaper.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>For details, start in the <a href=\"https://docs.k8ssandra.io/components/reaper/\" target=\"_blank\" rel=\"noopener noreferrer\">Reaper</a> topic. Then read about the <a href=\"https://docs.k8ssandra.io/tasks/repair/\" target=\"_blank\" rel=\"noopener noreferrer\">repair</a> tasks you can perform with Reaper.</p>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h2>\n<p>K8ssandra simplifies the deployment and management of Apache Cassandra on Kubernetes, providing robust features like automated backups with Medusa, repairs with Reaper, and comprehensive monitoring with MCAC and Vector.</p>\n<br>\n<br>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìªüß° References:</strong></p>\n<ul>\n<li><a href=\"https://k8ssandra.io/\" target=\"_blank\" rel=\"noopener noreferrer\">https://k8ssandra.io/</a></li>\n<li><a href=\"https://dok.community/blog/1000-node-cassandra-cluster-on-amazons-eks\" target=\"_blank\" rel=\"noopener noreferrer\">https://dok.community/blog/1000-node-cassandra-cluster-on-amazons-eks</a></li>\n<li><a href=\"https://medium.com/rahasak/deploy-cassandra-cluster-on-kubernetes-with-k8ssandra-fd19c535376c\" target=\"_blank\" rel=\"noopener noreferrer\">https://medium.com/rahasak/deploy-cassandra-cluster-on-kubernetes-with-k8ssandra-fd19c535376c</a></li>\n<li><a href=\"https://docs.k8ssandra.io/install/eks/\" target=\"_blank\" rel=\"noopener noreferrer\">https://docs.k8ssandra.io/install/eks/</a></li>\n<li><a href=\"https://github.com/k8ssandra/k8ssandra\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/k8ssandra/k8ssandra</a></li>\n<li><a href=\"https://github.com/k8ssandra/k8ssandra-operator\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/k8ssandra/k8ssandra-operator</a></li>\n<li><a href=\"https://cassandra.apache.org/_/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://cassandra.apache.org/_/index.html</a></li>\n</ul>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}