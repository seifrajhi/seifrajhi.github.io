{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/hidden-gems-kubernetes/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Lesser-Known Aspects of Kubernetes 💎</strong></p>\n</blockquote>\n<h2 id=\"overview-\" style=\"position:relative;\"><a href=\"#overview-\" aria-label=\"overview  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Overview 👀</h2>\n<p>Kubernetes has revolutionized the way we manage containerized applications, but it's packed with hidden features that even experienced users might not be aware of.</p>\n<p>Let's dive into a few of these hidden gems and discover the lesser-known capabilities of Kubernetes.</p>\n<p>I hope this blog post has been helpful.</p>\n<h2 id=\"sorting-and-organizing-your-pods-\" style=\"position:relative;\"><a href=\"#sorting-and-organizing-your-pods-\" aria-label=\"sorting and organizing your pods  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Sorting and Organizing Your Pods 📋</h2>\n<p>Ever wished you could organize your pod list in a more meaningful way? With Kubernetes, you can! Simply use the <code class=\"language-text\">--sort-by</code> flag along with the <code class=\"language-text\">kubectl get pods</code> command to sort your pods by various criteria, such as pod name or creation time.</p>\n<p>Running <code class=\"language-text\">kubectl get pods --sort-by=.metadata.name</code> might just save you from endless scrolling through your pod list. Let's sort the pods in descending order, i.e., with the newest pods appearing first:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get pods --sort-by<span class=\"token operator\">=</span>.metadata.creationTimestamp --no-headers <span class=\"token operator\">|</span> <span class=\"token function\">tail</span> <span class=\"token parameter variable\">-r</span>\nubuntu-pod-3             <span class=\"token number\">2</span>/2     Running   <span class=\"token number\">0</span>          5m17s\nubuntu-pod-2             <span class=\"token number\">2</span>/2     Running   <span class=\"token number\">0</span>          13m7s\nubuntu-pod-1             <span class=\"token number\">2</span>/2     Running   <span class=\"token number\">0</span>          26m</code></pre></div>\n<h2 id=\"listing-all-object-types-\" style=\"position:relative;\"><a href=\"#listing-all-object-types-\" aria-label=\"listing all object types  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Listing All Object Types 📜</h2>\n<p>Did you know you can list all the object types that your cluster supports? Use the <code class=\"language-text\">kubectl api-resources</code> command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl api-resources</code></pre></div>\n<p>When we want a more encompassing list of all resources in a namespace, we can combine the <code class=\"language-text\">kubectl api-resources</code> command with <code class=\"language-text\">kubectl get</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl api-resources <span class=\"token parameter variable\">--verbs</span><span class=\"token operator\">=</span>list <span class=\"token parameter variable\">--namespaced</span> <span class=\"token parameter variable\">-o</span> name <span class=\"token operator\">|</span> <span class=\"token function\">xargs</span> <span class=\"token parameter variable\">-n</span> <span class=\"token number\">1</span> kubectl get --ignore-not-found --show-kind <span class=\"token parameter variable\">-n</span> <span class=\"token operator\">&lt;</span>namespace<span class=\"token operator\">></span></code></pre></div>\n<p><code class=\"language-text\">kubectl api-resources --verbs=list --namespaced -o name</code> retrieves all namespaced API resource types that support the list API verb. Then it outputs their names. Those names are then redirected to <code class=\"language-text\">xargs</code> as standard input.</p>\n<p><a href=\"https://www.baeldung.com/linux/xargs-multiple-arguments\" target=\"_blank\" rel=\"noopener noreferrer\"><code class=\"language-text\">xargs -n 1</code></a> singly passes each of those names as initial arguments to <code class=\"language-text\">kubectl get --ignore-not-found --show-kind -n &lt;namespace></code>. Then, the <code class=\"language-text\">kubectl get</code> command returns a list of resources belonging to each resource type in the specified namespace.</p>\n<h2 id=\"default-resources-and-limits-with-limitrange-and-resourcequotas-\" style=\"position:relative;\"><a href=\"#default-resources-and-limits-with-limitrange-and-resourcequotas-\" aria-label=\"default resources and limits with limitrange and resourcequotas  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Default Resources and Limits with LimitRange and ResourceQuotas 🚦</h2>\n<p>In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Namespaces are a way to divide cluster resources into groups for multiple users (via resource-quota). Each namespace will have one or multiple containers running inside it.</p>\n<p>After creating a namespace for each team within the cluster, consider that what if one team, i.e., namespace, consumes more resources from the cluster like CPU and memory, and other teams' resources starve for resources as the cluster has a very limited amount of available hardware resources. This creates a noisy neighbor problem within the cluster.</p>\n<p>To avoid this, as an administrator, first, you create a namespace within the cluster, and then you can use ResourceQuota and LimitRange to assign resource quotas on namespaces and set limits for containers running inside any namespace.</p>\n<h3 id=\"resource-quotas\" style=\"position:relative;\"><a href=\"#resource-quotas\" aria-label=\"resource quotas permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Resource Quotas</h3>\n<p>After creating namespaces, we can use the ResourceQuota object to limit the total amount of resources used by the namespace. We can use ResourceQuota to set limits for different object types that can be created within a namespace along with setting quotas for resources like CPU and memory.</p>\n<p>A ResourceQuota for setting a quota on resources looks like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ResourceQuota\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> teamx<span class=\"token punctuation\">-</span>resource<span class=\"token punctuation\">-</span>quota\n    <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> teamx\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">hard</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">limits.cpu</span><span class=\"token punctuation\">:</span> 150m\n        <span class=\"token key atrule\">limits.memory</span><span class=\"token punctuation\">:</span> 600Mi\n        <span class=\"token key atrule\">requests.cpu</span><span class=\"token punctuation\">:</span> 150m\n        <span class=\"token key atrule\">requests.memory</span><span class=\"token punctuation\">:</span> 600Mi</code></pre></div>\n<ul>\n<li><code class=\"language-text\">limits.cpu</code> is the maximum CPU limit for all the containers in the Namespace, i.e., the entire namespace.</li>\n<li><code class=\"language-text\">limits.memory</code> is the maximum Memory limit for all containers in the Namespace, i.e., the entire namespace.</li>\n<li><code class=\"language-text\">requests.cpu</code> is the maximum CPU requests for all the containers in the Namespace. As per the above YAML, the total requested CPU in the Namespace should be less than 150m.</li>\n<li><code class=\"language-text\">requests.memory</code> is the maximum Memory requests for all the containers in the Namespace. As per the above YAML, the total requested memory in the namespace should be less than 600Mi.</li>\n</ul>\n<h3 id=\"limitrange-for-containers\" style=\"position:relative;\"><a href=\"#limitrange-for-containers\" aria-label=\"limitrange for containers permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>LimitRange for Containers</h3>\n<p>We can create a LimitRange object in our Namespace which can be used to set limits on resources on containers running within the namespace. This is used to provide default limit values for Pods which do not specify this value themselves to equally distribute resources within a namespace.</p>\n<p>A LimitRange provides constraints that can:</p>\n<ul>\n<li>Apply minimum and maximum CPU resources usage limit per Pod or Container in a namespace.</li>\n<li>Apply minimum and maximum memory request limit per PersistentVolumeClaim in a namespace.</li>\n<li>Apply minimum and maximum CPU resources usage limit per Pod or Container in a namespace.</li>\n<li>Set default request/limit for resources within a namespace and then automatically set the limits to Containers at runtime.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> LimitRange\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> teamx<span class=\"token punctuation\">-</span>limit<span class=\"token punctuation\">-</span>range\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">default</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 200Mi\n            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 50m\n        <span class=\"token key atrule\">defaultRequest</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 200Mi\n            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 50m\n        <span class=\"token key atrule\">max</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 200Mi\n            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 50m\n        <span class=\"token key atrule\">min</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 200Mi\n            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 50m\n        <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Container</code></pre></div>\n<p>The above YAML file has 4 sections: <code class=\"language-text\">max</code>, <code class=\"language-text\">min</code>, <code class=\"language-text\">default</code>, and <code class=\"language-text\">defaultRequest</code>.</p>\n<ul>\n<li>The <code class=\"language-text\">default</code> section will set up the default limits for a container in a pod. Any container with no limits defined will get these values assigned as default.</li>\n<li>The <code class=\"language-text\">defaultRequest</code> section will set up the default requests for a container in a pod. Any container with no requests defined will get these values assigned as default.</li>\n<li>The <code class=\"language-text\">max</code> section will set up the maximum limits that a container in a Pod can set. The value specified in the <code class=\"language-text\">default</code> section cannot be higher than this value.</li>\n<li>The <code class=\"language-text\">min</code> section will set up the minimum requests that a container in a Pod can set. The value specified in the <code class=\"language-text\">defaultRequest</code> section cannot be lower than this value.</li>\n</ul>\n<h2 id=\"kubectl-debug\" style=\"position:relative;\"><a href=\"#kubectl-debug\" aria-label=\"kubectl debug permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>kubectl debug</h2>\n<p>One of the most forgotten but powerful <code class=\"language-text\">kubectl</code> commands is <code class=\"language-text\">debug</code>. It allows you to create a sidecar container on any pod, copy a pod to a new instance for debugging, and even access the pod's filesystem.</p>\n<h3 id=\"debugging-a-node-️\" style=\"position:relative;\"><a href=\"#debugging-a-node-%EF%B8%8F\" aria-label=\"debugging a node ️ permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Debugging a Node 🖥️</h3>\n<p>Use the <code class=\"language-text\">kubectl debug node</code> command to deploy a Pod to a Node that you want to troubleshoot. This command is helpful in scenarios where you can't access your Node by using an SSH connection. When the Pod is created, the Pod opens an interactive shell on the Node. To create an interactive shell on a Node named <code class=\"language-text\">mynode</code>, run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl debug node/mynode <span class=\"token parameter variable\">-ti</span> <span class=\"token parameter variable\">--image</span><span class=\"token operator\">=</span>ubuntu -- <span class=\"token function\">chroot</span> /host <span class=\"token function\">bash</span></code></pre></div>\n<h3 id=\"adding-ephemeral-containers-\" style=\"position:relative;\"><a href=\"#adding-ephemeral-containers-\" aria-label=\"adding ephemeral containers  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Adding Ephemeral Containers 🐳</h3>\n<p>You can also use the <code class=\"language-text\">kubectl debug</code> command to add ephemeral containers to a running Pod for debugging.</p>\n<p>First, create a pod for the example:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl run ephemeral-demo <span class=\"token parameter variable\">--image</span><span class=\"token operator\">=</span>registry.k8s.io/pause:3.1 <span class=\"token parameter variable\">--restart</span><span class=\"token operator\">=</span>Never</code></pre></div>\n<p>The examples in this section use the <a href=\"https://registry.k8s.io/pause:3.1\" target=\"_blank\" rel=\"noopener noreferrer\">pause container image</a> because it does not contain debugging utilities, but this method works with all container images. If you attempt to use <code class=\"language-text\">kubectl exec</code> to create a shell, you will see an error because there is no shell in this container image.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> ephemeral-demo -- <span class=\"token function\">sh</span>\nOCI runtime <span class=\"token builtin class-name\">exec</span> failed: <span class=\"token builtin class-name\">exec</span> failed: container_linux.go:346: starting container process caused <span class=\"token string\">\"exec: <span class=\"token entity\" title=\"\\&quot;\">\\\"</span>sh<span class=\"token entity\" title=\"\\&quot;\">\\\"</span>: executable file not found in <span class=\"token environment constant\">$PATH</span>\"</span><span class=\"token builtin class-name\">:</span> unknown</code></pre></div>\n<p>You can instead add a debugging container using <code class=\"language-text\">kubectl debug</code>. If you specify the <code class=\"language-text\">-i/--interactive</code> argument, <code class=\"language-text\">kubectl</code> will automatically attach to the console of the Ephemeral Container.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl debug <span class=\"token parameter variable\">-it</span> ephemeral-demo <span class=\"token parameter variable\">--image</span><span class=\"token operator\">=</span>busybox:1.28 <span class=\"token parameter variable\">--target</span><span class=\"token operator\">=</span>ephemeral-demo\nDefaulting debug container name to debugger-8xzrl.\nIf you don't see a <span class=\"token builtin class-name\">command</span> prompt, try pressing enter.\n/ <span class=\"token comment\">#</span></code></pre></div>\n<p>This command adds a new <code class=\"language-text\">busybox</code> container and attaches to it. The <code class=\"language-text\">--target</code> parameter targets the process namespace of another container. It's necessary here because <code class=\"language-text\">kubectl run</code> does not enable process namespace sharing in the pod it creates.</p>\n<h2 id=\"krew-the-plugin-marketplace-\" style=\"position:relative;\"><a href=\"#krew-the-plugin-marketplace-\" aria-label=\"krew the plugin marketplace  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Krew: The Plugin Marketplace 🚀</h2>\n<p>There's a massive marketplace of Kubectl plugins that can extend its functionality and make your life easier. Meet <a href=\"https://krew.sigs.k8s.io/docs/user-guide/setup/install/\" target=\"_blank\" rel=\"noopener noreferrer\">Krew</a>:</p>\n<p>Krew is the plugin manager for the <code class=\"language-text\">kubectl</code> command-line tool.</p>\n<p>Krew helps you:</p>\n<ul>\n<li>Discover <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\" target=\"_blank\" rel=\"noopener noreferrer\">kubectl plugins</a>,</li>\n<li>Install them on your machine,</li>\n<li>And keep the installed plugins up-to-date.</li>\n</ul>\n<p>There are 225 kubectl plugins currently distributed on Krew. Krew works across all major platforms, like macOS, Linux, and Windows.</p>\n<h2 id=\"prow-cicd-for-kubernetes-️\" style=\"position:relative;\"><a href=\"#prow-cicd-for-kubernetes-%EF%B8%8F\" aria-label=\"prow cicd for kubernetes ️ permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Prow: CI/CD for Kubernetes ⚙️</h2>\n<p>Kubernetes' Project CI/CD is powered by <a href=\"http://prow.k8s.io\" target=\"_blank\" rel=\"noopener noreferrer\">Prow</a>, an open-source CI system that can scale to hundreds of thousands of jobs.</p>\n<p>The Kubernetes Testing SIG describes Prow as \"a CI/CD system built on Kubernetes for Kubernetes that executes jobs for building, testing, publishing, and deploying.\" However, that description does not highlight perhaps the most important inferred capability of Prow — a capability that is at the heart of best-of-breed CI/CD automation tools — that capability is automation that starts with code commits. In the case of Prow, it starts with a scalable stateless microservice called <a href=\"https://github.com/kubernetes/test-infra/blob/master/prow/cmd/README.md#core-components\" target=\"_blank\" rel=\"noopener noreferrer\">hook</a> that triggers native K8s CI/CD jobs (among a number of things that hook does via <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins\" target=\"_blank\" rel=\"noopener noreferrer\">plugins</a>).</p>\n<p>It is this GitHub automation capability that has been one of the key reasons why other K8s projects have adopted Prow for their own CI/CD. But Prow is more than just GitHub webhook automation and CI/CD job execution. Prow is also:</p>\n<ul>\n<li><strong>Comprehensive GitHub Automation</strong></li>\n<li><strong>ChatOps</strong> <a href=\"https://prow.k8s.io/command-help\" target=\"_blank\" rel=\"noopener noreferrer\">via simple /foo commands</a></li>\n<li><strong>Fine-grained GitHub policy and permission management</strong> via <a href=\"https://github.com/kubernetes/community/blob/master/contributors/guide/owners.md\" target=\"_blank\" rel=\"noopener noreferrer\">OWNERS files</a></li>\n<li><strong>GitHub PR/merge automation</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide\" target=\"_blank\" rel=\"noopener noreferrer\">tide</a></li>\n<li><strong>GitHub API request cache</strong> to minimize the impact of GitHub API limits — <a href=\"https://github.com/kubernetes/test-infra/tree/master/ghproxy\" target=\"_blank\" rel=\"noopener noreferrer\">ghProxy</a></li>\n<li><strong>GitHub Organization and repository membership and permissions management</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/peribolos\" target=\"_blank\" rel=\"noopener noreferrer\">peribolos</a></li>\n<li><strong>GitHub labels management</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins/label\" target=\"_blank\" rel=\"noopener noreferrer\">label plugin</a></li>\n<li><strong>GitHub branch protection configuration</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector\" target=\"_blank\" rel=\"noopener noreferrer\">branchprotector</a></li>\n<li><strong>GitHub release notes management</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins/releasenote\" target=\"_blank\" rel=\"noopener noreferrer\">releasenote</a></li>\n<li><strong>Scalable, cacheable</strong> <a href=\"https://github.com/kubernetes/test-infra/blob/master/prow/scaling.md#github-api-cache\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub API cache</a></li>\n<li><strong>GitHub bot</strong> with <a href=\"https://github.com/k8s-ci-robot\" target=\"_blank\" rel=\"noopener noreferrer\">Prow's bot being an active GitHub user since 2016</a></li>\n<li><strong>Multi-engine CI/CD Job Execution</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/plank\" target=\"_blank\" rel=\"noopener noreferrer\">plank</a></li>\n<li><strong>CI/CD Reporting</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/crier\" target=\"_blank\" rel=\"noopener noreferrer\">crier</a></li>\n<li><strong>CI/CD Dashboards</strong> for viewing job history, merge status, and more — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/cmd/deck\" target=\"_blank\" rel=\"noopener noreferrer\">deck</a></li>\n<li><strong>Pluggable Artifact Viewer</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/spyglass\" target=\"_blank\" rel=\"noopener noreferrer\">Spyglass</a></li>\n<li><strong>Prometheus Metrics</strong> for monitoring and alerting — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/metrics\" target=\"_blank\" rel=\"noopener noreferrer\">metrics</a></li>\n<li><strong>Config-as-Code for Its Own Configuration</strong> — <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins/updateconfig\" target=\"_blank\" rel=\"noopener noreferrer\">updateconfig</a></li>\n</ul>\n<p>And I am sure I am still missing a bunch of things — like <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins/cat\" target=\"_blank\" rel=\"noopener noreferrer\">cats</a> and <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow/plugins/dog\" target=\"_blank\" rel=\"noopener noreferrer\">dogs</a>.</p>\n<h2 id=\"extending-kubernetes-api-\" style=\"position:relative;\"><a href=\"#extending-kubernetes-api-\" aria-label=\"extending kubernetes api  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Extending Kubernetes API 🚀</h2>\n<p>Did you know you can extend the Kubernetes API itself? Meet the <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes API Aggregator Layer</a>, a powerful tool for introducing subresources or aggregating them, like the custom metrics server.</p>\n<p>The aggregation layer enables installing additional Kubernetes-style APIs in your cluster. These can either be pre-built, existing 3rd party solutions, such as <a href=\"https://github.com/kubernetes-incubator/service-catalog/blob/master/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">service-catalog</a>, or user-created APIs like <a href=\"https://github.com/kubernetes-incubator/apiserver-builder/blob/master/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">apiserver-builder</a>, which can get you started.</p>\n<h2 id=\"auto-provisioning-namespaces-️\" style=\"position:relative;\"><a href=\"#auto-provisioning-namespaces-%EF%B8%8F\" aria-label=\"auto provisioning namespaces ️ permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Auto-Provisioning Namespaces 🛠️</h2>\n<p>There's an easy way to auto-provision namespaces without giving extra permissions to your users. Use the <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceautoprovision\" target=\"_blank\" rel=\"noopener noreferrer\">NamespaceAutoProvision Admission controller</a>.</p>\n<p>This admission controller examines all incoming requests on namespaced resources and checks if the referenced namespace exists. It creates a namespace if it cannot be found. This admission controller is useful in deployments that do not want to restrict the creation of a namespace prior to its usage.</p>\n<h2 id=\"enforcing-customrules\" style=\"position:relative;\"><a href=\"#enforcing-customrules\" aria-label=\"enforcing customrules permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Enforcing Custom Rules</h2>\n<p>Kubernetes offers a simple way to intercept and validate requests with ValidatingAdmissionWebhooks and MutatingAdmissionWebhooks.</p>\n<p><a href=\"https://github.com/slackhq/simple-kubernetes-webhook\" target=\"_blank\" rel=\"noopener noreferrer\">This</a> is a simple <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes admission webhook</a>. It is meant to be used as a validating and mutating admission webhook only and does not support any controller logic.</p>\n<p>It has been developed as a simple Go web service without using any framework or <code class=\"language-text\">boilerplate</code> such as <code class=\"language-text\">kubebuilder</code>.</p>\n<p>This project is aimed at illustrating how to build a fully functioning admission webhook in the simplest way possible. Most existing examples found on the web rely on heavy machinery using powerful frameworks, yet fail to illustrate how to implement a lightweight webhook that can do much-needed actions such as rejecting a pod for compliance reasons or injecting helpful environment variables.</p>\n<h2 id=\"dynamic-resource-allocation-\" style=\"position:relative;\"><a href=\"#dynamic-resource-allocation-\" aria-label=\"dynamic resource allocation  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Dynamic Resource Allocation 🚀</h2>\n<p>Allocate resources outside your cluster with Dynamic <a href=\"https://kubernetes.io/blog/2022/12/15/dynamic-resource-allocation/\" target=\"_blank\" rel=\"noopener noreferrer\">Resource Allocation</a>. Since K8s v1.26, use <strong>ResourceClass</strong> and <strong>ResourceClaims</strong> to extend offerings beyond the cluster.</p>\n<p>In contrast to native resources (such as CPU or RAM) and <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources\" target=\"_blank\" rel=\"noopener noreferrer\">extended resources</a> (managed by a device plugin, advertised by kubelet), the scheduler has no knowledge of what dynamic resources are available in a cluster or how they could be split up to satisfy the requirements of a specific ResourceClaim. Resource drivers are responsible for that.</p>\n<p>Drivers mark <strong>ResourceClaims</strong> as allocated once resources for it are reserved. This also then tells the scheduler where in the cluster a claimed resource is actually available.</p>\n<p><strong>ResourceClaims</strong> can get resources allocated as soon as the ResourceClaim is created (immediate allocation), without considering which Pods will use the resource. The default (wait for first consumer) is to delay allocation until a Pod that relies on the ResourceClaim becomes eligible for scheduling. This design with two allocation options is similar to how Kubernetes handles storage provisioning with PersistentVolumes and PersistentVolumeClaims.</p>\n<h2 id=\"managing-requests-in-the-kubernetes-api-\" style=\"position:relative;\"><a href=\"#managing-requests-in-the-kubernetes-api-\" aria-label=\"managing requests in the kubernetes api  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Managing Requests in the Kubernetes API 📜</h2>\n<p>In Kubernetes, request queue management is handled by <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/flow-control\" target=\"_blank\" rel=\"noopener noreferrer\">API Priority and Fairness (APF)</a>. It is enabled by default in Kubernetes 1.20 and beyond. The API server also provides two parameters, <code class=\"language-text\">--max-requests-inflight</code> (default is 400) and <code class=\"language-text\">--max-mutating-requests-inflight</code> (default is 200), for limiting the number of requests. If APF is enabled, both of these parameters are summed up — and that's how the API server's total concurrency limit is defined.</p>\n<p>That said, there are some finer details to account for:</p>\n<ul>\n<li>Long-running API requests (e.g., viewing logs or executing commands in a pod) are not subject to APF limits, and neither are WATCH requests.</li>\n<li>There is also a special predefined priority level called exempt. Requests from this level are processed immediately.</li>\n</ul>\n<p>So you can fine-tune how the Kubernetes API server queues and handles requests to prioritize essential requests and manage latency effectively.</p>\n<h3 id=\"api-priority-with-code-classlanguage-textkubectlcode\" style=\"position:relative;\"><a href=\"#api-priority-with-code-classlanguage-textkubectlcode\" aria-label=\"api priority with code classlanguage textkubectlcode permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>API Priority with <code class=\"language-text\">kubectl</code></h3>\n<p>You can explore how busy your Kubernetes API server is by examining the Priority Level queue. With the APIPriorityAndFairness feature enabled, the kube-apiserver serves the following additional paths at its HTTP(S) ports. You need to ensure you have permissions to access these endpoints. You don't have to do anything if you are using admin. Permissions can be granted if needed following the RBAC doc to access <code class=\"language-text\">/debug/api_priority_and_fairness/</code> by specifying nonResourceURLs.</p>\n<ul>\n<li>\n<p><code class=\"language-text\">/debug/api_priority_and_fairness/dump_priority_levels</code> - a listing of all the priority levels and the current state of each. You can fetch like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> /debug/api_priority_and_fairness/dump_priority_levels</code></pre></div>\n<p>The output will be in CSV and similar to this:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests, DispatchedRequests, RejectedRequests, TimedoutRequests, CancelledRequests\ncatch-all,         0,            true,   false,       0,               0,                 1,                  0,                0,                0\nexempt,            0,            true,   false,       0,               0,                 0,                  0,                0,                0\nglobal-default,    0,            true,   false,       0,               0,                 46,                 0,                0,                0\nleader-election,   0,            true,   false,       0,               0,                 4,                  0,                0,                0\nnode-high,         0,            true,   false,       0,               0,                 34,                 0,                0,                0\nsystem,            0,            true,   false,       0,               0,                 48,                 0,                0,                0\nworkload-high,     0,            true,   false,       0,               0,                 500,                0,                0,                0\nworkload-low,      0,            true,   false,       0,               0,                 0,                  0,                0,                0</code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">/debug/api_priority_and_fairness/dump_queues</code> - a listing of all the queues and their current state. You can fetch like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> /debug/api_priority_and_fairness/dump_queues</code></pre></div>\n<p>The output will be in CSV and similar to this:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">PriorityLevelName, Index,  PendingRequests, ExecutingRequests, SeatsInUse, NextDispatchR,   InitialSeatsSum, MaxSeatsSum, TotalWorkSum\nworkload-low,      14,     27,              0,                 0,          77.64342019ss,   270,             270,         0.81000000ss\nworkload-low,      74,     26,              0,                 0,          76.95387841ss,   260,             260,         0.78000000ss\n...\nleader-election,   0,      0,               0,                 0,          5088.87053833ss, 0,               0,           0.00000000ss\nleader-election,   1,      0,               0,                 0,          0.00000000ss,    0,               0,           0.00000000ss\n...\nworkload-high,     0,      0,               0,                 0,          0.00000000ss,    0,               0,           0.00000000ss\nworkload-high,     1,      0,               0,                 0,          1119.44936475ss, 0,               0,           0.00000000ss</code></pre></div>\n</li>\n<li>\n<p><code class=\"language-text\">/debug/api_priority_and_fairness/dump_requests</code> - a listing of all the requests including requests waiting in a queue and requests being executed. You can fetch like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> /debug/api_priority_and_fairness/dump_requests</code></pre></div>\n<p>The output will be in CSV and similar to this:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">PriorityLevelName, FlowSchemaName,   QueueIndex, RequestIndexInQueue, FlowDistingsher,                        ArriveTime,                     InitialSeats, FinalSeats, AdditionalLatency, StartTime\nexempt,            exempt,           -1,         -1,                  ,                                       <span class=\"token number\">2023</span>-07-15T04:51:25.596404345Z, <span class=\"token number\">1</span>,            <span class=\"token number\">0</span>,          0s,                <span class=\"token number\">2023</span>-07-15T04:51:25.596404345Z\nworkload-low,      service-accounts, <span class=\"token number\">14</span>,         <span class=\"token number\">0</span>,                   system:serviceaccount:default:loadtest, <span class=\"token number\">2023</span>-07-18T00:12:51.386556253Z, <span class=\"token number\">10</span>,           <span class=\"token number\">0</span>,          0s,                0001-01-01T00:00:00Z\nworkload-low,      service-accounts, <span class=\"token number\">14</span>,         <span class=\"token number\">1</span>,                   system:serviceaccount:default:loadtest, <span class=\"token number\">2023</span>-07-18T00:12:51.487092539Z, <span class=\"token number\">10</span>,           <span class=\"token number\">0</span>,          0s,                0001-01-01T00:00:00Z</code></pre></div>\n</li>\n</ul>\n<h2 id=\"manually-triggering-pod-evictions-\" style=\"position:relative;\"><a href=\"#manually-triggering-pod-evictions-\" aria-label=\"manually triggering pod evictions  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Manually Triggering Pod Evictions 🚨</h2>\n<p>A safer alternative to deleting pods is using evictions, because they respect pod disruption budgets and other termination policies. You can manually trigger a pod eviction using the Kubernetes eviction API.</p>\n<p>Create a file called <code class=\"language-text\">eviction.json</code> with content similar to this:</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"apiVersion\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"policy/v1\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"kind\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Eviction\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"metadata\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"pod-name-here\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"namespace\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"default\"</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>And run this command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">curl</span> <span class=\"token parameter variable\">-v</span> <span class=\"token parameter variable\">-H</span> <span class=\"token string\">'Content-type: application/json'</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/pod-name-here/eviction <span class=\"token parameter variable\">-d</span> @eviction.json</code></pre></div>\n<h2 id=\"pod-overhead-️️\" style=\"position:relative;\"><a href=\"#pod-overhead-%EF%B8%8F%EF%B8%8F\" aria-label=\"pod overhead ️️ permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Pod Overhead 🏋️‍♂️</h2>\n<p>When you run a Pod on a Node, the Pod itself takes an amount of system resources. These resources are additional to the resources needed to run the container(s) inside the Pod. In Kubernetes, Pod Overhead is a way to account for the resources consumed by the Pod infrastructure on top of the container requests &#x26; limits.</p>\n<p>In Kubernetes, the Pod's overhead is set at <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\" target=\"_blank\" rel=\"noopener noreferrer\">admission</a> time according to the overhead associated with the Pod's <a href=\"https://kubernetes.io/docs/concepts/containers/runtime-class/\" target=\"_blank\" rel=\"noopener noreferrer\">RuntimeClass</a>.</p>\n<p>A pod's overhead is considered in addition to the sum of container resource requests when scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup, and when carrying out Pod eviction ranking. You need to make sure a RuntimeClass is utilized which defines the overhead field.</p>\n<p>To work with Pod overhead, you need a RuntimeClass that defines the overhead field. As an example, you could use the following RuntimeClass definition with a virtualization container runtime that uses around 120MiB per Pod for the virtual machine and the guest OS:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> node.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> RuntimeClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> kata<span class=\"token punctuation\">-</span>fc\n<span class=\"token key atrule\">handler</span><span class=\"token punctuation\">:</span> kata<span class=\"token punctuation\">-</span>fc\n<span class=\"token key atrule\">overhead</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">podFixed</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"120Mi\"</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"250m\"</span></code></pre></div>\n<p>Workloads which are created which specify the kata-fc RuntimeClass handler will take the memory and cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.</p>\n<p>Consider running the given example workload, <code class=\"language-text\">test-pod</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> test<span class=\"token punctuation\">-</span>pod\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">runtimeClassName</span><span class=\"token punctuation\">:</span> kata<span class=\"token punctuation\">-</span>fc\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> busybox<span class=\"token punctuation\">-</span>ctr\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> busybox<span class=\"token punctuation\">:</span><span class=\"token number\">1.28</span>\n        <span class=\"token key atrule\">stdin</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n        <span class=\"token key atrule\">tty</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 500m\n                <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 100Mi\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx<span class=\"token punctuation\">-</span>ctr\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 1500m\n                <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 100Mi</code></pre></div>\n<p>At admission time the RuntimeClass admission controller updates the workload's PodSpec to include the overhead as described in the RuntimeClass. If the PodSpec already has this field defined, the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod to include an overhead.</p>\n<p>After the RuntimeClass admission controller has made modifications, you can check the updated Pod overhead value:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get pod test-pod <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">'{.spec.overhead}'</span></code></pre></div>\n<p>The output is:</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\">map<span class=\"token punctuation\">[</span>cpu<span class=\"token operator\">:</span>250m memory<span class=\"token operator\">:</span>120Mi<span class=\"token punctuation\">]</span></code></pre></div>\n<h2 id=\"future-enhancements-\" style=\"position:relative;\"><a href=\"#future-enhancements-\" aria-label=\"future enhancements  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Future Enhancements 🔮</h2>\n<p>All the future enhancements to the Kubernetes-adjacent projects are publicly available and maintained in git. You can find it <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>. If you have any good idea (and the resources to make it a reality), you can even submit your own!</p>\n<br>\n<p><strong><em>Until next time, つづく 🎉</em></strong></p>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <strong><em>Until next time 🎉</em></strong></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":13,"rawMarkdownBody":"\n> **Lesser-Known Aspects of Kubernetes 💎**\n\n## Overview 👀\n\nKubernetes has revolutionized the way we manage containerized applications, but it's packed with hidden features that even experienced users might not be aware of.\n\nLet's dive into a few of these hidden gems and discover the lesser-known capabilities of Kubernetes.\n\nI hope this blog post has been helpful.\n\n## Sorting and Organizing Your Pods 📋\n\nEver wished you could organize your pod list in a more meaningful way? With Kubernetes, you can! Simply use the `--sort-by` flag along with the `kubectl get pods` command to sort your pods by various criteria, such as pod name or creation time.\n\nRunning `kubectl get pods --sort-by=.metadata.name` might just save you from endless scrolling through your pod list. Let's sort the pods in descending order, i.e., with the newest pods appearing first:\n\n```shell\nkubectl get pods --sort-by=.metadata.creationTimestamp --no-headers | tail -r\nubuntu-pod-3             2/2     Running   0          5m17s\nubuntu-pod-2             2/2     Running   0          13m7s\nubuntu-pod-1             2/2     Running   0          26m\n```\n\n## Listing All Object Types 📜\n\nDid you know you can list all the object types that your cluster supports? Use the `kubectl api-resources` command:\n\n```shell\nkubectl api-resources\n```\n\nWhen we want a more encompassing list of all resources in a namespace, we can combine the `kubectl api-resources` command with `kubectl get`:\n\n```shell\nkubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --ignore-not-found --show-kind -n <namespace>\n```\n\n`kubectl api-resources --verbs=list --namespaced -o name` retrieves all namespaced API resource types that support the list API verb. Then it outputs their names. Those names are then redirected to `xargs` as standard input.\n\n[`xargs -n 1`](https://www.baeldung.com/linux/xargs-multiple-arguments) singly passes each of those names as initial arguments to `kubectl get --ignore-not-found --show-kind -n <namespace>`. Then, the `kubectl get` command returns a list of resources belonging to each resource type in the specified namespace.\n\n## Default Resources and Limits with LimitRange and ResourceQuotas 🚦\n\nIn Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Namespaces are a way to divide cluster resources into groups for multiple users (via resource-quota). Each namespace will have one or multiple containers running inside it.\n\nAfter creating a namespace for each team within the cluster, consider that what if one team, i.e., namespace, consumes more resources from the cluster like CPU and memory, and other teams' resources starve for resources as the cluster has a very limited amount of available hardware resources. This creates a noisy neighbor problem within the cluster.\n\nTo avoid this, as an administrator, first, you create a namespace within the cluster, and then you can use ResourceQuota and LimitRange to assign resource quotas on namespaces and set limits for containers running inside any namespace.\n\n### Resource Quotas\n\nAfter creating namespaces, we can use the ResourceQuota object to limit the total amount of resources used by the namespace. We can use ResourceQuota to set limits for different object types that can be created within a namespace along with setting quotas for resources like CPU and memory.\n\nA ResourceQuota for setting a quota on resources looks like this:\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n    name: teamx-resource-quota\n    namespace: teamx\nspec:\n    hard:\n        limits.cpu: 150m\n        limits.memory: 600Mi\n        requests.cpu: 150m\n        requests.memory: 600Mi\n```\n\n- `limits.cpu` is the maximum CPU limit for all the containers in the Namespace, i.e., the entire namespace.\n- `limits.memory` is the maximum Memory limit for all containers in the Namespace, i.e., the entire namespace.\n- `requests.cpu` is the maximum CPU requests for all the containers in the Namespace. As per the above YAML, the total requested CPU in the Namespace should be less than 150m.\n- `requests.memory` is the maximum Memory requests for all the containers in the Namespace. As per the above YAML, the total requested memory in the namespace should be less than 600Mi.\n\n### LimitRange for Containers\n\nWe can create a LimitRange object in our Namespace which can be used to set limits on resources on containers running within the namespace. This is used to provide default limit values for Pods which do not specify this value themselves to equally distribute resources within a namespace.\n\nA LimitRange provides constraints that can:\n\n- Apply minimum and maximum CPU resources usage limit per Pod or Container in a namespace.\n- Apply minimum and maximum memory request limit per PersistentVolumeClaim in a namespace.\n- Apply minimum and maximum CPU resources usage limit per Pod or Container in a namespace.\n- Set default request/limit for resources within a namespace and then automatically set the limits to Containers at runtime.\n\n```yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n    name: teamx-limit-range\nspec:\n    limits:\n    - default:\n            memory: 200Mi\n            cpu: 50m\n        defaultRequest:\n            memory: 200Mi\n            cpu: 50m\n        max:\n            memory: 200Mi\n            cpu: 50m\n        min:\n            memory: 200Mi\n            cpu: 50m\n        type: Container\n```\n\nThe above YAML file has 4 sections: `max`, `min`, `default`, and `defaultRequest`.\n\n- The `default` section will set up the default limits for a container in a pod. Any container with no limits defined will get these values assigned as default.\n- The `defaultRequest` section will set up the default requests for a container in a pod. Any container with no requests defined will get these values assigned as default.\n- The `max` section will set up the maximum limits that a container in a Pod can set. The value specified in the `default` section cannot be higher than this value.\n- The `min` section will set up the minimum requests that a container in a Pod can set. The value specified in the `defaultRequest` section cannot be lower than this value.\n\n## kubectl debug\n\nOne of the most forgotten but powerful `kubectl` commands is `debug`. It allows you to create a sidecar container on any pod, copy a pod to a new instance for debugging, and even access the pod's filesystem.\n\n### Debugging a Node 🖥️\n\nUse the `kubectl debug node` command to deploy a Pod to a Node that you want to troubleshoot. This command is helpful in scenarios where you can't access your Node by using an SSH connection. When the Pod is created, the Pod opens an interactive shell on the Node. To create an interactive shell on a Node named `mynode`, run:\n\n```shell\nkubectl debug node/mynode -ti --image=ubuntu -- chroot /host bash\n```\n\n### Adding Ephemeral Containers 🐳\n\nYou can also use the `kubectl debug` command to add ephemeral containers to a running Pod for debugging.\n\nFirst, create a pod for the example:\n\n```shell\nkubectl run ephemeral-demo --image=registry.k8s.io/pause:3.1 --restart=Never\n```\n\nThe examples in this section use the [pause container image](https://registry.k8s.io/pause:3.1) because it does not contain debugging utilities, but this method works with all container images. If you attempt to use `kubectl exec` to create a shell, you will see an error because there is no shell in this container image.\n\n```shell\nkubectl exec -it ephemeral-demo -- sh\nOCI runtime exec failed: exec failed: container_linux.go:346: starting container process caused \"exec: \\\"sh\\\": executable file not found in $PATH\": unknown\n```\n\nYou can instead add a debugging container using `kubectl debug`. If you specify the `-i/--interactive` argument, `kubectl` will automatically attach to the console of the Ephemeral Container.\n\n```shell\nkubectl debug -it ephemeral-demo --image=busybox:1.28 --target=ephemeral-demo\nDefaulting debug container name to debugger-8xzrl.\nIf you don't see a command prompt, try pressing enter.\n/ #\n```\n\nThis command adds a new `busybox` container and attaches to it. The `--target` parameter targets the process namespace of another container. It's necessary here because `kubectl run` does not enable process namespace sharing in the pod it creates.\n\n## Krew: The Plugin Marketplace 🚀\n\nThere's a massive marketplace of Kubectl plugins that can extend its functionality and make your life easier. Meet [Krew](https://krew.sigs.k8s.io/docs/user-guide/setup/install/):\n\nKrew is the plugin manager for the `kubectl` command-line tool.\n\nKrew helps you:\n\n- Discover [kubectl plugins](https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/),\n- Install them on your machine,\n- And keep the installed plugins up-to-date.\n\nThere are 225 kubectl plugins currently distributed on Krew. Krew works across all major platforms, like macOS, Linux, and Windows.\n\n## Prow: CI/CD for Kubernetes ⚙️\n\nKubernetes' Project CI/CD is powered by [Prow](http://prow.k8s.io), an open-source CI system that can scale to hundreds of thousands of jobs.\n\nThe Kubernetes Testing SIG describes Prow as \"a CI/CD system built on Kubernetes for Kubernetes that executes jobs for building, testing, publishing, and deploying.\" However, that description does not highlight perhaps the most important inferred capability of Prow — a capability that is at the heart of best-of-breed CI/CD automation tools — that capability is automation that starts with code commits. In the case of Prow, it starts with a scalable stateless microservice called [hook](https://github.com/kubernetes/test-infra/blob/master/prow/cmd/README.md#core-components) that triggers native K8s CI/CD jobs (among a number of things that hook does via [plugins](https://github.com/kubernetes/test-infra/tree/master/prow/plugins)).\n\nIt is this GitHub automation capability that has been one of the key reasons why other K8s projects have adopted Prow for their own CI/CD. But Prow is more than just GitHub webhook automation and CI/CD job execution. Prow is also:\n\n- **Comprehensive GitHub Automation**\n- **ChatOps** [via simple /foo commands](https://prow.k8s.io/command-help)\n- **Fine-grained GitHub policy and permission management** via [OWNERS files](https://github.com/kubernetes/community/blob/master/contributors/guide/owners.md)\n- **GitHub PR/merge automation** — [tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide)\n- **GitHub API request cache** to minimize the impact of GitHub API limits — [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy)\n- **GitHub Organization and repository membership and permissions management** — [peribolos](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/peribolos)\n- **GitHub labels management** — [label plugin](https://github.com/kubernetes/test-infra/tree/master/prow/plugins/label)\n- **GitHub branch protection configuration** — [branchprotector](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector)\n- **GitHub release notes management** — [releasenote](https://github.com/kubernetes/test-infra/tree/master/prow/plugins/releasenote)\n- **Scalable, cacheable** [GitHub API cache](https://github.com/kubernetes/test-infra/blob/master/prow/scaling.md#github-api-cache)\n- **GitHub bot** with [Prow's bot being an active GitHub user since 2016](https://github.com/k8s-ci-robot)\n- **Multi-engine CI/CD Job Execution** — [plank](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/plank)\n- **CI/CD Reporting** — [crier](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/crier)\n- **CI/CD Dashboards** for viewing job history, merge status, and more — [deck](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/deck)\n- **Pluggable Artifact Viewer** — [Spyglass](https://github.com/kubernetes/test-infra/tree/master/prow/spyglass)\n- **Prometheus Metrics** for monitoring and alerting — [metrics](https://github.com/kubernetes/test-infra/tree/master/prow/metrics)\n- **Config-as-Code for Its Own Configuration** — [updateconfig](https://github.com/kubernetes/test-infra/tree/master/prow/plugins/updateconfig)\n\nAnd I am sure I am still missing a bunch of things — like [cats](https://github.com/kubernetes/test-infra/tree/master/prow/plugins/cat) and [dogs](https://github.com/kubernetes/test-infra/tree/master/prow/plugins/dog).\n\n## Extending Kubernetes API 🚀\n\nDid you know you can extend the Kubernetes API itself? Meet the [Kubernetes API Aggregator Layer](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/), a powerful tool for introducing subresources or aggregating them, like the custom metrics server.\n\nThe aggregation layer enables installing additional Kubernetes-style APIs in your cluster. These can either be pre-built, existing 3rd party solutions, such as [service-catalog](https://github.com/kubernetes-incubator/service-catalog/blob/master/README.md), or user-created APIs like [apiserver-builder](https://github.com/kubernetes-incubator/apiserver-builder/blob/master/README.md), which can get you started.\n\n## Auto-Provisioning Namespaces 🛠️\n\nThere's an easy way to auto-provision namespaces without giving extra permissions to your users. Use the [NamespaceAutoProvision Admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceautoprovision).\n\nThis admission controller examines all incoming requests on namespaced resources and checks if the referenced namespace exists. It creates a namespace if it cannot be found. This admission controller is useful in deployments that do not want to restrict the creation of a namespace prior to its usage.\n\n## Enforcing Custom Rules\n\nKubernetes offers a simple way to intercept and validate requests with ValidatingAdmissionWebhooks and MutatingAdmissionWebhooks.\n\n[This](https://github.com/slackhq/simple-kubernetes-webhook) is a simple [Kubernetes admission webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/). It is meant to be used as a validating and mutating admission webhook only and does not support any controller logic.\n\nIt has been developed as a simple Go web service without using any framework or `boilerplate` such as `kubebuilder`.\n\nThis project is aimed at illustrating how to build a fully functioning admission webhook in the simplest way possible. Most existing examples found on the web rely on heavy machinery using powerful frameworks, yet fail to illustrate how to implement a lightweight webhook that can do much-needed actions such as rejecting a pod for compliance reasons or injecting helpful environment variables.\n\n## Dynamic Resource Allocation 🚀\n\nAllocate resources outside your cluster with Dynamic [Resource Allocation](https://kubernetes.io/blog/2022/12/15/dynamic-resource-allocation/). Since K8s v1.26, use **ResourceClass** and **ResourceClaims** to extend offerings beyond the cluster.\n\nIn contrast to native resources (such as CPU or RAM) and [extended resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources) (managed by a device plugin, advertised by kubelet), the scheduler has no knowledge of what dynamic resources are available in a cluster or how they could be split up to satisfy the requirements of a specific ResourceClaim. Resource drivers are responsible for that.\n\nDrivers mark **ResourceClaims** as allocated once resources for it are reserved. This also then tells the scheduler where in the cluster a claimed resource is actually available.\n\n**ResourceClaims** can get resources allocated as soon as the ResourceClaim is created (immediate allocation), without considering which Pods will use the resource. The default (wait for first consumer) is to delay allocation until a Pod that relies on the ResourceClaim becomes eligible for scheduling. This design with two allocation options is similar to how Kubernetes handles storage provisioning with PersistentVolumes and PersistentVolumeClaims.\n\n## Managing Requests in the Kubernetes API 📜\n\nIn Kubernetes, request queue management is handled by [API Priority and Fairness (APF)](https://kubernetes.io/docs/concepts/cluster-administration/flow-control). It is enabled by default in Kubernetes 1.20 and beyond. The API server also provides two parameters, `--max-requests-inflight` (default is 400) and `--max-mutating-requests-inflight` (default is 200), for limiting the number of requests. If APF is enabled, both of these parameters are summed up — and that's how the API server's total concurrency limit is defined.\n\nThat said, there are some finer details to account for:\n- Long-running API requests (e.g., viewing logs or executing commands in a pod) are not subject to APF limits, and neither are WATCH requests.\n- There is also a special predefined priority level called exempt. Requests from this level are processed immediately.\n\nSo you can fine-tune how the Kubernetes API server queues and handles requests to prioritize essential requests and manage latency effectively.\n\n### API Priority with `kubectl`\n\nYou can explore how busy your Kubernetes API server is by examining the Priority Level queue. With the APIPriorityAndFairness feature enabled, the kube-apiserver serves the following additional paths at its HTTP(S) ports. You need to ensure you have permissions to access these endpoints. You don't have to do anything if you are using admin. Permissions can be granted if needed following the RBAC doc to access `/debug/api_priority_and_fairness/` by specifying nonResourceURLs.\n\n- `/debug/api_priority_and_fairness/dump_priority_levels` - a listing of all the priority levels and the current state of each. You can fetch like this:\n    ```shell\n    kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels\n    ```\n    The output will be in CSV and similar to this:\n    ```\n    PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests, DispatchedRequests, RejectedRequests, TimedoutRequests, CancelledRequests\n    catch-all,         0,            true,   false,       0,               0,                 1,                  0,                0,                0\n    exempt,            0,            true,   false,       0,               0,                 0,                  0,                0,                0\n    global-default,    0,            true,   false,       0,               0,                 46,                 0,                0,                0\n    leader-election,   0,            true,   false,       0,               0,                 4,                  0,                0,                0\n    node-high,         0,            true,   false,       0,               0,                 34,                 0,                0,                0\n    system,            0,            true,   false,       0,               0,                 48,                 0,                0,                0\n    workload-high,     0,            true,   false,       0,               0,                 500,                0,                0,                0\n    workload-low,      0,            true,   false,       0,               0,                 0,                  0,                0,                0\n    ```\n\n- `/debug/api_priority_and_fairness/dump_queues` - a listing of all the queues and their current state. You can fetch like this:\n    ```shell\n    kubectl get --raw /debug/api_priority_and_fairness/dump_queues\n    ```\n    The output will be in CSV and similar to this:\n    ```\n    PriorityLevelName, Index,  PendingRequests, ExecutingRequests, SeatsInUse, NextDispatchR,   InitialSeatsSum, MaxSeatsSum, TotalWorkSum\n    workload-low,      14,     27,              0,                 0,          77.64342019ss,   270,             270,         0.81000000ss\n    workload-low,      74,     26,              0,                 0,          76.95387841ss,   260,             260,         0.78000000ss\n    ...\n    leader-election,   0,      0,               0,                 0,          5088.87053833ss, 0,               0,           0.00000000ss\n    leader-election,   1,      0,               0,                 0,          0.00000000ss,    0,               0,           0.00000000ss\n    ...\n    workload-high,     0,      0,               0,                 0,          0.00000000ss,    0,               0,           0.00000000ss\n    workload-high,     1,      0,               0,                 0,          1119.44936475ss, 0,               0,           0.00000000ss\n    ```\n\n- `/debug/api_priority_and_fairness/dump_requests` - a listing of all the requests including requests waiting in a queue and requests being executed. You can fetch like this:\n\n    ```shell\n    kubectl get --raw /debug/api_priority_and_fairness/dump_requests\n    ```\n\n    The output will be in CSV and similar to this:\n\n    ```shell\n    PriorityLevelName, FlowSchemaName,   QueueIndex, RequestIndexInQueue, FlowDistingsher,                        ArriveTime,                     InitialSeats, FinalSeats, AdditionalLatency, StartTime\n    exempt,            exempt,           -1,         -1,                  ,                                       2023-07-15T04:51:25.596404345Z, 1,            0,          0s,                2023-07-15T04:51:25.596404345Z\n    workload-low,      service-accounts, 14,         0,                   system:serviceaccount:default:loadtest, 2023-07-18T00:12:51.386556253Z, 10,           0,          0s,                0001-01-01T00:00:00Z\n    workload-low,      service-accounts, 14,         1,                   system:serviceaccount:default:loadtest, 2023-07-18T00:12:51.487092539Z, 10,           0,          0s,                0001-01-01T00:00:00Z\n    ```\n\n## Manually Triggering Pod Evictions 🚨\n\nA safer alternative to deleting pods is using evictions, because they respect pod disruption budgets and other termination policies. You can manually trigger a pod eviction using the Kubernetes eviction API.\n\nCreate a file called `eviction.json` with content similar to this:\n\n```json\n{\n    \"apiVersion\": \"policy/v1\",\n    \"kind\": \"Eviction\",\n    \"metadata\": {\n        \"name\": \"pod-name-here\",\n        \"namespace\": \"default\"\n    }\n}\n```\n\nAnd run this command:\n\n```shell\ncurl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/pod-name-here/eviction -d @eviction.json\n```\n\n## Pod Overhead 🏋️‍♂️\n\nWhen you run a Pod on a Node, the Pod itself takes an amount of system resources. These resources are additional to the resources needed to run the container(s) inside the Pod. In Kubernetes, Pod Overhead is a way to account for the resources consumed by the Pod infrastructure on top of the container requests & limits.\n\nIn Kubernetes, the Pod's overhead is set at [admission](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks) time according to the overhead associated with the Pod's [RuntimeClass](https://kubernetes.io/docs/concepts/containers/runtime-class/).\n\nA pod's overhead is considered in addition to the sum of container resource requests when scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup, and when carrying out Pod eviction ranking. You need to make sure a RuntimeClass is utilized which defines the overhead field.\n\nTo work with Pod overhead, you need a RuntimeClass that defines the overhead field. As an example, you could use the following RuntimeClass definition with a virtualization container runtime that uses around 120MiB per Pod for the virtual machine and the guest OS:\n\n```yaml\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n    name: kata-fc\nhandler: kata-fc\noverhead:\n    podFixed:\n        memory: \"120Mi\"\n        cpu: \"250m\"\n```\n\nWorkloads which are created which specify the kata-fc RuntimeClass handler will take the memory and cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.\n\nConsider running the given example workload, `test-pod`:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n    name: test-pod\nspec:\n    runtimeClassName: kata-fc\n    containers:\n    - name: busybox-ctr\n        image: busybox:1.28\n        stdin: true\n        tty: true\n        resources:\n            limits:\n                cpu: 500m\n                memory: 100Mi\n    - name: nginx-ctr\n        image: nginx\n        resources:\n            limits:\n                cpu: 1500m\n                memory: 100Mi\n```\n\nAt admission time the RuntimeClass admission controller updates the workload's PodSpec to include the overhead as described in the RuntimeClass. If the PodSpec already has this field defined, the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod to include an overhead.\n\nAfter the RuntimeClass admission controller has made modifications, you can check the updated Pod overhead value:\n\n```shell\nkubectl get pod test-pod -o jsonpath='{.spec.overhead}'\n```\n\nThe output is:\n\n```json\nmap[cpu:250m memory:120Mi]\n```\n\n## Future Enhancements 🔮\n\nAll the future enhancements to the Kubernetes-adjacent projects are publicly available and maintained in git. You can find it [here](https://github.com/kubernetes/enhancements/tree/master/keps). If you have any good idea (and the resources to make it a reality), you can even submit your own!\n\n<br>\n\n**_Until next time, つづく 🎉_**\n\n> 💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  **_Until next time 🎉_**\n\n🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**♻️ LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**♻️ X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ✌🏻**\n\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n\n**📅 Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":2496},"frontmatter":{"id":"c542a5236a0fde8bc7c3cc8b","path":"/blog/hidden-gems-kubernetes/","humanDate":"Oct 29, 2024","fullDate":"2024-10-29","title":"Hidden Gems: A Few Things You Might Not Know About Kubernetes 💎","keywords":["Kubernetes","Hidden Features","Cloud-native","DevOps","k8s"],"excerpt":"Explore the lesser-known aspects of Kubernetes, uncovering hidden features and tips that can enhance your cloud-native experience.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVR42gGfAmD9AKHg7qDf7qXj8Z3d7ZfY6p3d7ZbY6ZXY6Zvd7aHg76Dg76Hh76Lh76Hh76Li8KPi8KLh8KDg7p/f7p/f7gCl5fKj4vCS1edhstBpuNSP0uV8xd6JzOCY1eWk5PKh4O+a3OyU1uie3u6X2eqV1+mZ2+ul4/Km4/Sl4vMAj8naYa/OQ5zDPY+2PYyxecTdltXlhJ6rnNHgk9jqp+Xyg8rgZLPRjM3ffcTbhszjkdXmhdHZbcTFY769AC1afCVSdzyHrStTdjJ0l0ikzEuJnzJkakmfu1CkxmSy0JHU5ovR5Gq40nm/2H3Fz1q9tUKtokCtoEGuoQAsVIIlP2kiOl8iPWEtVoAqU39CnqFGmpc7g49Ac40mSnk3WYJmq8dVq809hq4yd5onU3AtWXgtWHgtWHgAITRcIDRbIDRbHy9ZGiZTLj1iP3aMZLnNPIOqJENwIC9eK0t3Zr7bVq7PNn6oOn6nIzdpITNjIjRlIjRkAB4yWCM6ZiE0XStXdjNxgyI/YVeQs0h2nClTgDh3oB4wWzFWfVOVuEaGqjJpkjVymidIcyM4ZSQ8aSQ8aQAkO2okOWglSGNCo5tBqZ1Lrapjqb8hMGAkQm8pTHojN2UgNV0bMVIaLk4eNFYWJkAbLk4lPWsjOmcjOWcAIzpoIjRjJ1BmQaSbQaWcQJKTP5GSNHmGNX6FLmt5IDplHjBhHTBZIDZfIDZcHC9RHzJZGyxSHzJaIzpoACU+aytSciRHYECim0KmnjyZkj6dlkStoUKpnkStoUuUoUh2mjldhiM5aBwvWyA0Wy5KbTxjiSpFcSA0YwAxZIA3fYYeMlU9l5NCp55Cpp1Cpp5AoppGpqBzy9iJ4POP6/xssMY4W3lfnLNOgJhRhJ5Dbopmp782V4G5m0HUhcZ42gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/2b4496f746ba5af86f54bfd3ca80c4e9/a7a66/hidden-gems-k8s-cover.png","srcSet":"/static/2b4496f746ba5af86f54bfd3ca80c4e9/d0df2/hidden-gems-k8s-cover.png 750w,\n/static/2b4496f746ba5af86f54bfd3ca80c4e9/a7a66/hidden-gems-k8s-cover.png 1040w","sizes":"100vw"},"sources":[{"srcSet":"/static/2b4496f746ba5af86f54bfd3ca80c4e9/b9516/hidden-gems-k8s-cover.webp 750w,\n/static/2b4496f746ba5af86f54bfd3ca80c4e9/acf36/hidden-gems-k8s-cover.webp 1040w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5615384615384615}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/kubernetes-rbac-privilege-escalation-mitigation/","title":"Kubernetes RBAC: Privilege Escalation Exploits and Mitigations 🔒","date":"2024-10-29 21:30:00"},"excerpt":"RBAC: Mapping Out Privilege Escalation Routes 🔒 🎑 Introduction Kubernetes, the open-source container orchestration system, has…","html":"<blockquote>\n<p><strong>RBAC: Mapping Out Privilege Escalation Routes 🔒</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🎑 Introduction</h2>\n<p>Kubernetes, the open-source container orchestration system, has revolutionized the world of container management. Its RBAC module ensures that only authorized entities can access cluster resources. However, as with any complex system, there are nuances to be explored, and certain permissions can inadvertently pave the way for privilege escalation.</p>\n<p>In this blog post, we will explore seven distinct privilege escalation threat vectors, empowered by specific permissions, including:</p>\n<ul>\n<li>Creating Pods</li>\n<li>Reading Secrets</li>\n<li>Binding Roles</li>\n<li>Escalating Existing Roles</li>\n<li>Impersonating Entities in the Cluster</li>\n</ul>\n<p>For each threat vector, we will provide a detailed explanation of how the exploit can be executed and discuss potential mitigations.</p>\n<div style=\"width:100%;height:0;padding-bottom:89%;position:relative;\"><iframe src=\"https://giphy.com/embed/ul1omlrGG6kpO\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameborder=\"0\" class=\"giphy-embed\" allowfullscreen></iframe></div>\n<h2 id=\"️-pod-creation\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-pod-creation\" aria-label=\"️ pod creation permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Pod Creation</h2>\n<p>Pods are the basic building blocks of Kubernetes clusters. Each pod contains at least one container, which runs an application. In other words, if you're using Kubernetes, you're using pods.</p>\n<p>While RBAC ensures that only authorized users can create pods, it doesn't control what they can put in their pod definitions. Without proper <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener noreferrer\">admission control</a>, authorized users can create pods with arbitrary images, which can contain arbitrary privileges and configurations. This can lead to privilege escalation attacks.</p>\n<p><strong>Examples of Exploitation:</strong></p>\n<ul>\n<li>An attacker can create a pod with a malicious image that has root privileges, allowing them to gain root access to the Kubernetes node where the pod is running.</li>\n<li>An attacker can create a pod with an image that has the ability to escape the container and run arbitrary code on the Kubernetes node.</li>\n<li>An attacker can create a pod with an image that can exploit a vulnerability in the Kubernetes cluster, potentially gaining access to sensitive data or resources.</li>\n</ul>\n<p><strong>Mitigation Strategies:</strong></p>\n<ul>\n<li>Use <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener noreferrer\">admission control</a> to restrict the types of pods that can be created.</li>\n<li>Implement security policies to restrict the privileges of pods.</li>\n<li>Scan container images for vulnerabilities before running them in production.</li>\n<li>Monitor your Kubernetes cluster for suspicious activity.</li>\n</ul>\n<h2 id=\"️-mounting-a-service-account-token\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-mounting-a-service-account-token\" aria-label=\"️ mounting a service account token permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Mounting a Service Account Token</h2>\n<p>Pods can be configured to <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server\" target=\"_blank\" rel=\"noopener noreferrer\">mount the authorization token</a> for a service account within the same namespace. This means that an attacker could create a pod with a malicious image that steals the token from a high-privileged service account. The attacker could then use this token to perform API actions and escalate their privileges.</p>\n<p><strong>Mitigation Strategies:</strong></p>\n<ul>\n<li>Only mount service account tokens to pods that need them.</li>\n<li>Use security policies to restrict the privileges of pods.</li>\n</ul>\n<h2 id=\"️-unauthorized-cluster-node-access-risks-and-mitigations\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-unauthorized-cluster-node-access-risks-and-mitigations\" aria-label=\"️ unauthorized cluster node access risks and mitigations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Unauthorized Cluster Node Access: Risks and Mitigations</h2>\n<p>If you allow entities to create highly privileged containers without comprehensive admission control, you are creating security vulnerabilities. This can result in the creation of vulnerable Pods that enable attackers to gain access to the corresponding container and potentially infiltrate the Cluster's underlying node.</p>\n<p><strong>Risks of Unauthorized Cluster Node Access:</strong></p>\n<ul>\n<li>An attacker typically follows a two-step process: gaining access to the container and then executing traditional container escape techniques. Such an attack can have severe consequences within Kubernetes environments.</li>\n<li>Access to any Cluster node is a significant breach. With node access, an attacker can leverage the container engine to access ServiceAccount tokens from other Pods on the same node, potentially compromising the security of the entire Cluster.</li>\n<li>Additionally, the attacker can search for application data and configuration files on the node, exposing critical parts of the Cluster. They can also identify and target unrelated network hosts and services like DNS Servers.</li>\n</ul>\n<p><strong>Mitigation Strategies:</strong></p>\n<ul>\n<li>Implement strict admission control.</li>\n<li>Continuous monitoring and auditing.</li>\n<li>Network segmentation and access control lists.</li>\n</ul>\n<h2 id=\"️-secret-access-in-kubernetes-risks-and-mitigations\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-secret-access-in-kubernetes-risks-and-mitigations\" aria-label=\"️ secret access in kubernetes risks and mitigations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Secret Access in Kubernetes: Risks and Mitigations</h2>\n<p>It is important to grant the permission to read Secrets with caution, as it is a very powerful permission. In Kubernetes, Secrets can be used to store application-critical configuration data, as well as authorization tokens for ServiceAccounts.</p>\n<p><strong>Risks of Unauthorized Secret Access:</strong></p>\n<ul>\n<li>If an attacker is able to read Secrets, they could steal authorization tokens and essentially take over the corresponding ServiceAccounts. This could give them access to sensitive data, the ability to launch attacks against other nodes in the cluster, or disrupt the cluster's operations.</li>\n</ul>\n<p><strong>Mitigation Strategies:</strong></p>\n<ul>\n<li>Implement strict Role-Based Access Control (RBAC) policies to restrict who can read Secrets.</li>\n<li>Use a secrets management tool to encrypt Secrets at rest and in transit.</li>\n<li>Monitor for suspicious activity, such as a large number of failed attempts to read Secrets.</li>\n</ul>\n<h2 id=\"️-impersonation-in-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-impersonation-in-kubernetes\" aria-label=\"️ impersonation in kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Impersonation in Kubernetes</h2>\n<p>Impersonation in Kubernetes is a mechanism that allows an entity to act as another entity while performing API requests. This can be useful for administrators to test configurations or troubleshoot problems. However, attackers can also use impersonation to gain unauthorized access to the Kubernetes cluster or its resources.</p>\n<p><strong>How Impersonation Works:</strong></p>\n<ul>\n<li>Impersonation is done by adding the <code class=\"language-text\">Impersonate-User</code> header to the API request. The value of the header is the name of the entity that the user wants to impersonate. Kubernetes first checks whether impersonation is allowed for the target entity. If it is, Kubernetes proceeds as if the impersonated entity made the request.</li>\n</ul>\n<p><strong>Impersonating Service Accounts:</strong></p>\n<ul>\n<li>To impersonate a service account, the <code class=\"language-text\">Impersonate-User</code> header must be set to the following value:\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Impersonate-User: system:serviceaccount:&lt;namespace>:&lt;service-account-name></code></pre></div>\nFor example, to impersonate the default service account in the default namespace, the header would be set to:\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Impersonate-User: system:serviceaccount:default:default</code></pre></div>\n</li>\n</ul>\n<p><strong>Risks of Impersonation:</strong></p>\n<ul>\n<li>Impersonation can be a very powerful tool for attackers. If an attacker is able to impersonate a privileged entity, they could gain access to sensitive data, delete resources, or disrupt the operation of the Kubernetes cluster.</li>\n</ul>\n<p><strong>Mitigation Strategies:</strong></p>\n<ul>\n<li>Implement strict Role-Based Access Control (RBAC) policies to restrict who can impersonate entities.</li>\n<li>Monitor for suspicious activity, such as a large number of failed attempts to impersonate entities.</li>\n<li>Educate users about the risks of impersonation and how to avoid them.</li>\n</ul>\n<h2 id=\"️-rolebinding-permissions-in-kubernetes-implications-and-safeguards\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-rolebinding-permissions-in-kubernetes-implications-and-safeguards\" aria-label=\"️ rolebinding permissions in kubernetes implications and safeguards permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>▶️ Rolebinding Permissions in Kubernetes: Implications and Safeguards</h2>\n<p>The <code class=\"language-text\">bind</code> verb in Kubernetes allows an entity to assign a Role or ClusterRole to a subject (e.g., a user, group, or service account). This is a powerful permission, as it allows the entity to grant itself new permissions.</p>\n<p><strong>Binding Roles:</strong></p>\n<ul>\n<li>If an entity has the permission to create RoleBindings and the permission to bind Roles, it can assign arbitrary Roles to itself and elevate its privileges depending on the Roles available in the namespace in which these permissions are available.</li>\n</ul>\n<p><strong>Binding ClusterRoles within a Namespace:</strong></p>\n<ul>\n<li>If an entity has the permission to create RoleBindings and the permission to bind ClusterRoles, it can bind the permissions of a ClusterRole to the respective namespace. This is generally a greater privilege escalation than the first variant, as it allows the entity to gain the permissions of any ClusterRole, regardless of which namespace it is defined in.</li>\n</ul>\n<p><strong>Binding ClusterRoles Cluster-wide:</strong></p>\n<ul>\n<li>Finally, if an entity has the permission to create ClusterRoleBindings and the permission to bind ClusterRoles, it can gain the highest possible permissions within the Cluster by binding the permissions of the most privileged entities within the <code class=\"language-text\">kube-system</code> namespace to itself.</li>\n</ul>\n<p><strong>Risks of Binding Roles and ClusterRoles:</strong></p>\n<ul>\n<li>If an attacker gains the permission to bind Roles or ClusterRoles, they could elevate their privileges and gain access to sensitive data or disrupt the operation of the Kubernetes cluster.</li>\n</ul>\n<h2 id=\"mitigation-strategies\" style=\"position:relative;\"><a href=\"#mitigation-strategies\" aria-label=\"mitigation strategies permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Mitigation Strategies</h2>\n<ul>\n<li>Implement strict Role-Based Access Control (RBAC) policies to restrict who can bind Roles and ClusterRoles.</li>\n<li>Monitor for suspicious activity, such as a large number of attempts to bind Roles or ClusterRoles.</li>\n<li>Educate users about the risks of binding Roles and ClusterRoles and how to avoid them.</li>\n</ul>\n<p>📒 <strong>Privilege Escalation Prevention and Bootstrapping</strong></p>\n<p>The RBAC API prevents users from escalating privileges by editing roles or role bindings. Because this is enforced at the API level, it applies even when the RBAC authorizer is not in use.</p>\n<p>📒 <strong>Restrictions on Role Creation or Update</strong></p>\n<p>You can only create/update a role if at least one of the following conditions is true:</p>\n<ul>\n<li><strong>You already have all the permissions contained in the role</strong>, at the same scope as the object being modified (cluster-wide for a ClusterRole, within the same namespace or cluster-wide for a Role).</li>\n<li><strong>You are granted explicit permission</strong> to perform the <code class=\"language-text\">escalate</code> verb on the roles or clusterroles resource in the <code class=\"language-text\">rbac.authorization.k8s.io</code> API group.</li>\n</ul>\n<p>For example, if <code class=\"language-text\">user-1</code> does not have the ability to list Secrets cluster-wide, they cannot create a ClusterRole containing that permission. To allow a user to create/update roles:</p>\n<ol>\n<li><strong>Grant them a role</strong> that allows them to create/update Role or ClusterRole objects, as desired.</li>\n<li><strong>Grant them permission</strong> to include specific permissions in the roles they create/update:\n<ul>\n<li><strong>Implicitly</strong>, by giving them those permissions (if they attempt to create or modify a Role or ClusterRole with permissions they themselves have not been granted, the API request will be forbidden).</li>\n<li><strong>Explicitly</strong>, by giving them permission to perform the <code class=\"language-text\">escalate</code> verb on roles or clusterroles resources in the <code class=\"language-text\">rbac.authorization.k8s.io</code> API group.</li>\n</ul>\n</li>\n</ol>\n<p>For more details, refer to the <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes RBAC documentation</a>.</p>\n<p>📒 <strong>Restrictions on Role Binding Creation or Update</strong></p>\n<p>You can only create/update a role binding if you already have all the permissions contained in the referenced role (at the same scope as the role binding) or if you have been authorized to perform the <code class=\"language-text\">bind</code> verb on the referenced role.</p>\n<p>For example, if <code class=\"language-text\">user-1</code> does not have the ability to list Secrets cluster-wide, they cannot create a <code class=\"language-text\">ClusterRoleBinding</code> to a role that grants that permission.</p>\n<p>To allow a user to create/update role bindings:</p>\n<ol>\n<li><strong>Grant them a role</strong> that allows them to create/update <code class=\"language-text\">RoleBinding</code> or <code class=\"language-text\">ClusterRoleBinding</code> objects, as desired.</li>\n<li><strong>Grant them permissions needed to bind a particular role:</strong>\n<ul>\n<li><strong>Implicitly</strong>, by giving them the permissions contained in the role.</li>\n<li><strong>Explicitly</strong>, by giving them permission to perform the <code class=\"language-text\">bind</code> verb on the particular Role (or ClusterRole).</li>\n</ul>\n</li>\n</ol>\n<p>For more details, refer to the <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes RBAC documentation</a>.</p>\n<p>For example, this ClusterRole and RoleBinding would allow user-1 to grant other users the admin, edit, and view roles in the namespace user-1-namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> rbac.authorization.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ClusterRole\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> role<span class=\"token punctuation\">-</span>grantor\n<span class=\"token key atrule\">rules</span><span class=\"token punctuation\">:</span>\n<span class=\"token punctuation\">-</span> <span class=\"token key atrule\">apiGroups</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"rbac.authorization.k8s.io\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"rolebindings\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">verbs</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"create\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">-</span> <span class=\"token key atrule\">apiGroups</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"rbac.authorization.k8s.io\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"clusterroles\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">verbs</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"bind\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token comment\"># omit resourceNames to allow binding any ClusterRole</span>\n  <span class=\"token key atrule\">resourceNames</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"admin\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"edit\"</span><span class=\"token punctuation\">,</span><span class=\"token string\">\"view\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> rbac.authorization.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> RoleBinding\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> role<span class=\"token punctuation\">-</span>grantor<span class=\"token punctuation\">-</span>binding\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> user<span class=\"token punctuation\">-</span>1<span class=\"token punctuation\">-</span>namespace\n<span class=\"token key atrule\">roleRef</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">apiGroup</span><span class=\"token punctuation\">:</span> rbac.authorization.k8s.io\n  <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> ClusterRole\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> role<span class=\"token punctuation\">-</span>grantor\n<span class=\"token key atrule\">subjects</span><span class=\"token punctuation\">:</span>\n<span class=\"token punctuation\">-</span> <span class=\"token key atrule\">apiGroup</span><span class=\"token punctuation\">:</span> rbac.authorization.k8s.io\n  <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> User\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> user<span class=\"token punctuation\">-</span><span class=\"token number\">1</span></code></pre></div>\n<p><strong>📜 Bootstrapping Initial Roles and Role Bindings</strong></p>\n<p>When bootstrapping the first roles and role bindings, it is necessary for the initial user to grant permissions they do not yet have. To bootstrap initial roles and role bindings:</p>\n<ul>\n<li><strong>Use a credential with the <code class=\"language-text\">system:masters</code> group</strong>, which is bound to the <code class=\"language-text\">cluster-admin</code> super-user role by the default bindings.</li>\n</ul>\n<p>For more details, refer to the <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes RBAC documentation</a>.</p>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🏁 Conclusion</h2>\n<p>In this blog post, we have discussed three of the most common security threats to Kubernetes environments:</p>\n<ul>\n<li><strong>Unauthorized access to Cluster nodes</strong></li>\n<li><strong>Impersonation</strong></li>\n<li><strong>Binding Roles and ClusterRoles</strong></li>\n</ul>\n<p>We have also provided some recommendations on how to mitigate these risks.</p>\n<p>🔐 <strong>Key Takeaway:</strong> Security is a continuous process. There is no single solution that will protect your Kubernetes cluster from all threats. However, by implementing a layered security approach and following the recommendations in this blog post, you can help to reduce your risk and protect your Kubernetes environment.</p>\n<p>For further reading, check out the <a href=\"https://kubernetes.io/docs/concepts/security/overview/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Security Best Practices</a>.</p>"},"nextThought":{"frontmatter":{"path":"/blog/cluster-dev-cloud-management/","title":"Discover Cluster.dev: A Simple Solution for Cloud Infrastructure Management ☁️","date":"2024-10-29 20:06:00"},"excerpt":"Manage Cloud Infrastructures Declaratively ☁️ 📙 Introduction Have you ever wished for a single tool to manage all your cloud infrastructure…","html":"<blockquote>\n<p><strong>Manage Cloud Infrastructures Declaratively ☁️</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📙 Introduction</h2>\n<p>Have you ever wished for a single tool to manage all your cloud infrastructure needs?</p>\n<p><a href=\"https://cluster.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">Cluster.dev</a> is one of these tools that will change the way you handle cloud-native infrastructures.</p>\n<p>In this blog post, we will explore the incredible features of Cluster.dev and how it simplifies the deployment, testing, and distribution of cloud-native components.</p>\n<p>Get ready to discover Cluster.dev and how it can make your infrastructure management a breeze.</p>\n<h2 id=\"-when-do-i-need-clusterdev\" style=\"position:relative;\"><a href=\"#-when-do-i-need-clusterdev\" aria-label=\" when do i need clusterdev permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>❓ When do I need Cluster.dev</h2>\n<p>If you have a common infrastructure pattern that contains multiple components stuck together, like a bunch of TF-modules or a set of K8s addons, you need to re-use this pattern inside your projects.\nIf you develop an infrastructure platform that you ship to other teams, and they need to launch new infrastructures from your template.\nIf you build a complex infrastructure that contains different technologies, and you need to perform integration testing to confirm the components' interoperability. After which you can promote the changes to the next environments.\nIf you are a software vendor and you need to deliver infrastructure deployment along with your software.</p>\n<h3 id=\"for-whom\" style=\"position:relative;\"><a href=\"#for-whom\" aria-label=\"for whom permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>For whom</h3>\n<p>The target audience is primarily infrastructure teams with a certain level of expertise, to whom Cluster.dev could simplify their daily routine: creating clusters, deploying services to the clusters, launching environments, sharing experience with development teams, etc.</p>\n<p>💠 <strong>Base concept diagrams</strong></p>\n<p>Stack templates are composed of <a href=\"https://docs.cluster.dev/units-overview/\" target=\"_blank\" rel=\"noopener noreferrer\">units</a> - Lego-like building blocks responsible for passing variables to a particular technology.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABiklEQVR42pVTPU/CUBR9IChIsQIBHRxJjAyEjaXg4MLowkbiYELi4O5i+jsYdeQHkOgACyGSMHc0sbUkECPftKHC9d5nSYgYqCc56U3Peaf3vd7H2G8sKnsMagKDpwCDsn9Nh5KXdcqoI8m7FYuXAwbgobIOEOxOp+fEz9k027OMi9fB4JS0BJR3uXcroB6M53LcqOu6ZBomECfjiQUIwzAfSCtBy0teR4HJQiFApaqq2dnMAkVRoFqt8kDLsijQHU+louyj4qBDNIXj8RNa1G63s5b1BaPRGHq9Pg+cz+ePZLuSZZ/jDostPHiEaZpn3U63+a5pTV3TGuqbqgz7w3vSZDpnp4E/f5rOqOZZ18HNNe5xFIghfCxobJDydZjdXUbYbS7KSkWRQcPP33PPHx/cADdyB3mIPLK5j3TZ2r/gEgQhKopiCGvqIoYUkT7kMTKyGppMJmOSJCW2hdIiwV4YWumORoWPVT6fpx2wdDp9k8lknjd2aG9VtGt6Lq9h0A5f+ggB3A352TeScL0D5jKWVAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"units\" title=\"\" src=\"/static/fcfca4177c21523518ad7da7dcee2417/c5bb3/units.png\" srcset=\"/static/fcfca4177c21523518ad7da7dcee2417/04472/units.png 170w,\n/static/fcfca4177c21523518ad7da7dcee2417/9f933/units.png 340w,\n/static/fcfca4177c21523518ad7da7dcee2417/c5bb3/units.png 680w,\n/static/fcfca4177c21523518ad7da7dcee2417/b12f7/units.png 1020w,\n/static/fcfca4177c21523518ad7da7dcee2417/b5a09/units.png 1360w,\n/static/fcfca4177c21523518ad7da7dcee2417/2cefc/units.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Templates define infrastructure patterns or even the whole platform.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACD0lEQVR42r1TTWgTQRR+TZqfTVs32VtABCEHqXjqIRIbllzWg3joYdBbwEOgXiQgXgMFvQXxUIuUohcPXRRFpWGTlhoCxcOqUHJpYrvZn8km0dSYlqiXjjPJFpbiScUHH9/Mzrf73vfmLYA7CIz9Ef5PkKofyIswkBI/TbaEaVIVLpDtyEWHZ4keOUMqQ2Z7phthk76jBUffIK5qhweqjz70ODY8gJDXJRxzeBxy9IzqZlTVFztaCwBLcDIiROUp+XPpdBCb5oJpmPl+r3/z8OBwcTAYpJtW82m3272292nvnq5pC5lMJuQUMg75LAfd4iL0158AfCutwr6yAnJegBnwIYQ4y2rOW5Z1u1Kp3NjZqa1ijJ9jbMvF4gYyDGvDtlvPqE6gCS41OvaVW0QLw0EBwWD9DsCPt1fhe1mCXdaPnOe44kQiMcW4Xq+ndN0ofen1JOZA1/UCTbYsSZJg6uaKZhhKtv0+Rhr23Z8Ne8lleZdZDiJRnKQVvWm32tVyoRBlPRNFkTXdW1aUszZufsAmfk0tMz0gInthv8TjVmvO6HSuw/ASWBy9OwXa4yDUHgTuf/0YXjaqwumtPAdylnPzw9525BFLTmR/rLYWiOJXIaC3/5uxoXbJ5uQIamiE470bysQIdP355RQtiDs3dzmaSqXOy7Ls/avRRc5YxePx+WQyWfyXv8UEz/NhtvoFLtQmHqVuYIkAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"infra\" title=\"\" src=\"/static/b2a8795612c0ea58cce80a519382a614/c5bb3/infra.png\" srcset=\"/static/b2a8795612c0ea58cce80a519382a614/04472/infra.png 170w,\n/static/b2a8795612c0ea58cce80a519382a614/9f933/infra.png 340w,\n/static/b2a8795612c0ea58cce80a519382a614/c5bb3/infra.png 680w,\n/static/b2a8795612c0ea58cce80a519382a614/b12f7/infra.png 1020w,\n/static/b2a8795612c0ea58cce80a519382a614/b5a09/infra.png 1360w,\n/static/b2a8795612c0ea58cce80a519382a614/2cefc/infra.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>🔥 <strong>Features</strong></p>\n<ul>\n<li>Common variables, secrets, and templating for different technologies.</li>\n<li>Same GitOps Development experience for Terraform, Shell, Kubernetes.</li>\n<li>Could be used with any Cloud, On-premises or Hybrid scenarios.</li>\n<li>Encourage teams to follow technology best practices.</li>\n</ul>\n<p>📌 <strong>Differences between Cluster.dev and Terraform:</strong></p>\n<p>Cluster.dev improves on Terraform in a few key ways:</p>\n<ul>\n<li><strong>Relations:</strong> Terraform struggles with defining relationships between components, but Cluster.dev makes it easier, allowing you to trigger only what's needed.</li>\n<li><strong>Templating:</strong> Unlike Terraform, Cluster.dev supports templating, making it simpler to include or exclude modules like Jenkins Terraform.</li>\n<li><strong>External Tools:</strong> Terraform has limitations when using external tools, while Cluster.dev provides consistent support and integration for better control.</li>\n</ul>\n<p>Overall, Cluster.dev simplifies infrastructure management and addresses some limitations of Terraform.</p>\n<p>❔ <strong>Why Infrastructure Templating is Essential:</strong></p>\n<p>In modern cloud-native settings, infrastructure templating plays a crucial role in simplifying the processes of developing and maintaining infrastructure. Here's why it's vital:</p>\n<ul>\n<li>Streamlines infrastructure development by establishing clear functional boundaries.</li>\n<li>Facilitates testing the interoperability of components, even if they employ different technologies.</li>\n<li>Enables the sharing of infrastructure patterns among team members.</li>\n<li>Allows the integration of infrastructure into the product, enhancing overall efficiency.</li>\n<li>Facilitates the division of responsibilities between Platform and SRE teams.</li>\n<li>Supports a comprehensive GitOps approach for all infrastructure components.</li>\n</ul>\n<p>Now, let's explore the primary use cases of infrastructure templating and examine the benefits it brings.</p>\n<p>Templating significantly simplifies tasks for DevOps and SRE teams, especially in deploying and testing within complex environments. So, what comprises infrastructure templates? Let's explore the essential building blocks necessary for creating an infrastructure template, starting with infrastructure layering:</p>\n<ul>\n<li><strong>Networking Layer:</strong> Involves VPCs, Peerings, VPNs, Security Groups, and Routing.</li>\n<li><strong>Permissions Layer:</strong> Encompasses IAM roles and user policies.</li>\n<li><strong>Infra and OS Layer:</strong> Encompasses server instance provisioning and operating system settings.</li>\n<li><strong>Data Management Layer:</strong> Includes Relational and NoSQL databases, caches, file and object storages.</li>\n<li><strong>Application Layer:</strong> Involves container orchestration, continuous delivery of applications, business applications and workloads, and infrastructure applications.</li>\n<li><strong>Observability Layer:</strong> Encompasses metric logs, tracing applications, and storages.</li>\n<li><strong>Configuration Layer:</strong> Involves declarative configuration storage, infrastructure state storages, and secret storages.</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABPElEQVR42j2R2Y7jMAwE/f9ft1gMMj51+JAPyWeQOAGS1NIe7AjgCylVd1NRCIHGNYR+RCcGbTSlcRRFQX7pyFLFNE+s60rT1HStJ/7bYKzGmlruKfrKkMbfeGFFB9A5R98O6MyeQKNLyrLEqhZVGOZlZp4X6roWqJO+w3uPLSqUUgytE6FWhGeicRzp+o6hGygSjdKKPM+x1grAEcfJr8ND+Egz+J5t3bC5JU2T09n1emXbNqJDqaoqXN2Sf6sTaFRNlmWkeXHOlmX5iexqWieuY4ko88uf5oyscsPX14VejEWPx4P7/c6+79xvO9M0ymAghJEpeEapdd4IPki0QO8G6Y0y9/Sd53a7yduDsfN8Pone7zefz+e3DlAtro4YbWloZOGulM9JUnQqzi8a19ZU5c++X68Xx/n//h/nir1ySeMgzAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"layers\" title=\"\" src=\"/static/2fac83b032385e56f487120cb1fa55a2/c5bb3/layers.png\" srcset=\"/static/2fac83b032385e56f487120cb1fa55a2/04472/layers.png 170w,\n/static/2fac83b032385e56f487120cb1fa55a2/9f933/layers.png 340w,\n/static/2fac83b032385e56f487120cb1fa55a2/c5bb3/layers.png 680w,\n/static/2fac83b032385e56f487120cb1fa55a2/b12f7/layers.png 1020w,\n/static/2fac83b032385e56f487120cb1fa55a2/2bef9/layers.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>📚 <strong>Conclusion:</strong></p>\n<p>Cluster.dev is a great tool that simplifies the management of cloud-native infrastructures. It uses simple declarative manifests, or stack templates, to enable users to describe and deploy entire infrastructures with a single tool. The platform's support for various technologies, powerful templating engine, GitOps-first approach, and cross-platform compatibility make it a versatile and adaptable solution for managing complex infrastructures. By consolidating previously disconnected infrastructure components under one roof, Cluster.dev enables efficient, one-shot deployment of entire stacks, saving time and effort on operating tasks and allowing teams to focus on code development.</p>\n<p>If you found this article helpful, please don't forget to hit the <strong>Follow</strong> 👉 and <strong>Clap</strong> 👏 buttons to help me write more articles like this.</p>\n<p><strong>Thank You 🖤</strong></p>\n<br>\n<p><strong><em>Until next time, つづく 🎉</em></strong></p>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <strong><em>Until next time 🎉</em></strong></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}