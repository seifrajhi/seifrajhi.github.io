{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/eks-upgrade-guide/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>A Step-by-Step upgrade handbook</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>The <a href=\"https://landscape.cncf.io/\" target=\"_blank\" rel=\"noopener noreferrer\">cloud computing landscape</a> is constantly evolving, and it can be difficult to keep up with the latest changes. However, staying up-to-date is essential for ensuring the security and reliability of your infrastructure.</p>\n<p><a href=\"https://aws.amazon.com/eks/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Kubernetes Service (EKS)</a> is a managed Kubernetes service that makes it easy to deploy, manage, and scale containerized applications.</p>\n<p>One of the most important tasks for EKS cluster administrators is to perform regular upgrades. This ensures that your cluster is running on the latest version of Kubernetes, which includes the latest security patches and features.</p>\n<p>This guide provides a step-by-step walkthrough of the EKS cluster upgrade process. We cover all the essential steps, from planning the upgrade to testing and deploying the new version. Whether you are working with self-managed nodes, managed node groups, Karpenter nodes, or Fargate nodes, we have you covered.\nBy following the guidance in this guide, you can confidently upgrade your EKS cluster without disrupting your applications.</p>\n<p>So let's get started!</p>\n<h2 id=\"-staying-current-how-often-to-upgrade-kubernetes\" style=\"position:relative;\"><a href=\"#-staying-current-how-often-to-upgrade-kubernetes\" aria-label=\" staying current how often to upgrade kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìÖ Staying Current: How Often to Upgrade Kubernetes</h2>\n<p>How frequently should you perform Kubernetes upgrades? This is a crucial aspect often overlooked by newcomers to Kubernetes. Unlike many traditional infrastructure projects, Kubernetes evolves rapidly through its versions. Upgrading Kubernetes cannot be likened to switching to a new long-term support (LTS) release of a Linux distribution; it's a continuous process that demands regular attention.</p>\n<p>To be fair, the Kubernetes team has taken significant steps to make this process more manageable. They follow an <a href=\"https://kubernetes.io/releases/version-skew-policy/\" target=\"_blank\" rel=\"noopener noreferrer\">N-2 support policy</a>, ensuring that the three most recent minor versions receive security fixes and bug patches. This approach gives you ample time to establish a cluster and incorporate upgrade planning into your initial cluster design. Waiting until your cluster is nearly at its end-of-life (EOL) to contemplate upgrades is not a viable strategy. Each release remains eligible for patches for 14 months, which may seem like an extended period, but you're unlikely to install the very latest release.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABJUlEQVR42o2S+0vDMBCA9///L+ovU4ZzPsp0+IQNnTCwsOFGVyuza3K55PLw2s4HatWjhGsuX75LSCuEoJTSWoem8L76giORwt1CXj+KYUEZz7S4muc58w2kY9KHQDZDPZLrAcpbIlEXW7UZEX8WhuAUUvagkwHO+0bNvLfvSxrMrhLywqdUX5xCewuinp3GzlK9cYP5TWg1msnYHHWg2zb3I/e9tRouimIDM+YcozZfsVDubqvjfT2N6+024xf4s9lrTfOZiQ4YM+ORXeehOUpYCIEAnJAQ+vJM7u0AC5PFx0F+gQEAjfHcwHkfTroYTzz//kVuYAkg06WKesVhh15W4d9RwqjU8/CmGF7ly0TyW0PkKyzbqRJVBSdc4Uk+Iyc8EtErSdJ8vXFxmVwAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Kubernetes Versions\" title=\"\" src=\"/static/a1f99354749040eceeca7ad10eeb28aa/c5bb3/k8s-versions.png\" srcset=\"/static/a1f99354749040eceeca7ad10eeb28aa/04472/k8s-versions.png 170w,\n/static/a1f99354749040eceeca7ad10eeb28aa/9f933/k8s-versions.png 340w,\n/static/a1f99354749040eceeca7ad10eeb28aa/c5bb3/k8s-versions.png 680w,\n/static/a1f99354749040eceeca7ad10eeb28aa/5a190/k8s-versions.png 800w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://endoflife.date/kubernetes\">End of Life Date for Kubernetes</a></div>\n<p>So, how frequently should you upgrade Kubernetes? The answer is quite often. Kubernetes aims for three releases per year, down from the previous rate of four releases annually. To assess Kubernetes releases for your organization, you'll likely need to manage multiple versions simultaneously in various environments.</p>\n<p>As a rule of thumb, I recommend letting a minor version bake in a development environment for at least two weeks, and the same applies to the subsequent stages, such as staging or sandbox environments. For production upgrades, ideally, you should have at least a month of solid data indicating that the organization won't encounter issues.</p>\n<p>üîÑ <strong>Key Takeaways:</strong></p>\n<ul>\n<li><strong>N-2 Support Policy:</strong> Ensures the three most recent minor versions receive updates.</li>\n<li><strong>14-Month Patch Eligibility:</strong> Each release is eligible for patches for 14 months.</li>\n<li><strong>Three Releases Per Year:</strong> Kubernetes aims for three releases annually.</li>\n<li><strong>Staging and Production:</strong> Test in development for two weeks and in production for at least a month.</li>\n</ul>\n<h3 id=\"-strategic-kubernetes-upgrade-scheduling\" style=\"position:relative;\"><a href=\"#-strategic-kubernetes-upgrade-scheduling\" aria-label=\" strategic kubernetes upgrade scheduling permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>ü§ñ Strategic Kubernetes Upgrade Scheduling</h3>\n<p>A Kubernetes version encompasses both the control plane and the data plane. To ensure smooth operation, both the control plane and the data plane should run the same Kubernetes <a href=\"https://kubernetes.io/releases/version-skew-policy/#supported-versions\" target=\"_blank\" rel=\"noopener noreferrer\">minor version, such as 1.27</a>.</p>\n<ul>\n<li><strong>Control plane</strong>: The control plane version is defined by the Kubernetes API server. In EKS clusters, this is managed by AWS. Upgrades to the control plane version are started using the AWS API.</li>\n<li><strong>Data plane</strong>: The data plane version references the version of the Kubelet running on your nodes. Different nodes in the same cluster may have different versions. See the version of all nodes with <code class=\"language-text\">kubectl get nodes</code>.</li>\n</ul>\n<h2 id=\"-kubernetes-version-management-a-staged-approach\" style=\"position:relative;\"><a href=\"#-kubernetes-version-management-a-staged-approach\" aria-label=\" kubernetes version management a staged approach permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üíπ Kubernetes Version Management: A Staged Approach</h2>\n<p>In my carefully planned layout:</p>\n<p>‚òëÔ∏è <strong>Development Cluster: Embrace the Bleeding Edge</strong><br>\nThe dev cluster should be on the cutting edge, aligned with the latest Kubernetes version. This approach helps establish SLAs for the dev environment, with a focus on frequent upgrades. The internal communication strategy is to upgrade dev often during specific time frames, relying on it to identify and surface early problems. It's common to encounter critical issues almost immediately, providing ample time for resolution and determining the maximum safe upgrade version on testing day.</p>\n<p>‚òëÔ∏è <strong>Staging Cluster: A Step Behind Dev</strong><br>\nStaging lags slightly behind dev, typically running a minor release older. The concern here might be the possibility of incompatible YAML files. However, it's a prevalent practice to employ per-environment YAMLs, allowing for variations in resource requests and limits. If you're exploring per-environment configuration, consider tools like <a href=\"https://kustomize.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Kustomize</a>.</p>\n<p>‚òëÔ∏è <strong>Production Cluster: Align with Staging</strong><br>\nIn production, the goal is to keep version alignment as close to staging as possible. This approach simplifies developers' lives by avoiding excessive version fragmentation. Kubernetes patch releases are usually conservative with changes, resulting in rare problems. The release cadence for patches on the same minor version follows a two-week cycle in staging before being deployed to production.</p>\n<p>‚òëÔ∏è <strong>Crucial Note: Exercise Caution with Minor Version Upgrades</strong><br>\nA key rule is not to upgrade the minor version until it reaches at least patch <code class=\"language-text\">.2</code>. This means that the latest Kubernetes version, such as 1.26.0, isn't considered ready for a dev release until it reaches 1.26.2. Once this milestone is achieved, the upgrade process begins, progressing from dev to stage and finally to production. By the time the dev upgrade is completed and rolled out to staging, it's often the <code class=\"language-text\">.3</code> release (depending on the time of year).</p>\n<p>‚òëÔ∏è <strong>Balancing Speed and Safety: The Delicate Art of Version Management</strong><br>\nWhile this approach may seem slow, it's rooted in past experiences. Rushing into upgrades too early has led to issues. It's challenging for the Kubernetes team to anticipate every use-case and prevent all regressions. Waiting until at least <code class=\"language-text\">.2</code> ensures extensive testing, with most issues discovered by that point. Some opt to wait until <code class=\"language-text\">.5</code> for the safest path, although it's a slower approach.</p>\n<h3 id=\"-keep-your-cluster-updated\" style=\"position:relative;\"><a href=\"#-keep-your-cluster-updated\" aria-label=\" keep your cluster updated permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Keep Your Cluster Updated</h3>\n<p>Staying current with Kubernetes releases is vital within the shared responsibility model for EKS and Kubernetes adoption.</p>\n<ul>\n<li><strong>Review the EKS Release Calendar</strong>: Frequent updates are the norm, with EKS typically releasing three minor Kubernetes versions annually, each supported for around 14 months. Always check the EKS Kubernetes <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar\" target=\"_blank\" rel=\"noopener noreferrer\">release calendar</a> for the latest information.</li>\n<li><strong>Understand Shared Responsibility</strong>: You're responsible for initiating upgrades for both the cluster control plane and data plane. While AWS handles the control plane during upgrades, the data plane, including Fargate pods and add-ons, falls under your purview. Planning is crucial to ensure workload availability.</li>\n<li><strong>In-Place Cluster Upgrades</strong>: EKS supports in-place cluster upgrades, preserving resources and configuration consistency. This minimizes disruption for users and retains existing workloads and resources. Note that only one minor version upgrade can occur at a time.</li>\n<li><strong>Plan Sequential Upgrades Carefully</strong>: For multiple version updates, sequential upgrades are necessary. However, they pose a higher risk of downtime. Consider evaluating a blue/green cluster upgrade strategy in such cases.</li>\n</ul>\n<h2 id=\"-how-aws-manages-eks-upgrades\" style=\"position:relative;\"><a href=\"#-how-aws-manages-eks-upgrades\" aria-label=\" how aws manages eks upgrades permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ How AWS Manages EKS Upgrades</h2>\n<p>The EKS upgrade process is managed by AWS to ensure a seamless and safe transition between Kubernetes versions. Here is a detailed breakdown of the steps AWS takes to upgrade the EKS control plane:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeklEQVR42o1Sy07DMBDM/38DJzhW3PgDDiBUJC5UokGogijNq3nbjp/Dxn3QUFXKSiuvvOvJzGwCzAzrHPo4BK9TOMMA08NpDtD9eQSnamz8a2qtoZSCMQaazrFrhhLi6wH9+x0c/6EvmSuAk9gDK6VR1zXSLAPvOyjWUGZo4zcUmyVsv6FROwV0B1aqySC7Ej0f0DEBxoW/77oOebGDlhymiogig2MENKbcXZcsqoRyC823sNWK5EQwSnjZJ0u8DxXa8B7N6haGZixJHi2RUvozSNMUTAwwbQotGsiC5LzcQCTPxIZj6jGdQ4G2eEVdLGkv3x6kaRrkee7tCRhjMNbBiRZWcej2E10RQlZrMlxM2fna0HaJDVmgBrLhqMCPufOluIPB7lCfb93tySUf0H0JRWN8GCCIzCh1xpb/wlgLTgtS9JDXBYYugYkf4bIn8rM+/hDzAUcGaZZ7ebqMoFmEbL1AHi7IpvhiPsDs2FOxsoIRW59k9AXgL6PWW2P3QwRpAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"EKS Upgrade Process\" title=\"\" src=\"/static/ddfd8b0f2d3debaabcdfdfb7c15eb884/c5bb3/upgrades.png\" srcset=\"/static/ddfd8b0f2d3debaabcdfdfb7c15eb884/04472/upgrades.png 170w,\n/static/ddfd8b0f2d3debaabcdfdfb7c15eb884/9f933/upgrades.png 340w,\n/static/ddfd8b0f2d3debaabcdfdfb7c15eb884/c5bb3/upgrades.png 680w,\n/static/ddfd8b0f2d3debaabcdfdfb7c15eb884/5a190/upgrades.png 800w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"Ô∏è-pre-upgrade-checks\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-pre-upgrade-checks\" aria-label=\"Ô∏è pre upgrade checks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Pre-upgrade Checks</h3>\n<p>AWS first performs pre-upgrade checks, including assessing the current cluster state and evaluating the compatibility of the new version with your workloads. If any issues are detected, the upgrade process will not proceed.</p>\n<h3 id=\"-backup-and-snapshot\" style=\"position:relative;\"><a href=\"#-backup-and-snapshot\" aria-label=\" backup and snapshot permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üíæ Backup and Snapshot</h3>\n<p>Before initiating the upgrade, AWS takes a backup of your existing control plane and creates a snapshot of your etcd data store. This is done to ensure data consistency and to enable rollback in case of an upgrade failure.</p>\n<p>For additional data protection, consider using <a href=\"https://velero.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Velero</a>, an open-source tool that simplifies the backup and recovery process for Kubernetes cluster resources and persistent volumes. Velero allows you to schedule and manage backups, as well as restore processes, providing an extra layer of safety for your data.</p>\n<h3 id=\"-creating-a-new-control-plane\" style=\"position:relative;\"><a href=\"#-creating-a-new-control-plane\" aria-label=\" creating a new control plane permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üÜï Creating a New Control Plane</h3>\n<p>AWS creates a new control plane with the desired Kubernetes version. This new control plane runs in parallel with your existing control plane, ensuring minimal disruption to your workload.</p>\n<h3 id=\"-testing-compatibility\" style=\"position:relative;\"><a href=\"#-testing-compatibility\" aria-label=\" testing compatibility permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚úÖ Testing Compatibility</h3>\n<p>The new control plane is tested for compatibility with your workloads, including running automated tests to verify that your applications continue to function as expected. The goal is to minimize potential disruptions during the upgrade process and maintain the stability of your services. It's important to mention that this only looks for your application health and not for APIs that may be removed or deprecated.</p>\n<h3 id=\"-switching-control-plane-endpoints\" style=\"position:relative;\"><a href=\"#-switching-control-plane-endpoints\" aria-label=\" switching control plane endpoints permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Switching Control Plane Endpoints</h3>\n<p>Once compatibility is confirmed, AWS switches the control plane endpoints (API server) to the new control plane. This switch happens atomically, resulting in minimal downtime during the upgrade process.</p>\n<h3 id=\"Ô∏è-terminating-the-old-control-plane\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-terminating-the-old-control-plane\" aria-label=\"Ô∏è terminating the old control plane permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóëÔ∏è Terminating the Old Control Plane</h3>\n<p>The old control plane is terminated once the upgrade is complete, and all resources associated with it are cleaned up.</p>\n<h2 id=\"-eks-rollback-on-upgrade-failure\" style=\"position:relative;\"><a href=\"#-eks-rollback-on-upgrade-failure\" aria-label=\" eks rollback on upgrade failure permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîô EKS Rollback on Upgrade Failure</h2>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52.352941176470594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABb0lEQVR42o1Sy07DMBDM/18rwQ1x4iPghlQe4oKoilQkCBJVcNrQOHHi2Im9w9ahbapSxEqrdWzPZGbXEf4ZngiViKFlCmol0OagruIT2rsXbVcMCDmIrutgrQ215bo+dVpAzy6gHk9AasYYf4RwL3pi5xyklBAiRaMrWJWjrRKo5B6r9xv4cnYgItrArRSwZYZKG6jaoNZNuFCWCsvsC8428IVg2SWoeAY4yaS/W15vmWLJuYBTMfzyge28Molmu27XknUYwVznyJ9OmfsluOhcB2NMWEdJ8smqGjj+e6clrLjG8naE5uOKPdc/Ddj1lxoBlY6hFmNQ/cYkHkVRIMuyUCOtNbxniK14aqwon6LOpmhXE/j1txu4CoPzIVtrUBZ5UHZkKDREbgk84ztHrIT6XW+4dxI1O5KrxV+EA2LqyYlrnuc8ZX4qjQlqvY4h70aoJ2f8FpN9AcefzYCeeuINiKoYK+6vnF9yD+cHhN9segqm7sQpQAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"EKS Rollback Process\" title=\"\" src=\"/static/e78540955b703aecd2357c7fc0248b41/c5bb3/rolloback.png\" srcset=\"/static/e78540955b703aecd2357c7fc0248b41/04472/rolloback.png 170w,\n/static/e78540955b703aecd2357c7fc0248b41/9f933/rolloback.png 340w,\n/static/e78540955b703aecd2357c7fc0248b41/c5bb3/rolloback.png 680w,\n/static/e78540955b703aecd2357c7fc0248b41/5a190/rolloback.png 800w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>In case an EKS upgrade fails, AWS has measures in place to minimize disruption and revert the control plane to its previous version:</p>\n<h3 id=\"-detecting-the-failure\" style=\"position:relative;\"><a href=\"#-detecting-the-failure\" aria-label=\" detecting the failure permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö® Detecting the Failure</h3>\n<p>AWS constantly monitors the upgrade process to detect any issues. If a problem arises during the upgrade, the process is immediately halted.</p>\n<h3 id=\"-restoring-from-backup\" style=\"position:relative;\"><a href=\"#-restoring-from-backup\" aria-label=\" restoring from backup permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Restoring from Backup</h3>\n<p>AWS uses the backup and snapshot created before the upgrade to restore the control plane and etcd data store to their previous state.</p>\n<h3 id=\"-switching-control-plane-endpoints-1\" style=\"position:relative;\"><a href=\"#-switching-control-plane-endpoints-1\" aria-label=\" switching control plane endpoints 1 permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Switching Control Plane Endpoints</h3>\n<p>AWS atomically switches the control plane endpoints back to the previous control plane, ensuring minimal downtime.</p>\n<h3 id=\"Ô∏è-terminating-the-new-control-plane\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-terminating-the-new-control-plane\" aria-label=\"Ô∏è terminating the new control plane permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóëÔ∏è Terminating the New Control Plane</h3>\n<p>Once the rollback is complete, AWS terminates the new control plane and cleans up any associated resources.</p>\n<h3 id=\"-post-rollback-assessment\" style=\"position:relative;\"><a href=\"#-post-rollback-assessment\" aria-label=\" post rollback assessment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìù Post-rollback Assessment</h3>\n<p>After the rollback, AWS will assess the reasons behind the upgrade failure and provide guidance on how to address the issues. You will need to troubleshoot and resolve the problems before attempting the upgrade again.</p>\n<h2 id=\"upgrade-your-control-plane-and-data-plane-in-sequence\" style=\"position:relative;\"><a href=\"#upgrade-your-control-plane-and-data-plane-in-sequence\" aria-label=\"upgrade your control plane and data plane in sequence permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Upgrade your control plane and data plane in sequence</h2>\n<p>To upgrade a cluster you will need to take the following actions:\n<a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#use-the-eks-documentation-to-create-an-upgrade-checklist\" target=\"_blank\" rel=\"noopener noreferrer\">Review the Kubernetes and EKS release notes</a>.\n<a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#backup-the-cluster-before-upgrading\" target=\"_blank\" rel=\"noopener noreferrer\">Take a backup of the cluster</a>. (optional)\n<a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#identify-and-remediate-removed-api-usage-before-upgrading-the-control-plane\" target=\"_blank\" rel=\"noopener noreferrer\">Identify and remediate deprecated and removed API usage in your workloads</a>.\n<a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#track-the-version-skew-of-nodes-ensure-managed-node-groups-are-on-the-same-version-as-the-control-plane-before-upgrading\" target=\"_blank\" rel=\"noopener noreferrer\">Ensure Managed Node Groups, if used, are on the same Kubernetes version as the control plane</a>. EKS managed node groups and nodes created by EKS Fargate Profiles only support 1 minor version skew between the control plane and data plane.\n<a href=\"https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html\" target=\"_blank\" rel=\"noopener noreferrer\">Upgrade the cluster control plane using the AWS console or cli</a>.\n<a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#upgrade-add-ons-and-components-using-the-kubernetes-api\" target=\"_blank\" rel=\"noopener noreferrer\">Review add-on compatibility</a>. Upgrade your Kubernetes add-ons and custom controllers, as required.\n<a href=\"https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\" target=\"_blank\" rel=\"noopener noreferrer\">Update kubectl</a>.\n<a href=\"https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html\" target=\"_blank\" rel=\"noopener noreferrer\">Upgrade the cluster data plane</a>. Upgrade your nodes to the same Kubernetes minor version as your upgraded cluster.</p>\n<h2 id=\"-use-the-eks-documentation-to-create-an-upgrade-checklist\" style=\"position:relative;\"><a href=\"#-use-the-eks-documentation-to-create-an-upgrade-checklist\" aria-label=\" use the eks documentation to create an upgrade checklist permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìã Use the EKS Documentation to Create an Upgrade Checklist</h2>\n<p>The EKS Kubernetes version documentation includes a detailed list of changes for each version. Build a checklist for each upgrade.</p>\n<p>For specific EKS version upgrade guidance, review the documentation for notable changes and considerations for each version. The following Kubernetes versions are currently available in Amazon EKS standard support:</p>\n<ul>\n<li><strong>1.30</strong></li>\n<li><strong>1.29</strong></li>\n<li><strong>1.28</strong></li>\n</ul>\n<p>For important changes to be aware of for each version in standard support, see <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html\" target=\"_blank\" rel=\"noopener noreferrer\">Review release notes for Kubernetes versions on standard support</a>.</p>\n<p><strong>üìÖ Available Versions on Extended Support</strong></p>\n<p>The following Kubernetes versions are currently available in Amazon EKS extended support:</p>\n<ul>\n<li><strong>1.27</strong></li>\n<li><strong>1.26</strong></li>\n<li><strong>1.25</strong></li>\n<li><strong>1.24</strong></li>\n<li><strong>1.23</strong></li>\n</ul>\n<h3 id=\"-upgrading-add-ons-and-components-via-the-kubernetes-api\" style=\"position:relative;\"><a href=\"#-upgrading-add-ons-and-components-via-the-kubernetes-api\" aria-label=\" upgrading add ons and components via the kubernetes api permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Upgrading Add-ons and Components via the Kubernetes API</h3>\n<p>Before initiating a cluster upgrade, it's essential to have a comprehensive understanding of the versions of Kubernetes components in use. Conduct an inventory of cluster components, specifically focusing on those that directly interact with the Kubernetes API. These critical cluster components encompass:</p>\n<ul>\n<li><strong>Monitoring and Logging Agents</strong></li>\n<li><strong>Cluster Autoscalers</strong></li>\n<li><strong>Container Storage Drivers</strong> (e.g., <a href=\"https://github.com/kubernetes-sigs/aws-ebs-csi-driver\" target=\"_blank\" rel=\"noopener noreferrer\">EBS CSI</a>, <a href=\"https://github.com/kubernetes-sigs/aws-efs-csi-driver\" target=\"_blank\" rel=\"noopener noreferrer\">EFS CSI</a>)</li>\n<li><strong>Ingress Controllers</strong></li>\n<li><strong>Other Workloads or Add-ons</strong> reliant on direct Kubernetes API interactions</li>\n</ul>\n<h3 id=\"-pro-tip\" style=\"position:relative;\"><a href=\"#-pro-tip\" aria-label=\" pro tip permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí° Pro Tip</h3>\n<p>Critical cluster components are frequently found within namespaces ending in <code class=\"language-text\">*-system</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get ns <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">'-system'</span></code></pre></div>\n<p>Once you've identified components that depend on the Kubernetes API, refer to their documentation to ascertain version compatibility and any prerequisites for upgrading. For instance, consult the <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Load Balancer Controller documentation</a> for insights into version compatibility. Certain components might necessitate updates or configuration adjustments before proceeding with a cluster upgrade. It's imperative to pay special attention to critical components like CoreDNS, kube-proxy, VPC CNI, and storage drivers.</p>\n<p>Clusters typically encompass a multitude of workloads relying on the Kubernetes API, essential for functionalities such as ingress control, continuous delivery systems, and monitoring tools. When embarking on an EKS cluster upgrade, it's equally crucial to upgrade your add-ons and third-party tools, ensuring their seamless compatibility with the upgraded environment.</p>\n<p>See the following examples of common add-ons and their relevant upgrade documentation:</p>\n<ul>\n<li><strong>Amazon VPC CNI</strong>: For the recommended version of the Amazon VPC CNI add-on for each cluster version, see <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html\" target=\"_blank\" rel=\"noopener noreferrer\">Updating the Amazon VPC CNI plugin for Kubernetes self-managed add-on</a>. When installed as an Amazon EKS Add-on, it can only be upgraded one minor version at a time.</li>\n<li><strong>kube-proxy</strong>: See <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html\" target=\"_blank\" rel=\"noopener noreferrer\">Updating the Kubernetes kube-proxy self-managed add-on</a>.</li>\n<li><strong>CoreDNS</strong>: See <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html\" target=\"_blank\" rel=\"noopener noreferrer\">Updating the CoreDNS self-managed add-on</a>.</li>\n<li><strong>AWS Load Balancer Controller</strong>: The AWS Load Balancer Controller needs to be compatible with the EKS version you have deployed. See the <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html\" target=\"_blank\" rel=\"noopener noreferrer\">installation guide</a> for more information.</li>\n<li><strong>Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver</strong>: For installation and upgrade information, see <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html\" target=\"_blank\" rel=\"noopener noreferrer\">Managing the Amazon EBS CSI driver as an Amazon EKS add-on</a>.</li>\n<li><strong>Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver</strong>: For installation and upgrade information, see <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon EFS CSI driver</a>.</li>\n<li><strong>Kubernetes Metrics Server</strong>: For more information, see <a href=\"https://kubernetes-sigs.github.io/metrics-server\" target=\"_blank\" rel=\"noopener noreferrer\">metrics-server on GitHub</a>.</li>\n<li><strong>Kubernetes Cluster Autoscaler</strong>: To upgrade the version of Kubernetes Cluster Autoscaler, change the version of the image in the deployment. The Cluster Autoscaler is tightly coupled with the Kubernetes scheduler. You will always need to upgrade it when you upgrade the cluster. Review the <a href=\"https://github.com/kubernetes/autoscaler/releases\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub releases</a> to find the address of the latest release corresponding to your Kubernetes minor version.</li>\n<li><strong>Karpenter</strong>: For installation and upgrade information, see the <a href=\"https://karpenter.sh/docs/getting-started/\" target=\"_blank\" rel=\"noopener noreferrer\">Karpenter documentation</a>.</li>\n<li><strong>Ingress</strong>: You need to really understand how traffic is coming into the cluster and through what systems.</li>\n<li><strong>Service mesh</strong>: Are you using one, what does it do and what version is it set at? Istio can be a BEAR to upgrade, so if you can switch to Linkerd you'll likely be much happier in the long term. However, understanding what controls access to what namespaces and pods is critical to a happy upgrade.</li>\n<li><strong>Certificates</strong>: By default, they expire after a year. You get fresh ones with every upgrade, but you can also trigger a manual refresh whenever with <code class=\"language-text\">kubeadm certs renew</code>. If you are running an old cluster, PLEASE check the expiration dates of your client certificates with:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubeadm certs check-expiration now</code></pre></div>\n<ul>\n<li><strong>Stateful deployments</strong>: Are they storing something, where are they storing it, and how do you manage them? This would be databases, Redis, message queues, applications that hold state. These are often the hardest to move or interact with during an upgrade. You can review the options for moving those <a href=\"https://www.velotio.com/engineering-blog/exploring-upgrade-strategies-for-stateful-sets-in-kubernetes\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>. The biggest thing is to set the pod disruption budget so that there is some minimum available during the upgrade process as shown <a href=\"https://kubernetes.io/docs/tasks/run-application/configure-pdb/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</li>\n</ul>\n<h3 id=\"-verify-available-ip-addresses\" style=\"position:relative;\"><a href=\"#-verify-available-ip-addresses\" aria-label=\" verify available ip addresses permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîç Verify Available IP Addresses</h3>\n<p>To update the cluster, Amazon EKS requires up to five available IP addresses from the subnets that you specified when you created your cluster. To verify that your subnets have enough IP addresses to upgrade the cluster, you can run the following command:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token assign-left variable\">CLUSTER</span><span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>cluster name<span class=\"token operator\">></span>\naws ec2 describe-subnets --subnet-ids <span class=\"token punctuation\">\\</span>\n    <span class=\"token variable\"><span class=\"token variable\">$(</span>aws eks describe-cluster <span class=\"token parameter variable\">--name</span> $<span class=\"token punctuation\">{</span>CLUSTER<span class=\"token punctuation\">}</span> <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--query</span> <span class=\"token string\">'cluster.resourcesVpcConfig.subnetIds'</span> <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--output</span> text<span class=\"token variable\">)</span></span> <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--query</span> <span class=\"token string\">'Subnets[*].[SubnetId,AvailabilityZone,AvailableIpAddressCount]'</span> <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--output</span> table</code></pre></div>\n<p>The <a href=\"https://github.com/aws/amazon-vpc-cni-k8s/tree/master/cmd/cni-metrics-helper\" target=\"_blank\" rel=\"noopener noreferrer\">VPC CNI Metrics Helper</a> may be used to create a CloudWatch dashboard for VPC metrics.</p>\n<h3 id=\"-transition-to-eks-add-ons\" style=\"position:relative;\"><a href=\"#-transition-to-eks-add-ons\" aria-label=\" transition to eks add ons permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ Transition to EKS Add-ons</h3>\n<p>Amazon EKS seamlessly deploys essential add-ons like the Amazon VPC CNI plugin for Kubernetes, kube-proxy, and CoreDNS for each cluster. These add-ons can either be self-managed or installed via Amazon EKS Add-ons, offering an alternative approach to add-on management through the EKS API.</p>\n<p>With Amazon EKS Add-ons, you gain the convenience of updating versions with a single command. For instance:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws eks update-addon --cluster-name my-cluster --addon-name vpc-cni --addon-version version-number <span class=\"token punctuation\">\\</span>\n--service-account-role-arn arn:aws:iam::111122223333:role/role-name --configuration-values <span class=\"token string\">'{}'</span> --resolve-conflicts PRESERVE</code></pre></div>\n<p>Check if you have any EKS Add-ons with:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws eks list-addons --cluster-name <span class=\"token operator\">&lt;</span>cluster name<span class=\"token operator\">></span></code></pre></div>\n<p>EKS Add-ons are not automatically upgraded during a control plane upgrade. You must initiate EKS add-on updates and select the desired version. You are responsible for selecting a compatible version from all available versions. Review the guidance on <a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#upgrade-add-ons-and-components-using-the-kubernetes-api\" target=\"_blank\" rel=\"noopener noreferrer\">add-on version compatibility</a>.</p>\n<div class=\"warning\">\n    <p><strong>‚ö†Ô∏è Warning:</strong></p>\n    <p>Amazon EKS Add-ons may only be upgraded one minor version at a time.</p>\n</div>\n<h3 id=\"Ô∏è-kube-no-trouble\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kube-no-trouble\" aria-label=\"Ô∏è kube no trouble permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Kube-no-trouble</h3>\n<p><a href=\"https://github.com/doitintl/kube-no-trouble\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>Kube-no-trouble</strong></a> is an open-source command line utility with the command <code class=\"language-text\">kubent</code>. When you run <code class=\"language-text\">kubent</code> without any arguments, it will use your current KubeConfig context, scan the cluster, and print a report with what APIs will be deprecated and removed.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubent</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token number\">4</span>:17PM INF <span class=\"token operator\">>></span><span class=\"token operator\">></span> Kube No Trouble <span class=\"token variable\"><span class=\"token variable\">`</span>kubent<span class=\"token variable\">`</span></span> <span class=\"token operator\">&lt;&lt;&lt;</span>\n<span class=\"token number\">4</span>:17PM INF version <span class=\"token number\">0.7</span>.0 <span class=\"token punctuation\">(</span>git sha d1bb4e5fd6550b533b2013671aa8419d923ee042<span class=\"token punctuation\">)</span>\n<span class=\"token number\">4</span>:17PM INF Initializing collectors and retrieving data\n<span class=\"token number\">4</span>:17PM INF Target K8s version is <span class=\"token number\">1.24</span>.8-eks-ffeb93d\n<span class=\"token number\">4</span>:17PM INF Retrieved <span class=\"token number\">93</span> resources from collector <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>Cluster\n<span class=\"token number\">4</span>:17PM INF Retrieved <span class=\"token number\">16</span> resources from collector <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span><span class=\"token string\">\"Helm v3\"</span>\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>custom.rego.tmpl\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>deprecated-1-16.rego\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>deprecated-1-22.rego\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>deprecated-1-25.rego\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>deprecated-1-26.rego\n<span class=\"token number\">4</span>:17PM INF Loaded ruleset <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span>deprecated-future.rego\n__________________________________________________________________________________________\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> Deprecated APIs removed <span class=\"token keyword\">in</span> <span class=\"token number\">1.25</span> <span class=\"token operator\">&lt;&lt;&lt;</span>\n------------------------------------------------------------------------------------------\nKIND                NAMESPACE     NAME             API_VERSION      REPLACE_WITH <span class=\"token punctuation\">(</span>SINCE<span class=\"token punctuation\">)</span>\nPodSecurityPolicy   <span class=\"token operator\">&lt;</span>undefined<span class=\"token operator\">></span>   eks.privileged   policy/v1beta1   <span class=\"token operator\">&lt;</span>removed<span class=\"token operator\">></span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.21</span>.0<span class=\"token punctuation\">)</span></code></pre></div>\n<p>It can also be used to scan static manifest files and helm packages. It is recommended to run <code class=\"language-text\">kubent</code> as part of a continuous integration (CI) process to identify issues before manifests are deployed. Scanning manifests is also more accurate than scanning live clusters.</p>\n<p>Kube-no-trouble provides a sample <a href=\"https://github.com/doitintl/kube-no-trouble/blob/master/docs/k8s-sa-and-role-example.yaml\" target=\"_blank\" rel=\"noopener noreferrer\">Service Account and Role</a> with the appropriate permissions for scanning the cluster.</p>\n<h3 id=\"Ô∏è-pluto\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-pluto\" aria-label=\"Ô∏è pluto permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Pluto</h3>\n<p>Another option is <a href=\"https://pluto.docs.fairwinds.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>Pluto</strong></a>, which is similar to <code class=\"language-text\">kubent</code> because it supports scanning a live cluster, manifest files, helm charts, and has a <a href=\"https://github.com/FairwindsOps/pluto\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub Action</a> you can include in your CI process.</p>\n<p>See if you can upgrade safely against API paths. I use Pluto. This will check to see if you are calling deprecated or removed API paths in your configuration or helm charts. Run Pluto against local files with:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">pluto detect-files <span class=\"token parameter variable\">-d</span></code></pre></div>\n<p>You can also check Helm with:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">pluto detect-helm <span class=\"token parameter variable\">-o</span> wide</code></pre></div>\n<p>Adding all of this to CI is also pretty trivial and something I recommend for people managing many clusters.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">pluto detect-all-in-cluster</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"plaintext\"><pre class=\"language-plaintext\"><code class=\"language-plaintext\">NAME             KIND                VERSION          REPLACEMENT   REMOVED   DEPRECATED   REPL AVAIL  \neks.privileged   PodSecurityPolicy   policy/v1beta1                 false     true         true</code></pre></div>\n<p>After you have identified what workloads and manifests need to be updated, you may need to change the resource type in your manifest files (e.g., PodSecurityPolicies to PodSecurityStandards). This will require updating the resource specification and additional research depending on what resource is being replaced.</p>\n<p>If the resource type is staying the same but the API version needs to be updated, you can use the <code class=\"language-text\">kubectl-convert</code> command to automatically convert your manifest files. For example, to convert an older Deployment to <code class=\"language-text\">apps/v1</code>. For more information, see <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\" target=\"_blank\" rel=\"noopener noreferrer\">Install kubectl convert plugin</a> on the Kubernetes website.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl-convert <span class=\"token parameter variable\">-f</span> <span class=\"token operator\">&lt;</span>file<span class=\"token operator\">></span> --output-version <span class=\"token operator\">&lt;</span>group<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>version<span class=\"token operator\">></span></code></pre></div>\n<h3 id=\"Ô∏è-nova\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-nova\" aria-label=\"Ô∏è nova permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Nova</h3>\n<p>Check your Helm releases for upgrades. Since typically things like the CNI and other dependencies like CoreDNS are installed with Helm, this is often the fastest way to make sure you are running the latest version (check patch notes to ensure they support the version you are targeting). I use <strong>Nova</strong> for this.</p>\n<h3 id=\"Ô∏è-kubepug\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kubepug\" aria-label=\"Ô∏è kubepug permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è KubePug</h3>\n<p><strong>KubePug/Deprecations</strong> is designed to function as a kubectl plugin with the following capabilities:</p>\n<ul>\n<li><strong>Downloads Deprecation Data</strong>: It fetches a <code class=\"language-text\">data.json</code> file containing deprecation information related to Kubernetes APIs.</li>\n<li><strong>Validation Check</strong>: Verifies the current Kubernetes cluster or input files to determine whether objects exist in deprecated API Versions. This enables users to assess and plan for migration before proceeding.</li>\n</ul>\n<h4 id=\"key-features\" style=\"position:relative;\"><a href=\"#key-features\" aria-label=\"key features permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Key Features</h4>\n<ul>\n<li>Operates against a Kubernetes cluster, utilizing kubeconfig or the active cluster.</li>\n<li>Can be executed against a distinct set of manifests or files.</li>\n<li>Allows users to specify the target Kubernetes version for validation.</li>\n<li>Provides information on the replacement API that should be adopted.</li>\n<li>Offers details about the version in which the API was deprecated or removed, based on the target cluster version.</li>\n</ul>\n<h4 id=\"how-to-install-as-a-krew-plugin\" style=\"position:relative;\"><a href=\"#how-to-install-as-a-krew-plugin\" aria-label=\"how to install as a krew plugin permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How to Install as a Krew Plugin</h4>\n<p>Simply run the following command to install it as a Krew plugin:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl krew <span class=\"token function\">install</span> deprecations</code></pre></div>\n<h3 id=\"Ô∏è-eksup-cluster-upgrade-preparation-assistant\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-eksup-cluster-upgrade-preparation-assistant\" aria-label=\"Ô∏è eksup cluster upgrade preparation assistant permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è eksup: Cluster Upgrade Preparation Assistant</h3>\n<p><strong>eksup</strong> is a command-line interface (CLI) designed to empower users with comprehensive information and tools for preparing their clusters for an upgrade. It streamlines the upgrade process by delivering critical insights and actions for a seamless transition.</p>\n<h4 id=\"key-functions\" style=\"position:relative;\"><a href=\"#key-functions\" aria-label=\"key functions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Key Functions</h4>\n<ul>\n<li><strong>Cluster Analysis</strong>: eksup allows users to assess their clusters against the next Kubernetes version, identifying any potential issues that could impact the upgrade process.</li>\n<li><strong>Playbook Generation</strong>: Users can generate customized playbooks that outline the upgrade steps specific to their cluster's analysis results. These playbooks detail the necessary actions and remediations.</li>\n<li><strong>Flexibility and Learning</strong>: The generated playbook is editable, enabling users to adapt the upgrade steps to align with their cluster configurations and business needs. It also provides a platform to document insights gained during the upgrade process.</li>\n<li><strong>Enhanced Collaboration</strong>: As upgrades are often initiated on non-production clusters first, any additional steps or insights discovered during this phase can be captured and utilized to enhance the upgrade process for production clusters.</li>\n<li><strong>Historical Artifact</strong>: Users are encouraged to preserve their playbooks as historical references. This practice ensures that each upgrade cycle benefits from a deeper understanding of the process, instilling confidence and efficiency in future upgrades before Kubernetes version support expires.</li>\n</ul>\n<p><strong>High Level¬†Diagram</strong></p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 203.5294117647059%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,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'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Alt text\" title=\"\" src=\"/static/2bcf04daf28b5bc0a01a97056b293483/c5bb3/eksup.png\" srcset=\"/static/2bcf04daf28b5bc0a01a97056b293483/04472/eksup.png 170w,\n/static/2bcf04daf28b5bc0a01a97056b293483/9f933/eksup.png 340w,\n/static/2bcf04daf28b5bc0a01a97056b293483/c5bb3/eksup.png 680w,\n/static/2bcf04daf28b5bc0a01a97056b293483/5a190/eksup.png 800w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-gonogo\" style=\"position:relative;\"><a href=\"#-gonogo\" aria-label=\" gonogo permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö¶ GoNoGo</h3>\n<p><a href=\"https://github.com/FairwindsOps/GoNoGo\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>GoNoGo</strong></a> is an alpha-stage tool to determine the upgrade confidence of your cluster add-ons.</p>\n<h3 id=\"Ô∏è-configure-poddisruptionbudgets-and-topologyspreadconstraints-during-data-plane-upgrade\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-configure-poddisruptionbudgets-and-topologyspreadconstraints-during-data-plane-upgrade\" aria-label=\"Ô∏è configure poddisruptionbudgets and topologyspreadconstraints during data plane upgrade permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ°Ô∏è Configure PodDisruptionBudgets and TopologySpreadConstraints During Data Plane Upgrade</h3>\n<p>To safeguard the availability of your workloads during a data plane upgrade, it's crucial to configure <strong>PodDisruptionBudgets</strong> and <strong>topologySpreadConstraints</strong> appropriately. Remember that not all workloads demand the same level of availability. Thus, it's imperative to assess your workload's scale and requirements.</p>\n<p>Ensuring that workloads are distributed across multiple Availability Zones and hosts with topology spreads enhances the confidence that migrations to the new data plane will occur seamlessly and without disruptions.</p>\n<p>Here's an illustrative example of a workload configuration that guarantees 80% of replicas are consistently available and efficiently spreads replicas across zones and hosts:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> policy/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PodDisruptionBudget\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> myapp\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">minAvailable</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"80%\"</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> myapp\n<span class=\"token punctuation\">---</span>\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> myapp\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> myapp\n    <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> myapp\n        <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> public.ecr.aws/eks<span class=\"token punctuation\">-</span>distro/kubernetes/pause<span class=\"token punctuation\">:</span><span class=\"token number\">3.2</span>\n                <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> myapp\n                <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1\"</span>\n                        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 256M\n            <span class=\"token key atrule\">topologySpreadConstraints</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> host<span class=\"token punctuation\">-</span>zone<span class=\"token punctuation\">-</span>spread\n                <span class=\"token key atrule\">maxSkew</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n                <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> kubernetes.io/hostname\n                <span class=\"token key atrule\">whenUnsatisfiable</span><span class=\"token punctuation\">:</span> DoNotSchedule\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> host<span class=\"token punctuation\">-</span>zone<span class=\"token punctuation\">-</span>spread\n                <span class=\"token key atrule\">maxSkew</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n                <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> topology.kubernetes.io/zone\n                <span class=\"token key atrule\">whenUnsatisfiable</span><span class=\"token punctuation\">:</span> DoNotSchedule</code></pre></div>\n<h3 id=\"-aws-resilience-hub\" style=\"position:relative;\"><a href=\"#-aws-resilience-hub\" aria-label=\" aws resilience hub permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåê AWS Resilience Hub</h3>\n<p><a href=\"https://aws.amazon.com/resilience-hub/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Resilience Hub</a> has added Amazon Elastic Kubernetes Service (Amazon EKS) as a supported resource. Resilience Hub provides a single place to define, validate, and track the resilience of your applications so that you can avoid unnecessary downtime caused by software, infrastructure, or operational disruptions.</p>\n<h3 id=\"Ô∏è-use-managed-node-groups-or-karpenter-to-simplify-data-plane-upgrades\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-use-managed-node-groups-or-karpenter-to-simplify-data-plane-upgrades\" aria-label=\"Ô∏è use managed node groups or karpenter to simplify data plane upgrades permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Use Managed Node Groups or Karpenter to Simplify Data Plane Upgrades</h3>\n<p>Managed Node Groups and Karpenter both simplify node upgrades, but they take different approaches.</p>\n<ul>\n<li><strong>Managed Node Groups</strong>: Automate the provisioning and lifecycle management of nodes. This means that you can create, automatically update, or terminate nodes with a single operation.</li>\n<li><strong>Karpenter</strong>: Automatically creates new nodes using the latest compatible EKS Optimized AMI. As EKS releases updated EKS Optimized AMIs or the cluster is upgraded, Karpenter will automatically start using these images. <a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/#enable-node-expiry-for-karpenter-managed-nodes\" target=\"_blank\" rel=\"noopener noreferrer\">Karpenter also implements Node Expiry to update nodes</a>. <a href=\"https://karpenter.sh/docs/concepts/node-templates/\" target=\"_blank\" rel=\"noopener noreferrer\">Karpenter can be configured to use custom AMIs</a>. If you use custom AMIs with Karpenter, you are responsible for the version of kubelet.</li>\n</ul>\n<h3 id=\"-use-code-classlanguage-texteksctlcode-to-automate-upgrades-for-self-managed-node-groups\" style=\"position:relative;\"><a href=\"#-use-code-classlanguage-texteksctlcode-to-automate-upgrades-for-self-managed-node-groups\" aria-label=\" use code classlanguage texteksctlcode to automate upgrades for self managed node groups permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>ü§ñ Use <code class=\"language-text\">eksctl</code> to Automate Upgrades for Self-Managed Node Groups</h3>\n<p>Self-managed node groups are EC2 instances that were deployed in your account and attached to the cluster outside of the EKS service. These are usually deployed and managed by some form of automation tooling. To upgrade self-managed node groups you should refer to your tools documentation.</p>\n<p>For example, <code class=\"language-text\">eksctl</code> supports <a href=\"https://eksctl.io/usage/managing-nodegroups/#deleting-and-draining\" target=\"_blank\" rel=\"noopener noreferrer\">deleting and draining self-managed nodes</a>. Some common tools include:</p>\n<ul>\n<li><a href=\"https://eksctl.io/usage/nodegroup-upgrade/\" target=\"_blank\" rel=\"noopener noreferrer\">eksctl</a></li>\n<li><a href=\"https://kops.sigs.k8s.io/operations/updates_and_upgrades/\" target=\"_blank\" rel=\"noopener noreferrer\">kOps</a></li>\n<li><a href=\"https://aws-ia.github.io/terraform-aws-eks-blueprints/node-groups/#self-managed-node-groups\" target=\"_blank\" rel=\"noopener noreferrer\">EKS Blueprints</a></li>\n</ul>\n<h3 id=\"-backup-the-cluster-before-upgrading\" style=\"position:relative;\"><a href=\"#-backup-the-cluster-before-upgrading\" aria-label=\" backup the cluster before upgrading permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üíæ Backup the Cluster Before Upgrading</h3>\n<p>New versions of Kubernetes introduce significant changes to your Amazon EKS cluster. After you upgrade a cluster, you can't downgrade it.</p>\n<p><a href=\"https://velero.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Velero</a> is a community-supported open-source tool that can be used to take backups of existing clusters and apply the backups to a new cluster.</p>\n<p>Note that you can only create new clusters for Kubernetes versions currently supported by EKS. If the version your cluster is currently running is still supported and an upgrade fails, you can create a new cluster with the original version and restore the data plane. Note that AWS resources, including IAM, are not included in the backup by Velero. These resources would need to be recreated.</p>\n<h3 id=\"-anticipate-major-kubernetes-changes-stay-ahead-of-the-curve\" style=\"position:relative;\"><a href=\"#-anticipate-major-kubernetes-changes-stay-ahead-of-the-curve\" aria-label=\" anticipate major kubernetes changes stay ahead of the curve permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Anticipate Major Kubernetes Changes‚Ää-‚ÄäStay Ahead of the Curve</h3>\n<p>Rather than solely focusing on the immediate next version of Kubernetes, adopt a forward-thinking approach. Continuously monitor new Kubernetes releases and be vigilant in identifying significant alterations. For instance, certain applications directly interfaced with the Docker API, and Kubernetes 1.24 made a pivotal change by removing support for Container Runtime Interface (CRI) for Docker, commonly known as Dockershim. üê≥ Preparing for such substantial changes demands additional time and planning.</p>\n<p>Examine all documented modifications for the version to which you plan to upgrade, meticulously noting any mandatory upgrade procedures. Additionally, pay attention to specific requirements or processes tailored to Amazon EKS managed clusters. This proactive stance ensures a smoother upgrade process while mitigating potential disruptions caused by unforeseen changes.</p>\n<p>Always keep an eye on <a href=\"https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes CHANGLELOG</a></p>\n<h3 id=\"-important-guidance-on-feature-removals\" style=\"position:relative;\"><a href=\"#-important-guidance-on-feature-removals\" aria-label=\" important guidance on feature removals permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö¢ Important Guidance on Feature Removals</h3>\n<h4 id=\"Ô∏è-removal-of-dockershim-in-125-transition-to-detector-for-docker-socket-dds\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-removal-of-dockershim-in-125-transition-to-detector-for-docker-socket-dds\" aria-label=\"Ô∏è removal of dockershim in 125 transition to detector for docker socket dds permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ñ∂Ô∏è Removal of Dockershim in 1.25‚Ää-‚ÄäTransition to Detector for Docker Socket (DDS)</h4>\n<p>In Kubernetes 1.25, Dockershim support has been discontinued, particularly in the EKS Optimized AMI for 1.25. If your applications rely on Dockershim, for instance, by mounting the Docker socket, it's imperative to eliminate these dependencies before proceeding with the upgrade of your worker nodes to version 1.25. This ensures a seamless transition without any disruptions caused by the removal of Dockershim.</p>\n<p>Find instances where you have a dependency on the Docker socket before upgrading to 1.25. We recommend using <a href=\"https://github.com/aws-containers/kubectl-detector-for-docker-socket\" target=\"_blank\" rel=\"noopener noreferrer\">Detector for Docker Socket (DDS)</a>, a kubectl plugin.</p>\n<h4 id=\"Ô∏è-removal-of-podsecuritypolicy-in-125-migration-to-pod-security-standards-or-policy-as-code-solution\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-removal-of-podsecuritypolicy-in-125-migration-to-pod-security-standards-or-policy-as-code-solution\" aria-label=\"Ô∏è removal of podsecuritypolicy in 125 migration to pod security standards or policy as code solution permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ñ∂Ô∏è Removal of PodSecurityPolicy in 1.25‚Ää-‚ÄäMigration to Pod Security Standards or Policy-as-Code Solution</h4>\n<p>PodSecurityPolicy, deprecated in Kubernetes 1.21, has been entirely removed in Kubernetes 1.25. If your cluster currently utilizes PodSecurityPolicy, it's paramount to initiate a migration process before upgrading your cluster to version 1.25. This migration should involve transitioning to the native Kubernetes Pod Security Standards (PSS) or implementing a policy-as-code solution. This proactive step is crucial for maintaining the uninterrupted functionality of your workloads during the upgrade.</p>\n<ul>\n<li>Review the <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-standards/\" target=\"_blank\" rel=\"noopener noreferrer\">Pod Security Standards (PSS)</a> and <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\" target=\"_blank\" rel=\"noopener noreferrer\">Pod Security Admission (PSA)</a> best practices.</li>\n<li>Review the <a href=\"https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/\" target=\"_blank\" rel=\"noopener noreferrer\">PodSecurityPolicy Deprecation blog post</a> on the Kubernetes website.</li>\n</ul>\n<h4 id=\"Ô∏è-deprecation-of-in-tree-storage-driver-in-123-migrate-to-container-storage-interface-csi-drivers\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-deprecation-of-in-tree-storage-driver-in-123-migrate-to-container-storage-interface-csi-drivers\" aria-label=\"Ô∏è deprecation of in tree storage driver in 123 migrate to container storage interface csi drivers permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ñ∂Ô∏è Deprecation of In-Tree Storage Driver in 1.23‚Ää-‚ÄäMigrate to Container Storage Interface (CSI) Drivers</h4>\n<p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms. The Amazon EBS container storage interface (CSI) migration feature is enabled by default in Amazon EKS 1.23 and later clusters. If you have pods running on a version 1.22 or earlier cluster, then you must install the Amazon EBS CSI driver before updating your cluster to version 1.23 to avoid service interruption.</p>\n<ul>\n<li>Review the <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon EBS CSI migration frequently asked questions</a>.</li>\n</ul>\n<h3 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h3>\n<p>I trust that the information provided has been valuable to you. The journey of keeping Kubernetes consistently upgraded becomes less daunting with regular practice. The key takeaway is to allocate ample time to acclimate your environment with each minor release. By adhering to a predictable schedule, the process of upgrading clusters becomes remarkably painless and straightforward, even for less experienced users, as long as you perform the necessary checks.</p>\n<p>Remember, the key to success lies in regularity and thorough preparation when it comes to Kubernetes upgrades.</p>\n<p>We hope that you have found this blog post helpful. If you have any other tips or tricks that you would like to share, please leave a comment below.</p>\n<h2 id=\"-conclusion-1\" style=\"position:relative;\"><a href=\"#-conclusion-1\" aria-label=\" conclusion 1 permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h2>\n<p>I trust that the information provided has been valuable to you. The journey of keeping Kubernetes consistently upgraded becomes less daunting with regular practice. The key takeaway is to allocate ample time to acclimate your environment with each minor release. By adhering to a predictable schedule, the process of upgrading clusters becomes remarkably painless and straightforward, even for less experienced users, as long as you perform the necessary checks.</p>\n<p>Remember, the key to success lies in regularity and thorough preparation when it comes to Kubernetes upgrades.</p>\n<p>We hope that you have found this blog post helpful. If you have any other tips or tricks that you would like to share, please leave a comment below.</p>\n<p><strong>üìï References:</strong></p>\n<ul>\n<li><a href=\"https://repost.aws/knowledge-center/eks-plan-upgrade-cluster\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Knowledge Center: Plan an EKS Cluster Upgrade</a></li>\n<li><a href=\"https://aws.github.io/aws-eks-best-practices/upgrades/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS EKS Best Practices: Upgrades</a></li>\n</ul>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <em><strong>Until next time üéâ</strong></em></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":20,"rawMarkdownBody":"\n> **A Step-by-Step upgrade handbook**\n\n## üìö Introduction\n\nThe [cloud computing landscape](https://landscape.cncf.io/) is constantly evolving, and it can be difficult to keep up with the latest changes. However, staying up-to-date is essential for ensuring the security and reliability of your infrastructure.\n\n[Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/) is a managed Kubernetes service that makes it easy to deploy, manage, and scale containerized applications.\n\nOne of the most important tasks for EKS cluster administrators is to perform regular upgrades. This ensures that your cluster is running on the latest version of Kubernetes, which includes the latest security patches and features.\n\nThis guide provides a step-by-step walkthrough of the EKS cluster upgrade process. We cover all the essential steps, from planning the upgrade to testing and deploying the new version. Whether you are working with self-managed nodes, managed node groups, Karpenter nodes, or Fargate nodes, we have you covered.\nBy following the guidance in this guide, you can confidently upgrade your EKS cluster without disrupting your applications.\n\nSo let's get started!\n\n## üìÖ Staying Current: How Often to Upgrade Kubernetes\n\nHow frequently should you perform Kubernetes upgrades? This is a crucial aspect often overlooked by newcomers to Kubernetes. Unlike many traditional infrastructure projects, Kubernetes evolves rapidly through its versions. Upgrading Kubernetes cannot be likened to switching to a new long-term support (LTS) release of a Linux distribution; it's a continuous process that demands regular attention.\n\nTo be fair, the Kubernetes team has taken significant steps to make this process more manageable. They follow an [N-2 support policy](https://kubernetes.io/releases/version-skew-policy/), ensuring that the three most recent minor versions receive security fixes and bug patches. This approach gives you ample time to establish a cluster and incorporate upgrade planning into your initial cluster design. Waiting until your cluster is nearly at its end-of-life (EOL) to contemplate upgrades is not a viable strategy. Each release remains eligible for patches for 14 months, which may seem like an extended period, but you're unlikely to install the very latest release.\n\n![Kubernetes Versions](./k8s-versions.png)\n<div class=\"image-title\"><a href=\"https://endoflife.date/kubernetes\">End of Life Date for Kubernetes</a></div>\n\nSo, how frequently should you upgrade Kubernetes? The answer is quite often. Kubernetes aims for three releases per year, down from the previous rate of four releases annually. To assess Kubernetes releases for your organization, you'll likely need to manage multiple versions simultaneously in various environments.\n\nAs a rule of thumb, I recommend letting a minor version bake in a development environment for at least two weeks, and the same applies to the subsequent stages, such as staging or sandbox environments. For production upgrades, ideally, you should have at least a month of solid data indicating that the organization won't encounter issues.\n\nüîÑ **Key Takeaways:**\n- **N-2 Support Policy:** Ensures the three most recent minor versions receive updates.\n- **14-Month Patch Eligibility:** Each release is eligible for patches for 14 months.\n- **Three Releases Per Year:** Kubernetes aims for three releases annually.\n- **Staging and Production:** Test in development for two weeks and in production for at least a month.\n  \n### ü§ñ Strategic Kubernetes Upgrade Scheduling\n\nA Kubernetes version encompasses both the control plane and the data plane. To ensure smooth operation, both the control plane and the data plane should run the same Kubernetes [minor version, such as 1.27](https://kubernetes.io/releases/version-skew-policy/#supported-versions).\n\n- **Control plane**: The control plane version is defined by the Kubernetes API server. In EKS clusters, this is managed by AWS. Upgrades to the control plane version are started using the AWS API.\n- **Data plane**: The data plane version references the version of the Kubelet running on your nodes. Different nodes in the same cluster may have different versions. See the version of all nodes with `kubectl get nodes`.\n\n## üíπ Kubernetes Version Management: A Staged Approach\n\nIn my carefully planned layout:\n\n‚òëÔ∏è **Development Cluster: Embrace the Bleeding Edge**  \nThe dev cluster should be on the cutting edge, aligned with the latest Kubernetes version. This approach helps establish SLAs for the dev environment, with a focus on frequent upgrades. The internal communication strategy is to upgrade dev often during specific time frames, relying on it to identify and surface early problems. It's common to encounter critical issues almost immediately, providing ample time for resolution and determining the maximum safe upgrade version on testing day.\n\n‚òëÔ∏è **Staging Cluster: A Step Behind Dev**  \nStaging lags slightly behind dev, typically running a minor release older. The concern here might be the possibility of incompatible YAML files. However, it's a prevalent practice to employ per-environment YAMLs, allowing for variations in resource requests and limits. If you're exploring per-environment configuration, consider tools like [Kustomize](https://kustomize.io/).\n\n‚òëÔ∏è **Production Cluster: Align with Staging**  \nIn production, the goal is to keep version alignment as close to staging as possible. This approach simplifies developers' lives by avoiding excessive version fragmentation. Kubernetes patch releases are usually conservative with changes, resulting in rare problems. The release cadence for patches on the same minor version follows a two-week cycle in staging before being deployed to production.\n\n‚òëÔ∏è **Crucial Note: Exercise Caution with Minor Version Upgrades**  \nA key rule is not to upgrade the minor version until it reaches at least patch `.2`. This means that the latest Kubernetes version, such as 1.26.0, isn't considered ready for a dev release until it reaches 1.26.2. Once this milestone is achieved, the upgrade process begins, progressing from dev to stage and finally to production. By the time the dev upgrade is completed and rolled out to staging, it's often the `.3` release (depending on the time of year).\n\n‚òëÔ∏è **Balancing Speed and Safety: The Delicate Art of Version Management**  \nWhile this approach may seem slow, it's rooted in past experiences. Rushing into upgrades too early has led to issues. It's challenging for the Kubernetes team to anticipate every use-case and prevent all regressions. Waiting until at least `.2` ensures extensive testing, with most issues discovered by that point. Some opt to wait until `.5` for the safest path, although it's a slower approach.\n\n### üîÑ Keep Your Cluster Updated\n\nStaying current with Kubernetes releases is vital within the shared responsibility model for EKS and Kubernetes adoption.\n\n- **Review the EKS Release Calendar**: Frequent updates are the norm, with EKS typically releasing three minor Kubernetes versions annually, each supported for around 14 months. Always check the EKS Kubernetes [release calendar](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar) for the latest information.\n- **Understand Shared Responsibility**: You're responsible for initiating upgrades for both the cluster control plane and data plane. While AWS handles the control plane during upgrades, the data plane, including Fargate pods and add-ons, falls under your purview. Planning is crucial to ensure workload availability.\n- **In-Place Cluster Upgrades**: EKS supports in-place cluster upgrades, preserving resources and configuration consistency. This minimizes disruption for users and retains existing workloads and resources. Note that only one minor version upgrade can occur at a time.\n- **Plan Sequential Upgrades Carefully**: For multiple version updates, sequential upgrades are necessary. However, they pose a higher risk of downtime. Consider evaluating a blue/green cluster upgrade strategy in such cases.\n\n## üîÑ How AWS Manages EKS Upgrades\n\nThe EKS upgrade process is managed by AWS to ensure a seamless and safe transition between Kubernetes versions. Here is a detailed breakdown of the steps AWS takes to upgrade the EKS control plane:\n\n![EKS Upgrade Process](./upgrades.png)\n\n### üõ†Ô∏è Pre-upgrade Checks\nAWS first performs pre-upgrade checks, including assessing the current cluster state and evaluating the compatibility of the new version with your workloads. If any issues are detected, the upgrade process will not proceed.\n\n### üíæ Backup and Snapshot\nBefore initiating the upgrade, AWS takes a backup of your existing control plane and creates a snapshot of your etcd data store. This is done to ensure data consistency and to enable rollback in case of an upgrade failure.\n\nFor additional data protection, consider using [Velero](https://velero.io/), an open-source tool that simplifies the backup and recovery process for Kubernetes cluster resources and persistent volumes. Velero allows you to schedule and manage backups, as well as restore processes, providing an extra layer of safety for your data.\n\n### üÜï Creating a New Control Plane\nAWS creates a new control plane with the desired Kubernetes version. This new control plane runs in parallel with your existing control plane, ensuring minimal disruption to your workload.\n\n### ‚úÖ Testing Compatibility\nThe new control plane is tested for compatibility with your workloads, including running automated tests to verify that your applications continue to function as expected. The goal is to minimize potential disruptions during the upgrade process and maintain the stability of your services. It's important to mention that this only looks for your application health and not for APIs that may be removed or deprecated.\n\n### üîÑ Switching Control Plane Endpoints\nOnce compatibility is confirmed, AWS switches the control plane endpoints (API server) to the new control plane. This switch happens atomically, resulting in minimal downtime during the upgrade process.\n\n### üóëÔ∏è Terminating the Old Control Plane\nThe old control plane is terminated once the upgrade is complete, and all resources associated with it are cleaned up.\n\n## üîô EKS Rollback on Upgrade Failure\n\n![EKS Rollback Process](./rolloback.png)\n\nIn case an EKS upgrade fails, AWS has measures in place to minimize disruption and revert the control plane to its previous version:\n\n### üö® Detecting the Failure\nAWS constantly monitors the upgrade process to detect any issues. If a problem arises during the upgrade, the process is immediately halted.\n\n### üîÑ Restoring from Backup\nAWS uses the backup and snapshot created before the upgrade to restore the control plane and etcd data store to their previous state.\n\n### üîÑ Switching Control Plane Endpoints\nAWS atomically switches the control plane endpoints back to the previous control plane, ensuring minimal downtime.\n\n### üóëÔ∏è Terminating the New Control Plane\nOnce the rollback is complete, AWS terminates the new control plane and cleans up any associated resources.\n\n### üìù Post-rollback Assessment\nAfter the rollback, AWS will assess the reasons behind the upgrade failure and provide guidance on how to address the issues. You will need to troubleshoot and resolve the problems before attempting the upgrade again.\n\n## Upgrade your control plane and data plane in sequence\n\nTo upgrade a cluster you will need to take the following actions:\n[Review the Kubernetes and EKS release notes](https://aws.github.io/aws-eks-best-practices/upgrades/#use-the-eks-documentation-to-create-an-upgrade-checklist).\n[Take a backup of the cluster](https://aws.github.io/aws-eks-best-practices/upgrades/#backup-the-cluster-before-upgrading). (optional)\n[Identify and remediate deprecated and removed API usage in your workloads](https://aws.github.io/aws-eks-best-practices/upgrades/#identify-and-remediate-removed-api-usage-before-upgrading-the-control-plane).\n[Ensure Managed Node Groups, if used, are on the same Kubernetes version as the control plane](https://aws.github.io/aws-eks-best-practices/upgrades/#track-the-version-skew-of-nodes-ensure-managed-node-groups-are-on-the-same-version-as-the-control-plane-before-upgrading). EKS managed node groups and nodes created by EKS Fargate Profiles only support 1 minor version skew between the control plane and data plane.\n[Upgrade the cluster control plane using the AWS console or cli](https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html).\n[Review add-on compatibility](https://aws.github.io/aws-eks-best-practices/upgrades/#upgrade-add-ons-and-components-using-the-kubernetes-api). Upgrade your Kubernetes add-ons and custom controllers, as required.\n[Update kubectl](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html).\n[Upgrade the cluster data plane](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html). Upgrade your nodes to the same Kubernetes minor version as your upgraded cluster.\n\n## üìã Use the EKS Documentation to Create an Upgrade Checklist\n\nThe EKS Kubernetes version documentation includes a detailed list of changes for each version. Build a checklist for each upgrade.\n\nFor specific EKS version upgrade guidance, review the documentation for notable changes and considerations for each version. The following Kubernetes versions are currently available in Amazon EKS standard support:\n\n- **1.30**\n- **1.29**\n- **1.28**\n\nFor important changes to be aware of for each version in standard support, see [Review release notes for Kubernetes versions on standard support](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html).\n\n**üìÖ Available Versions on Extended Support**\n\nThe following Kubernetes versions are currently available in Amazon EKS extended support:\n\n- **1.27**\n- **1.26**\n- **1.25**\n- **1.24**\n- **1.23**\n\n### üîÑ Upgrading Add-ons and Components via the Kubernetes API\n\nBefore initiating a cluster upgrade, it's essential to have a comprehensive understanding of the versions of Kubernetes components in use. Conduct an inventory of cluster components, specifically focusing on those that directly interact with the Kubernetes API. These critical cluster components encompass:\n\n- **Monitoring and Logging Agents**\n- **Cluster Autoscalers**\n- **Container Storage Drivers** (e.g., [EBS CSI](https://github.com/kubernetes-sigs/aws-ebs-csi-driver), [EFS CSI](https://github.com/kubernetes-sigs/aws-efs-csi-driver))\n- **Ingress Controllers**\n- **Other Workloads or Add-ons** reliant on direct Kubernetes API interactions\n\n### üí° Pro Tip\n\nCritical cluster components are frequently found within namespaces ending in `*-system`:\n\n```shell\nkubectl get ns | grep '-system'\n```\n\nOnce you've identified components that depend on the Kubernetes API, refer to their documentation to ascertain version compatibility and any prerequisites for upgrading. For instance, consult the [AWS Load Balancer Controller documentation](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html) for insights into version compatibility. Certain components might necessitate updates or configuration adjustments before proceeding with a cluster upgrade. It's imperative to pay special attention to critical components like CoreDNS, kube-proxy, VPC CNI, and storage drivers.\n\nClusters typically encompass a multitude of workloads relying on the Kubernetes API, essential for functionalities such as ingress control, continuous delivery systems, and monitoring tools. When embarking on an EKS cluster upgrade, it's equally crucial to upgrade your add-ons and third-party tools, ensuring their seamless compatibility with the upgraded environment.\n\nSee the following examples of common add-ons and their relevant upgrade documentation:\n\n- **Amazon VPC CNI**: For the recommended version of the Amazon VPC CNI add-on for each cluster version, see [Updating the Amazon VPC CNI plugin for Kubernetes self-managed add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html). When installed as an Amazon EKS Add-on, it can only be upgraded one minor version at a time.\n- **kube-proxy**: See [Updating the Kubernetes kube-proxy self-managed add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html).\n- **CoreDNS**: See [Updating the CoreDNS self-managed add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html).\n- **AWS Load Balancer Controller**: The AWS Load Balancer Controller needs to be compatible with the EKS version you have deployed. See the [installation guide](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html) for more information.\n- **Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver**: For installation and upgrade information, see [Managing the Amazon EBS CSI driver as an Amazon EKS add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html).\n- **Amazon Elastic File System (Amazon EFS) Container Storage Interface (CSI) driver**: For installation and upgrade information, see [Amazon EFS CSI driver](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html).\n- **Kubernetes Metrics Server**: For more information, see [metrics-server on GitHub](https://kubernetes-sigs.github.io/metrics-server).\n- **Kubernetes Cluster Autoscaler**: To upgrade the version of Kubernetes Cluster Autoscaler, change the version of the image in the deployment. The Cluster Autoscaler is tightly coupled with the Kubernetes scheduler. You will always need to upgrade it when you upgrade the cluster. Review the [GitHub releases](https://github.com/kubernetes/autoscaler/releases) to find the address of the latest release corresponding to your Kubernetes minor version.\n- **Karpenter**: For installation and upgrade information, see the [Karpenter documentation](https://karpenter.sh/docs/getting-started/).\n- **Ingress**: You need to really understand how traffic is coming into the cluster and through what systems.\n- **Service mesh**: Are you using one, what does it do and what version is it set at? Istio can be a BEAR to upgrade, so if you can switch to Linkerd you'll likely be much happier in the long term. However, understanding what controls access to what namespaces and pods is critical to a happy upgrade.\n- **Certificates**: By default, they expire after a year. You get fresh ones with every upgrade, but you can also trigger a manual refresh whenever with `kubeadm certs renew`. If you are running an old cluster, PLEASE check the expiration dates of your client certificates with:\n\n```shell\nkubeadm certs check-expiration now\n```\n\n- **Stateful deployments**: Are they storing something, where are they storing it, and how do you manage them? This would be databases, Redis, message queues, applications that hold state. These are often the hardest to move or interact with during an upgrade. You can review the options for moving those [here](https://www.velotio.com/engineering-blog/exploring-upgrade-strategies-for-stateful-sets-in-kubernetes). The biggest thing is to set the pod disruption budget so that there is some minimum available during the upgrade process as shown [here](https://kubernetes.io/docs/tasks/run-application/configure-pdb/).\n  \n### üîç Verify Available IP Addresses\n\nTo update the cluster, Amazon EKS requires up to five available IP addresses from the subnets that you specified when you created your cluster. To verify that your subnets have enough IP addresses to upgrade the cluster, you can run the following command:\n\n```bash\nCLUSTER=<cluster name>\naws ec2 describe-subnets --subnet-ids \\\n    $(aws eks describe-cluster --name ${CLUSTER} \\\n    --query 'cluster.resourcesVpcConfig.subnetIds' \\\n    --output text) \\\n    --query 'Subnets[*].[SubnetId,AvailabilityZone,AvailableIpAddressCount]' \\\n    --output table\n```\n\nThe [VPC CNI Metrics Helper](https://github.com/aws/amazon-vpc-cni-k8s/tree/master/cmd/cni-metrics-helper) may be used to create a CloudWatch dashboard for VPC metrics.\n\n### üîÑ Transition to EKS Add-ons\n\nAmazon EKS seamlessly deploys essential add-ons like the Amazon VPC CNI plugin for Kubernetes, kube-proxy, and CoreDNS for each cluster. These add-ons can either be self-managed or installed via Amazon EKS Add-ons, offering an alternative approach to add-on management through the EKS API.\n\nWith Amazon EKS Add-ons, you gain the convenience of updating versions with a single command. For instance:\n\n```bash\naws eks update-addon --cluster-name my-cluster --addon-name vpc-cni --addon-version version-number \\\n--service-account-role-arn arn:aws:iam::111122223333:role/role-name --configuration-values '{}' --resolve-conflicts PRESERVE\n```\n\nCheck if you have any EKS Add-ons with:\n\n```bash\naws eks list-addons --cluster-name <cluster name>\n```\n\nEKS Add-ons are not automatically upgraded during a control plane upgrade. You must initiate EKS add-on updates and select the desired version. You are responsible for selecting a compatible version from all available versions. Review the guidance on [add-on version compatibility](https://aws.github.io/aws-eks-best-practices/upgrades/#upgrade-add-ons-and-components-using-the-kubernetes-api).\n\n<div class=\"warning\">\n    <p><strong>‚ö†Ô∏è Warning:</strong></p>\n    <p>Amazon EKS Add-ons may only be upgraded one minor version at a time.</p>\n</div>\n\n### üõ†Ô∏è Kube-no-trouble\n\n[**Kube-no-trouble**](https://github.com/doitintl/kube-no-trouble) is an open-source command line utility with the command `kubent`. When you run `kubent` without any arguments, it will use your current KubeConfig context, scan the cluster, and print a report with what APIs will be deprecated and removed.\n\n```shell\nkubent\n```\n\n```shell\n4:17PM INF >>> Kube No Trouble `kubent` <<<\n4:17PM INF version 0.7.0 (git sha d1bb4e5fd6550b533b2013671aa8419d923ee042)\n4:17PM INF Initializing collectors and retrieving data\n4:17PM INF Target K8s version is 1.24.8-eks-ffeb93d\n4:17PM INF Retrieved 93 resources from collector name=Cluster\n4:17PM INF Retrieved 16 resources from collector name=\"Helm v3\"\n4:17PM INF Loaded ruleset name=custom.rego.tmpl\n4:17PM INF Loaded ruleset name=deprecated-1-16.rego\n4:17PM INF Loaded ruleset name=deprecated-1-22.rego\n4:17PM INF Loaded ruleset name=deprecated-1-25.rego\n4:17PM INF Loaded ruleset name=deprecated-1-26.rego\n4:17PM INF Loaded ruleset name=deprecated-future.rego\n__________________________________________________________________________________________\n>>> Deprecated APIs removed in 1.25 <<<\n------------------------------------------------------------------------------------------\nKIND                NAMESPACE     NAME             API_VERSION      REPLACE_WITH (SINCE)\nPodSecurityPolicy   <undefined>   eks.privileged   policy/v1beta1   <removed> (1.21.0)\n```\n\nIt can also be used to scan static manifest files and helm packages. It is recommended to run `kubent` as part of a continuous integration (CI) process to identify issues before manifests are deployed. Scanning manifests is also more accurate than scanning live clusters.\n\nKube-no-trouble provides a sample [Service Account and Role](https://github.com/doitintl/kube-no-trouble/blob/master/docs/k8s-sa-and-role-example.yaml) with the appropriate permissions for scanning the cluster.\n\n### üõ†Ô∏è Pluto\n\nAnother option is [**Pluto**](https://pluto.docs.fairwinds.com/), which is similar to `kubent` because it supports scanning a live cluster, manifest files, helm charts, and has a [GitHub Action](https://github.com/FairwindsOps/pluto) you can include in your CI process.\n\nSee if you can upgrade safely against API paths. I use Pluto. This will check to see if you are calling deprecated or removed API paths in your configuration or helm charts. Run Pluto against local files with:\n\n```shell\npluto detect-files -d\n```\n\nYou can also check Helm with:\n\n```shell\npluto detect-helm -o wide\n```\n\nAdding all of this to CI is also pretty trivial and something I recommend for people managing many clusters.\n\n```shell\npluto detect-all-in-cluster\n```\n\n```plaintext\nNAME             KIND                VERSION          REPLACEMENT   REMOVED   DEPRECATED   REPL AVAIL  \neks.privileged   PodSecurityPolicy   policy/v1beta1                 false     true         true\n```\n\nAfter you have identified what workloads and manifests need to be updated, you may need to change the resource type in your manifest files (e.g., PodSecurityPolicies to PodSecurityStandards). This will require updating the resource specification and additional research depending on what resource is being replaced.\n\nIf the resource type is staying the same but the API version needs to be updated, you can use the `kubectl-convert` command to automatically convert your manifest files. For example, to convert an older Deployment to `apps/v1`. For more information, see [Install kubectl convert plugin](https://kubernetes.io/docs/tasks/tools/install-kubectl/) on the Kubernetes website.\n\n```shell\nkubectl-convert -f <file> --output-version <group>/<version>\n```\n\n### üõ†Ô∏è Nova\n\nCheck your Helm releases for upgrades. Since typically things like the CNI and other dependencies like CoreDNS are installed with Helm, this is often the fastest way to make sure you are running the latest version (check patch notes to ensure they support the version you are targeting). I use **Nova** for this.\n\n### üõ†Ô∏è KubePug\n\n**KubePug/Deprecations** is designed to function as a kubectl plugin with the following capabilities:\n\n- **Downloads Deprecation Data**: It fetches a `data.json` file containing deprecation information related to Kubernetes APIs.\n- **Validation Check**: Verifies the current Kubernetes cluster or input files to determine whether objects exist in deprecated API Versions. This enables users to assess and plan for migration before proceeding.\n\n#### Key Features\n\n- Operates against a Kubernetes cluster, utilizing kubeconfig or the active cluster.\n- Can be executed against a distinct set of manifests or files.\n- Allows users to specify the target Kubernetes version for validation.\n- Provides information on the replacement API that should be adopted.\n- Offers details about the version in which the API was deprecated or removed, based on the target cluster version.\n\n#### How to Install as a Krew Plugin\n\nSimply run the following command to install it as a Krew plugin:\n\n```shell\nkubectl krew install deprecations\n```\n\n### üõ†Ô∏è eksup: Cluster Upgrade Preparation Assistant\n\n**eksup** is a command-line interface (CLI) designed to empower users with comprehensive information and tools for preparing their clusters for an upgrade. It streamlines the upgrade process by delivering critical insights and actions for a seamless transition.\n\n#### Key Functions\n\n- **Cluster Analysis**: eksup allows users to assess their clusters against the next Kubernetes version, identifying any potential issues that could impact the upgrade process.\n- **Playbook Generation**: Users can generate customized playbooks that outline the upgrade steps specific to their cluster's analysis results. These playbooks detail the necessary actions and remediations.\n- **Flexibility and Learning**: The generated playbook is editable, enabling users to adapt the upgrade steps to align with their cluster configurations and business needs. It also provides a platform to document insights gained during the upgrade process.\n- **Enhanced Collaboration**: As upgrades are often initiated on non-production clusters first, any additional steps or insights discovered during this phase can be captured and utilized to enhance the upgrade process for production clusters.\n- **Historical Artifact**: Users are encouraged to preserve their playbooks as historical references. This practice ensures that each upgrade cycle benefits from a deeper understanding of the process, instilling confidence and efficiency in future upgrades before Kubernetes version support expires.\n\n**High Level¬†Diagram**\n\n![Alt text](./eksup.png)\n\n### üö¶ GoNoGo\n\n[**GoNoGo**](https://github.com/FairwindsOps/GoNoGo) is an alpha-stage tool to determine the upgrade confidence of your cluster add-ons.\n\n### üõ°Ô∏è Configure PodDisruptionBudgets and TopologySpreadConstraints During Data Plane Upgrade\n\nTo safeguard the availability of your workloads during a data plane upgrade, it's crucial to configure **PodDisruptionBudgets** and **topologySpreadConstraints** appropriately. Remember that not all workloads demand the same level of availability. Thus, it's imperative to assess your workload's scale and requirements.\n\nEnsuring that workloads are distributed across multiple Availability Zones and hosts with topology spreads enhances the confidence that migrations to the new data plane will occur seamlessly and without disruptions.\n\nHere's an illustrative example of a workload configuration that guarantees 80% of replicas are consistently available and efficiently spreads replicas across zones and hosts:\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n    name: myapp\nspec:\n    minAvailable: \"80%\"\n    selector:\n        matchLabels:\n            app: myapp\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: myapp\nspec:\n    replicas: 10\n    selector:\n        matchLabels:\n            app: myapp\n    template:\n        metadata:\n            labels:\n                app: myapp\n        spec:\n            containers:\n            - image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n                name: myapp\n                resources:\n                    requests:\n                        cpu: \"1\"\n                        memory: 256M\n            topologySpreadConstraints:\n            - labelSelector:\n                    matchLabels:\n                        app: host-zone-spread\n                maxSkew: 2\n                topologyKey: kubernetes.io/hostname\n                whenUnsatisfiable: DoNotSchedule\n            - labelSelector:\n                    matchLabels:\n                        app: host-zone-spread\n                maxSkew: 2\n                topologyKey: topology.kubernetes.io/zone\n                whenUnsatisfiable: DoNotSchedule\n```\n\n### üåê AWS Resilience Hub\n\n[AWS Resilience Hub](https://aws.amazon.com/resilience-hub/) has added Amazon Elastic Kubernetes Service (Amazon EKS) as a supported resource. Resilience Hub provides a single place to define, validate, and track the resilience of your applications so that you can avoid unnecessary downtime caused by software, infrastructure, or operational disruptions.\n\n### üõ†Ô∏è Use Managed Node Groups or Karpenter to Simplify Data Plane Upgrades\n\nManaged Node Groups and Karpenter both simplify node upgrades, but they take different approaches.\n\n- **Managed Node Groups**: Automate the provisioning and lifecycle management of nodes. This means that you can create, automatically update, or terminate nodes with a single operation.\n- **Karpenter**: Automatically creates new nodes using the latest compatible EKS Optimized AMI. As EKS releases updated EKS Optimized AMIs or the cluster is upgraded, Karpenter will automatically start using these images. [Karpenter also implements Node Expiry to update nodes](https://aws.github.io/aws-eks-best-practices/upgrades/#enable-node-expiry-for-karpenter-managed-nodes). [Karpenter can be configured to use custom AMIs](https://karpenter.sh/docs/concepts/node-templates/). If you use custom AMIs with Karpenter, you are responsible for the version of kubelet.\n\n### ü§ñ Use `eksctl` to Automate Upgrades for Self-Managed Node Groups\n\nSelf-managed node groups are EC2 instances that were deployed in your account and attached to the cluster outside of the EKS service. These are usually deployed and managed by some form of automation tooling. To upgrade self-managed node groups you should refer to your tools documentation.\n\nFor example, `eksctl` supports [deleting and draining self-managed nodes](https://eksctl.io/usage/managing-nodegroups/#deleting-and-draining). Some common tools include:\n\n- [eksctl](https://eksctl.io/usage/nodegroup-upgrade/)\n- [kOps](https://kops.sigs.k8s.io/operations/updates_and_upgrades/)\n- [EKS Blueprints](https://aws-ia.github.io/terraform-aws-eks-blueprints/node-groups/#self-managed-node-groups)\n\n### üíæ Backup the Cluster Before Upgrading\n\nNew versions of Kubernetes introduce significant changes to your Amazon EKS cluster. After you upgrade a cluster, you can't downgrade it.\n\n[Velero](https://velero.io/) is a community-supported open-source tool that can be used to take backups of existing clusters and apply the backups to a new cluster.\n\nNote that you can only create new clusters for Kubernetes versions currently supported by EKS. If the version your cluster is currently running is still supported and an upgrade fails, you can create a new cluster with the original version and restore the data plane. Note that AWS resources, including IAM, are not included in the backup by Velero. These resources would need to be recreated.\n\n### üìå Anticipate Major Kubernetes Changes‚Ää-‚ÄäStay Ahead of the Curve\n\nRather than solely focusing on the immediate next version of Kubernetes, adopt a forward-thinking approach. Continuously monitor new Kubernetes releases and be vigilant in identifying significant alterations. For instance, certain applications directly interfaced with the Docker API, and Kubernetes 1.24 made a pivotal change by removing support for Container Runtime Interface (CRI) for Docker, commonly known as Dockershim. üê≥ Preparing for such substantial changes demands additional time and planning.\n\nExamine all documented modifications for the version to which you plan to upgrade, meticulously noting any mandatory upgrade procedures. Additionally, pay attention to specific requirements or processes tailored to Amazon EKS managed clusters. This proactive stance ensures a smoother upgrade process while mitigating potential disruptions caused by unforeseen changes.\n\nAlways keep an eye on [Kubernetes CHANGLELOG](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG)\n\n### üö¢ Important Guidance on Feature Removals\n\n#### ‚ñ∂Ô∏è Removal of Dockershim in 1.25‚Ää-‚ÄäTransition to Detector for Docker Socket (DDS)\n\nIn Kubernetes 1.25, Dockershim support has been discontinued, particularly in the EKS Optimized AMI for 1.25. If your applications rely on Dockershim, for instance, by mounting the Docker socket, it's imperative to eliminate these dependencies before proceeding with the upgrade of your worker nodes to version 1.25. This ensures a seamless transition without any disruptions caused by the removal of Dockershim.\n\nFind instances where you have a dependency on the Docker socket before upgrading to 1.25. We recommend using [Detector for Docker Socket (DDS)](https://github.com/aws-containers/kubectl-detector-for-docker-socket), a kubectl plugin.\n\n#### ‚ñ∂Ô∏è Removal of PodSecurityPolicy in 1.25‚Ää-‚ÄäMigration to Pod Security Standards or Policy-as-Code Solution\n\nPodSecurityPolicy, deprecated in Kubernetes 1.21, has been entirely removed in Kubernetes 1.25. If your cluster currently utilizes PodSecurityPolicy, it's paramount to initiate a migration process before upgrading your cluster to version 1.25. This migration should involve transitioning to the native Kubernetes Pod Security Standards (PSS) or implementing a policy-as-code solution. This proactive step is crucial for maintaining the uninterrupted functionality of your workloads during the upgrade.\n\n- Review the [Pod Security Standards (PSS)](https://kubernetes.io/docs/concepts/security/pod-security-standards/) and [Pod Security Admission (PSA)](https://kubernetes.io/docs/concepts/security/pod-security-admission/) best practices.\n- Review the [PodSecurityPolicy Deprecation blog post](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) on the Kubernetes website.\n\n#### ‚ñ∂Ô∏è Deprecation of In-Tree Storage Driver in 1.23‚Ää-‚ÄäMigrate to Container Storage Interface (CSI) Drivers\n\nThe Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms. The Amazon EBS container storage interface (CSI) migration feature is enabled by default in Amazon EKS 1.23 and later clusters. If you have pods running on a version 1.22 or earlier cluster, then you must install the Amazon EBS CSI driver before updating your cluster to version 1.23 to avoid service interruption.\n\n- Review the [Amazon EBS CSI migration frequently asked questions](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html).\n\n### üîö Conclusion\n\nI trust that the information provided has been valuable to you. The journey of keeping Kubernetes consistently upgraded becomes less daunting with regular practice. The key takeaway is to allocate ample time to acclimate your environment with each minor release. By adhering to a predictable schedule, the process of upgrading clusters becomes remarkably painless and straightforward, even for less experienced users, as long as you perform the necessary checks.\n\nRemember, the key to success lies in regularity and thorough preparation when it comes to Kubernetes upgrades.\n\nWe hope that you have found this blog post helpful. If you have any other tips or tricks that you would like to share, please leave a comment below.\n\n## üîö Conclusion\n\nI trust that the information provided has been valuable to you. The journey of keeping Kubernetes consistently upgraded becomes less daunting with regular practice. The key takeaway is to allocate ample time to acclimate your environment with each minor release. By adhering to a predictable schedule, the process of upgrading clusters becomes remarkably painless and straightforward, even for less experienced users, as long as you perform the necessary checks.\n\nRemember, the key to success lies in regularity and thorough preparation when it comes to Kubernetes upgrades.\n\nWe hope that you have found this blog post helpful. If you have any other tips or tricks that you would like to share, please leave a comment below.\n\n**üìï References:**\n\n- [AWS Knowledge Center: Plan an EKS Cluster Upgrade](https://repost.aws/knowledge-center/eks-plan-upgrade-cluster)\n- [AWS EKS Best Practices: Upgrades](https://aws.github.io/aws-eks-best-practices/upgrades/)\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  _**Until next time üéâ**_\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":4558},"frontmatter":{"id":"623dfab8422e6980e24c8da7","path":"/blog/eks-upgrade-guide/","humanDate":"Oct 31, 2024","fullDate":"2024-10-31","title":"EKS cluster upgrades: A Step-by-Step guide to a secure Process","keywords":["AWS EKS","Cluster upgrades","Security","Best practices"],"excerpt":"Comprehensive step-by-step guide to securely upgrade your EKS clusters, ensuring minimal downtime and maintaining best practices throughout the process.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAMBAQAAAAAAAAAAAAAAAAIDBAEF/8QAFwEAAwEAAAAAAAAAAAAAAAAAAAECA//aAAwDAQACEAMQAAAB9tnvkkiZm6Zq0Uf/xAAcEAACAgIDAAAAAAAAAAAAAAAAAQIRAxASIUH/2gAIAQEAAQUC3OfFloyqz1dn/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEQMf/aAAgBAgEBPwGrD//EABkQAAIDAQAAAAAAAAAAAAAAAAAQESGBMf/aAAgBAQAGPwJ7a6QYv//EABwQAAICAgMAAAAAAAAAAAAAAAERADEQIWFxof/aAAgBAQABPyHIdW/CbVhxVgOzLQlSnHBQQDIn/9oADAMBAAIAAwAAABDMAP8A/8QAFREBAQAAAAAAAAAAAAAAAAAAEAH/2gAIAQMBAT8QKf/EABkRAAEFAAAAAAAAAAAAAAAAAAABEBEhMf/aAAgBAgEBPxBpWzI//8QAHhABAAICAQUAAAAAAAAAAAAAAQARITFBEFFhcZH/2gAIAQEAAT8Q6GpSzkRqupj5FtAK45JotdgIYxZveV837m/FZXCpU0urQn//2Q=="},"images":{"fallback":{"src":"/static/5c23d9b79ad156778f7439282f5c26a2/baaed/eks-upg-cover.jpg","srcSet":"/static/5c23d9b79ad156778f7439282f5c26a2/0f5ce/eks-upg-cover.jpg 750w,\n/static/5c23d9b79ad156778f7439282f5c26a2/baaed/eks-upg-cover.jpg 800w","sizes":"100vw"},"sources":[{"srcSet":"/static/5c23d9b79ad156778f7439282f5c26a2/4f03f/eks-upg-cover.webp 750w,\n/static/5c23d9b79ad156778f7439282f5c26a2/c1587/eks-upg-cover.webp 800w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/discord-cassandra-to-scylladb/","title":"Scaling Trillions of Messages: Discord's Journey from Cassandra to ScyllaDB with Rust-Powered Solutions üöÄ","date":"2024-11-01 17:06:00"},"excerpt":"An Epic Journey from Cassandra to ScyllaDB with Rusty Reinforcements üöÄ üî∞ Overview In the world of messaging platforms, Discord stands as a‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/kubernetes-opa-gatekeeper-policy-as-code/","title":"Securing Your Kubernetes Cluster with OPA Gatekeeper: The Power of Policy-as-Code üõ°Ô∏è","date":"2024-10-31 20:00:00"},"excerpt":"Building a Fortress in Kubernetes: How OPA Gatekeeper and Policy-as-Code Keep Your Cluster Safe üõ°Ô∏è Introduction As organizations adopt‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}