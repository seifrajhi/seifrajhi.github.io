{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/kubernetes-dns-coredns-externaldns/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>The ABCs of Kubernetes DNS üê≥</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• Introduction</h2>\n<p>Kubernetes, an open-source platform for automating containerized applications, relies on Domain Name System (DNS) to facilitate communication between its various components. Two essential tools for managing DNS within a Kubernetes cluster are <strong>CoreDNS</strong> and <strong>ExternalDNS</strong>. In this blog post, we will take a straightforward look at these tools, their functions, and how they can benefit your Kubernetes environment.</p>\n<h2 id=\"-how-coredns-and-externaldns-work\" style=\"position:relative;\"><a href=\"#-how-coredns-and-externaldns-work\" aria-label=\" how coredns and externaldns work permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üï∏ How CoreDNS and ExternalDNS Work</h2>\n<p>Kubernetes relies on the Domain Name System (DNS) to enable seamless communication between its various components, such as pods and services. When a new Kubernetes service is created, the platform automatically generates a DNS record for it, allowing other pods to easily locate and connect to the service. Kubernetes also offers support for <strong>ExternalDNS</strong>, which simplifies the process of creating and managing DNS records for services that need to be accessible externally. This makes it easier for external clients to access the services within the cluster.</p>\n<p>In simpler terms:</p>\n<ul>\n<li>Kubernetes uses DNS to help pods and services find and communicate with each other using hostnames.</li>\n<li>When a Kubernetes service is created, a DNS record is automatically generated for it.</li>\n<li>Kubernetes supports ExternalDNS, which helps manage DNS records for services that need to be accessible outside the cluster.</li>\n</ul>\n<h3 id=\"externaldns\" style=\"position:relative;\"><a href=\"#externaldns\" aria-label=\"externaldns permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>ExternalDNS</h3>\n<p>In short, <strong>ExternalDNS</strong> is a pod running in your EKS cluster that watches over all your ingresses. When it detects an ingress with a host specified, it automatically picks up the hostname as well as the endpoint and creates a record for that resource in <a href=\"https://aws.amazon.com/route53/\" target=\"_blank\" rel=\"noopener noreferrer\">Route53</a>. If the host is changed or deleted, ExternalDNS will reflect the change immediately in Route53.</p>\n<p>This system allows for automatically creating and managing DNS records for services exposed externally with the supported DNS providers. It enables external clients to access the services running inside the cluster by resolving the service's hostname to the external IP address of the Kubernetes cluster.</p>\n<h3 id=\"coredns\" style=\"position:relative;\"><a href=\"#coredns\" aria-label=\"coredns permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>CoreDNS</h3>\n<p>This is a DNS server explicitly built for Kubernetes and is now the default DNS server in Kubernetes 1.14 and later. <strong>CoreDNS</strong> is a flexible, extensible DNS server that can perform service discovery and name resolution within the cluster, and with some configuration changes, it can leverage external DNS providers.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 6.470588235294117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAABCAYAAADeko4lAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAQklEQVR42h2LOw5AIQjA3v2vKSrE3yAmbH0JUzu0X2sNEWGMwVorOeeklIKqEhGcc9h7p5sZtdZs3J33Hvdeeu/5/t6jS1hd+/LxAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"CoreDNS\" title=\"\" src=\"/static/e0e1e0f7b24457d00cbc355c32ad765f/c5bb3/coreDNS.png\" srcset=\"/static/e0e1e0f7b24457d00cbc355c32ad765f/04472/coreDNS.png 170w,\n/static/e0e1e0f7b24457d00cbc355c32ad765f/9f933/coreDNS.png 340w,\n/static/e0e1e0f7b24457d00cbc355c32ad765f/c5bb3/coreDNS.png 680w,\n/static/e0e1e0f7b24457d00cbc355c32ad765f/b12f7/coreDNS.png 1020w,\n/static/e0e1e0f7b24457d00cbc355c32ad765f/b5a09/coreDNS.png 1360w,\n/static/e0e1e0f7b24457d00cbc355c32ad765f/f4b34/coreDNS.png 1457w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.176470588235293%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA2klEQVR42m2R16qGMBCE8/5vJ/besYuIFcv8TEDwHJyrTXYy+bIR930jSRJomgbbtmGaJnRdh+/7cBwHeZ5jXVeEYQhVVWXfsiz0fY9hGOQZ7tMbRRHEsiyySNNUBrmuKw/FcYwsy+B5nryQ/SAIoCgK2rZF0zSyz0Cu53kGswQJSEcDN7dtA6mv6wJFOhqLopAUDOAFXddh33ccx4G3RFmWkpD4FMMeveuqqmAYhqQnxHmef3yPV7zT3423SFHXtQziKDg/vuRL4j/Jl6Zpkp9CuucDxnH8DPwBG2THLwQf9rwAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Ingress\" title=\"\" src=\"/static/a652aaa19f4b64fab1dbfd4628f736c0/c5bb3/ingress.png\" srcset=\"/static/a652aaa19f4b64fab1dbfd4628f736c0/04472/ingress.png 170w,\n/static/a652aaa19f4b64fab1dbfd4628f736c0/9f933/ingress.png 340w,\n/static/a652aaa19f4b64fab1dbfd4628f736c0/c5bb3/ingress.png 680w,\n/static/a652aaa19f4b64fab1dbfd4628f736c0/b12f7/ingress.png 1020w,\n/static/a652aaa19f4b64fab1dbfd4628f736c0/b5a09/ingress.png 1360w,\n/static/a652aaa19f4b64fab1dbfd4628f736c0/29007/ingress.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"why-externaldns-is-a-valuable-addition-to-k8s-cluster\" style=\"position:relative;\"><a href=\"#why-externaldns-is-a-valuable-addition-to-k8s-cluster\" aria-label=\"why externaldns is a valuable addition to k8s cluster permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Why ExternalDNS is a Valuable Addition to K8s Cluster</h3>\n<p>Kubernetes's built-in DNS system, known as Kube-DNS or CoreDNS, is responsible for resolving DNS names for Kubernetes services and pods within a cluster. However, organizations often opt for an external DNS system due to several advantages:</p>\n<ul>\n<li><strong>Advanced features</strong>: External DNS systems offer additional functionalities such as global load balancing, automatic failover, and DNS-based traffic management. They also include built-in security features like DNSSEC to protect against tampering and spoofing attacks, which are crucial for organizations managing traffic across multiple regions, handling high traffic loads, or managing sensitive data.</li>\n<li><strong>Consistent DNS infrastructure</strong>: An external DNS system allows organizations to maintain a consistent DNS infrastructure across all their applications, whether running on Kubernetes or not. This simplifies management and enhances security.</li>\n<li><strong>Granular control</strong>: External DNS provides granular and dynamic control over DNS records or text instructions stored on DNS servers. Its primary role is to act as a bridge, enabling the use of specialized DNS providers outside of Kubernetes. External DNS can handle millions of DNS records and offer more options for managing them.</li>\n<li><strong>Scalability</strong>: As the number of services and pods within a Kubernetes cluster grows, the Kube-DNS system can become a bottleneck. An external DNS system can handle a much larger number of DNS queries, ensuring that the DNS system does not become a bottleneck for the rest of the cluster.</li>\n<li><strong>Flexibility</strong>: Using External DNS with Kubernetes offers greater flexibility in the choice of DNS server types. Depending on your requirements and preferences, you can select from various open-source DNS options like CoreDNS, SkyDNS, or Knot DNS, as well as commercial DNS solutions such as <a href=\"https://cloud.google.com/dns\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud DNS</a>, <a href=\"https://aws.amazon.com/route53/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Route 53</a>, BIND, or Microsoft DNS.</li>\n</ul>\n<p>Integrating an external DNS system with Kubernetes provides organizations with a more advanced and flexible DNS infrastructure and management. It is a recommended practice to use an external DNS when deploying Kubernetes in production, as several popular external DNS providers can work with Kubernetes.</p>\n<h2 id=\"set-up-externaldns-in-the-eks-cluster\" style=\"position:relative;\"><a href=\"#set-up-externaldns-in-the-eks-cluster\" aria-label=\"set up externaldns in the eks cluster permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Set up ExternalDNS in the EKS Cluster</h2>\n<p>The setup can be divided into two parts: setting up permissions (to give your service access to Route53) and deploying the ExternalDNS.</p>\n<h3 id=\"Ô∏è-setting-up-route53-permissions-for-your-externaldns\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-setting-up-route53-permissions-for-your-externaldns\" aria-label=\"Ô∏è setting up route53 permissions for your externaldns permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Setting up Route53 Permissions for Your ExternalDNS</h3>\n<p>In this tutorial, we will make use of an IAM role with an ID provider.</p>\n<p>The main idea here is to give the ExternalDNS pod the permission to create, update, and delete Route53 records in your AWS account. To do so, we need to use an identity provider in the AWS IAM service. An identity provider allows an external user to assume roles in your AWS account by setting up a trust relationship.</p>\n<h4 id=\"setting-up-the-identity-provider\" style=\"position:relative;\"><a href=\"#setting-up-the-identity-provider\" aria-label=\"setting up the identity provider permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Setting up the Identity Provider</h4>\n<p>To create an identity provider, you need three things: a type of identity provider, an audience, and a provider URL.</p>\n<p>In this tutorial, we will use OpenID Connect for the provider type and <code class=\"language-text\">sts.amazonaws.com</code> for the audience. The provider URL varies; to get your provider URL, you can use the following command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">aws eks describe-cluster <span class=\"token parameter variable\">--name</span> <span class=\"token operator\">&lt;</span>CLUSTER_NAME<span class=\"token operator\">></span> <span class=\"token parameter variable\">--query</span> <span class=\"token string\">\"cluster.identity.oidc.issuer\"</span> <span class=\"token parameter variable\">--output</span> text</code></pre></div>\n<p>The output should look something like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">https://oidc.eks.<span class=\"token operator\">&lt;</span>region<span class=\"token operator\">></span>.amazonaws.com/id/EXAMPLE86F27C29EF05B482628D9790EA7066.</code></pre></div>\n<p>You now have everything you need to set up your identity provider! Head over to <a href=\"https://console.aws.amazon.com/iam/home?#/providers\" target=\"_blank\" rel=\"noopener noreferrer\">the identity provider section of IAM in the AWS console</a> and create a new provider.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 32.35294117647059%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVR42oWPUQ6DIBBEvf8ljbUCqQZBQFGx9FXTxqZJOx+EhX07s8W6rsaYYRiccymlnPO2bUqpsiybphFCcOErv3Q/qYgxAltr4adpOmDKtm0hwZZlCSHwhQ3TP2Bal13n2Zzeeyml2xNV1aWur1prXBjx7izylw54nmcSPXP5KG5adUa2fa066/zh/wtGLDKOI4F7Y1nM+0DBCoT9D9OHP3y/i1nswiPOJEopPQAuUVx3TokG9wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"OIDC\" title=\"\" src=\"/static/34936790f09c523d255f1f0fc78c5f49/c5bb3/oidc.png\" srcset=\"/static/34936790f09c523d255f1f0fc78c5f49/04472/oidc.png 170w,\n/static/34936790f09c523d255f1f0fc78c5f49/9f933/oidc.png 340w,\n/static/34936790f09c523d255f1f0fc78c5f49/c5bb3/oidc.png 680w,\n/static/34936790f09c523d255f1f0fc78c5f49/b12f7/oidc.png 1020w,\n/static/34936790f09c523d255f1f0fc78c5f49/b5a09/oidc.png 1360w,\n/static/34936790f09c523d255f1f0fc78c5f49/29007/oidc.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Now that you have an identity provider, all that's left to do is create an IAM role with Route53 permissions and a trust relationship with your brand-new provider.</p>\n<p>First, create a new role in IAM and trust your provider by selecting Web Identity and inputting your provider information.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 28.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAxklEQVR42o1Qyw6CMBDk/3/Iox/gwYQTMSFCMAb6lnYpGEqdluBV5zDZ3enso8U4jtbadV3jL2xbPF2elpawJoQQCpYhpez7HozS8fRAjCFseItQuzdc0EJOk1kqRUTLsmCLV4Yxxk/Ucl/e9a3pW2aZ26RxWgmsmVTvMaDoug5+rTXMCPZ0GAYlxbVW5/JZtazqTM188xjaphFCYEf409p6xFRCJ/A0Td8gVcmRczYR4Wd2FTzPMzScXTDOcSrPjJloGf/GBxExV7IOyaMBAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Identity\" title=\"\" src=\"/static/2d236f25f2462608816e93ad54578c8f/c5bb3/identity.png\" srcset=\"/static/2d236f25f2462608816e93ad54578c8f/04472/identity.png 170w,\n/static/2d236f25f2462608816e93ad54578c8f/9f933/identity.png 340w,\n/static/2d236f25f2462608816e93ad54578c8f/c5bb3/identity.png 680w,\n/static/2d236f25f2462608816e93ad54578c8f/b12f7/identity.png 1020w,\n/static/2d236f25f2462608816e93ad54578c8f/b5a09/identity.png 1360w,\n/static/2d236f25f2462608816e93ad54578c8f/29007/identity.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Once this has been completed, click create a new policy and input the following in JSON:</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"Version\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"2012-10-17\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"Statement\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span>\n            <span class=\"token property\">\"Effect\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Allow\"</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"Action\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n                <span class=\"token string\">\"route53:ChangeResourceRecordSets\"</span>\n            <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"Resource\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n                <span class=\"token string\">\"arn:aws:route53:::hostedzone/*\"</span>\n            <span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">{</span>\n            <span class=\"token property\">\"Effect\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Allow\"</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"Action\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n                <span class=\"token string\">\"route53:ListHostedZones\"</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">\"route53:ListResourceRecordSets\"</span>\n            <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"Resource\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n                <span class=\"token string\">\"*\"</span>\n            <span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Then attach the <a href=\"https://console.aws.amazon.com/iam/home?region=eu-west-3#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonEKSClusterPolicy\" target=\"_blank\" rel=\"noopener noreferrer\">AmazonEKSClusterPolicy</a> to the role as well.</p>\n<p>Once the role is created, keep the ARN; you will need it later.</p>\n<h3 id=\"-installing-the-externaldns\" style=\"position:relative;\"><a href=\"#-installing-the-externaldns\" aria-label=\" installing the externaldns permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Installing the ExternalDNS</h3>\n<p>We will install the ExternalDNS using Helm. If you do not have Helm installed on your machine, you can find instructions to <a href=\"https://helm.sh/docs/intro/install/\" target=\"_blank\" rel=\"noopener noreferrer\">install Helm</a>.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token function\">install</span> <span class=\"token operator\">&lt;</span>RELEASE_NAME<span class=\"token operator\">></span> stable/external-dns <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">provider</span><span class=\"token operator\">=</span>aws <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> domainFilters<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>DOMAIN_FILTER<span class=\"token operator\">></span><span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">policy</span><span class=\"token operator\">=</span>sync <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">registry</span><span class=\"token operator\">=</span>txt <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">txtOwnerId</span><span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>HOSTED_ZONE_ID<span class=\"token operator\">></span> <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">interval</span><span class=\"token operator\">=</span>3m <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">rbac.create</span><span class=\"token operator\">=</span>true <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> <span class=\"token assign-left variable\">rbac.serviceAccountName</span><span class=\"token operator\">=</span>external-dns <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--set</span> rbac.serviceAccountAnnotations.eks<span class=\"token punctuation\">\\</span>.amazonaws<span class=\"token punctuation\">\\</span>.com/role-arn<span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>ROLE_ARN<span class=\"token operator\">></span></code></pre></div>\n<p>Where:</p>\n<ul>\n<li><code class=\"language-text\">RELEASE_NAME</code>: Name of the Helm release, can be anything you want (e.g., <code class=\"language-text\">external-dns</code>).</li>\n<li><code class=\"language-text\">DOMAIN_FILTER</code>: Name of your Route53 hosted zone. If <code class=\"language-text\">*.example.com</code>, it would be <code class=\"language-text\">example.com</code>. You can find information about the domain filter in the AWS console (Route53).</li>\n<li><code class=\"language-text\">HOSTED_ZONE_ID</code>: ID of your hosted zone in AWS. You can find this information in the AWS console (Route53).</li>\n<li><code class=\"language-text\">ROLE_ARN</code>: ARN of the role you created earlier in the tutorial.</li>\n</ul>\n<h3 id=\"-using-the-externaldns\" style=\"position:relative;\"><a href=\"#-using-the-externaldns\" aria-label=\" using the externaldns permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåê Using the ExternalDNS</h3>\n<p>To create a new Route53 record for your services, all you need to do is add the annotation: <code class=\"language-text\">external-dns.alpha.kubernetes.io/hostname</code>.</p>\n<h4 id=\"example-of-a-loadbalancer-service\" style=\"position:relative;\"><a href=\"#example-of-a-loadbalancer-service\" aria-label=\"example of a loadbalancer service permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Example of a LoadBalancer Service:</h4>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Service\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> test\n    <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">external-dns.alpha.kubernetes.io/hostname</span><span class=\"token punctuation\">:</span> myservice.example.com \n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> LoadBalancer\n    <span class=\"token key atrule\">ports</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> http\n        <span class=\"token key atrule\">targetPort</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> test</code></pre></div>\n<h4 id=\"for-an-ingress\" style=\"position:relative;\"><a href=\"#for-an-ingress\" aria-label=\"for an ingress permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>For an Ingress:</h4>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> extensions/v1beta1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Ingress\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">alb.ingress.kubernetes.io/scheme</span><span class=\"token punctuation\">:</span> internet<span class=\"token punctuation\">-</span>facing\n        <span class=\"token key atrule\">external-dns.alpha.kubernetes.io/hostname</span><span class=\"token punctuation\">:</span> myservice.example.com \n        <span class=\"token key atrule\">kubernetes.io/ingress.class</span><span class=\"token punctuation\">:</span> alb\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> test\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">rules</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">http</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">paths</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">backend</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">serviceName</span><span class=\"token punctuation\">:</span> test\n                    <span class=\"token key atrule\">servicePort</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n                <span class=\"token key atrule\">path</span><span class=\"token punctuation\">:</span> /</code></pre></div>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Summary</h2>\n<p>The Kubernetes DNS system, CoreDNS, and ExternalDNS enable seamless communication within clusters. They create DNS records for Services and Pods, allowing consistent access via DNS names instead of IP addresses. Understanding and customizing DNS resolution is crucial for effective management within a Kubernetes cluster.</p>\n<hr>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **The ABCs of Kubernetes DNS üê≥**\n\n## üî• Introduction\n\nKubernetes, an open-source platform for automating containerized applications, relies on Domain Name System (DNS) to facilitate communication between its various components. Two essential tools for managing DNS within a Kubernetes cluster are **CoreDNS** and **ExternalDNS**. In this blog post, we will take a straightforward look at these tools, their functions, and how they can benefit your Kubernetes environment.\n\n## üï∏ How CoreDNS and ExternalDNS Work\n\nKubernetes relies on the Domain Name System (DNS) to enable seamless communication between its various components, such as pods and services. When a new Kubernetes service is created, the platform automatically generates a DNS record for it, allowing other pods to easily locate and connect to the service. Kubernetes also offers support for **ExternalDNS**, which simplifies the process of creating and managing DNS records for services that need to be accessible externally. This makes it easier for external clients to access the services within the cluster.\n\nIn simpler terms:\n\n- Kubernetes uses DNS to help pods and services find and communicate with each other using hostnames.\n- When a Kubernetes service is created, a DNS record is automatically generated for it.\n- Kubernetes supports ExternalDNS, which helps manage DNS records for services that need to be accessible outside the cluster.\n\n### ExternalDNS\n\nIn short, **ExternalDNS** is a pod running in your EKS cluster that watches over all your ingresses. When it detects an ingress with a host specified, it automatically picks up the hostname as well as the endpoint and creates a record for that resource in [Route53](https://aws.amazon.com/route53/). If the host is changed or deleted, ExternalDNS will reflect the change immediately in Route53.\n\nThis system allows for automatically creating and managing DNS records for services exposed externally with the supported DNS providers. It enables external clients to access the services running inside the cluster by resolving the service's hostname to the external IP address of the Kubernetes cluster.\n\n### CoreDNS\n\nThis is a DNS server explicitly built for Kubernetes and is now the default DNS server in Kubernetes 1.14 and later. **CoreDNS** is a flexible, extensible DNS server that can perform service discovery and name resolution within the cluster, and with some configuration changes, it can leverage external DNS providers.\n\n![CoreDNS](./coreDNS.png)\n\n![Ingress](./ingress.png)\n\n### Why ExternalDNS is a Valuable Addition to K8s Cluster\n\nKubernetes's built-in DNS system, known as Kube-DNS or CoreDNS, is responsible for resolving DNS names for Kubernetes services and pods within a cluster. However, organizations often opt for an external DNS system due to several advantages:\n\n- **Advanced features**: External DNS systems offer additional functionalities such as global load balancing, automatic failover, and DNS-based traffic management. They also include built-in security features like DNSSEC to protect against tampering and spoofing attacks, which are crucial for organizations managing traffic across multiple regions, handling high traffic loads, or managing sensitive data.\n- **Consistent DNS infrastructure**: An external DNS system allows organizations to maintain a consistent DNS infrastructure across all their applications, whether running on Kubernetes or not. This simplifies management and enhances security.\n- **Granular control**: External DNS provides granular and dynamic control over DNS records or text instructions stored on DNS servers. Its primary role is to act as a bridge, enabling the use of specialized DNS providers outside of Kubernetes. External DNS can handle millions of DNS records and offer more options for managing them.\n- **Scalability**: As the number of services and pods within a Kubernetes cluster grows, the Kube-DNS system can become a bottleneck. An external DNS system can handle a much larger number of DNS queries, ensuring that the DNS system does not become a bottleneck for the rest of the cluster.\n- **Flexibility**: Using External DNS with Kubernetes offers greater flexibility in the choice of DNS server types. Depending on your requirements and preferences, you can select from various open-source DNS options like CoreDNS, SkyDNS, or Knot DNS, as well as commercial DNS solutions such as [Google Cloud DNS](https://cloud.google.com/dns), [Amazon Route 53](https://aws.amazon.com/route53/), BIND, or Microsoft DNS.\n\nIntegrating an external DNS system with Kubernetes provides organizations with a more advanced and flexible DNS infrastructure and management. It is a recommended practice to use an external DNS when deploying Kubernetes in production, as several popular external DNS providers can work with Kubernetes.\n\n## Set up ExternalDNS in the EKS Cluster\n\nThe setup can be divided into two parts: setting up permissions (to give your service access to Route53) and deploying the ExternalDNS.\n\n### üõ†Ô∏è Setting up Route53 Permissions for Your ExternalDNS\n\nIn this tutorial, we will make use of an IAM role with an ID provider.\n\nThe main idea here is to give the ExternalDNS pod the permission to create, update, and delete Route53 records in your AWS account. To do so, we need to use an identity provider in the AWS IAM service. An identity provider allows an external user to assume roles in your AWS account by setting up a trust relationship.\n\n#### Setting up the Identity Provider\n\nTo create an identity provider, you need three things: a type of identity provider, an audience, and a provider URL.\n\nIn this tutorial, we will use OpenID Connect for the provider type and `sts.amazonaws.com` for the audience. The provider URL varies; to get your provider URL, you can use the following command:\n\n```shell\naws eks describe-cluster --name <CLUSTER_NAME> --query \"cluster.identity.oidc.issuer\" --output text\n```\n\nThe output should look something like this:\n\n```shell\nhttps://oidc.eks.<region>.amazonaws.com/id/EXAMPLE86F27C29EF05B482628D9790EA7066.\n```\n\nYou now have everything you need to set up your identity provider! Head over to [the identity provider section of IAM in the AWS console](https://console.aws.amazon.com/iam/home?#/providers) and create a new provider.\n\n![OIDC](./oidc.png)\n\nNow that you have an identity provider, all that's left to do is create an IAM role with Route53 permissions and a trust relationship with your brand-new provider.\n\nFirst, create a new role in IAM and trust your provider by selecting Web Identity and inputting your provider information.\n\n![Identity](./identity.png)\n\nOnce this has been completed, click create a new policy and input the following in JSON:\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:ChangeResourceRecordSets\"\n            ],\n            \"Resource\": [\n                \"arn:aws:route53:::hostedzone/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"route53:ListHostedZones\",\n                \"route53:ListResourceRecordSets\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n```\n\nThen attach the [AmazonEKSClusterPolicy](https://console.aws.amazon.com/iam/home?region=eu-west-3#/policies/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonEKSClusterPolicy) to the role as well.\n\nOnce the role is created, keep the ARN; you will need it later.\n\n### üöÄ Installing the ExternalDNS\n\nWe will install the ExternalDNS using Helm. If you do not have Helm installed on your machine, you can find instructions to [install Helm](https://helm.sh/docs/intro/install/).\n\n```shell\nhelm install <RELEASE_NAME> stable/external-dns \\\n--set provider=aws \\\n--set domainFilters[0]=<DOMAIN_FILTER>\\\n--set policy=sync \\\n--set registry=txt \\\n--set txtOwnerId=<HOSTED_ZONE_ID> \\\n--set interval=3m \\\n--set rbac.create=true \\\n--set rbac.serviceAccountName=external-dns \\\n--set rbac.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=<ROLE_ARN>\n```\n\nWhere:\n\n- `RELEASE_NAME`: Name of the Helm release, can be anything you want (e.g., `external-dns`).\n- `DOMAIN_FILTER`: Name of your Route53 hosted zone. If `*.example.com`, it would be `example.com`. You can find information about the domain filter in the AWS console (Route53).\n- `HOSTED_ZONE_ID`: ID of your hosted zone in AWS. You can find this information in the AWS console (Route53).\n- `ROLE_ARN`: ARN of the role you created earlier in the tutorial.\n\n### üåê Using the ExternalDNS\n\nTo create a new Route53 record for your services, all you need to do is add the annotation: `external-dns.alpha.kubernetes.io/hostname`.\n\n#### Example of a LoadBalancer Service:\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n    name: test\n    annotations:\n        external-dns.alpha.kubernetes.io/hostname: myservice.example.com \nspec:\n    type: LoadBalancer\n    ports:\n    - port: 80\n        name: http\n        targetPort: 80\n    selector:\n        app: test\n```\n\n#### For an Ingress:\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n    annotations:\n        alb.ingress.kubernetes.io/scheme: internet-facing\n        external-dns.alpha.kubernetes.io/hostname: myservice.example.com \n        kubernetes.io/ingress.class: alb\n    name: test\nspec:\n    rules:\n    - http:\n            paths:\n            - backend:\n                    serviceName: test\n                    servicePort: 80\n                path: /\n```\n\n## üìå Summary\n\nThe Kubernetes DNS system, CoreDNS, and ExternalDNS enable seamless communication within clusters. They create DNS records for Services and Pods, allowing consistent access via DNS names instead of IP addresses. Understanding and customizing DNS resolution is crucial for effective management within a Kubernetes cluster.\n\n---\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1217},"frontmatter":{"id":"312a65d89f3184da8828a9f5","path":"/blog/kubernetes-dns-coredns-externaldns/","humanDate":"Oct 29, 2024","fullDate":"2024-10-29","title":"Kubernetes & DNS: A Guide to CoreDNS and ExternalDNS in AWS EKS clusterüê≥","keywords":["Kubernetes","DNS","CoreDNS","ExternalDNS","k8s","DevOps","AWS EKS","AWS"],"excerpt":"Dive into the essentials of Kubernetes DNS with this guide on CoreDNS and ExternalDNS, covering the basics and advanced configuration in AWS EKS cluster.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAgAD/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAWC5cqiv/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAECEQMSIf/aAAgBAQABBQKSsrjRjk92Wf/EABYRAQEBAAAAAAAAAAAAAAAAAAABIf/aAAgBAwEBPwGNf//EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAECAQE/AWf/xAAbEAABBAMAAAAAAAAAAAAAAAABAAIQESExgf/aAAgBAQAGPwK27jDbXQiI/8QAGxAAAgIDAQAAAAAAAAAAAAAAAREAITFBUYH/2gAIAQEAAT8hEyFoy6FDo5Lxp5MaDQgnp2A7sz//2gAMAwEAAgADAAAAENzf/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAEDAQE/EFuEE//EABYRAQEBAAAAAAAAAAAAAAAAAAERAP/aAAgBAgEBPxBwupv/xAAcEAEBAAICAwAAAAAAAAAAAAABEQAhMUFRYXH/2gAIAQEAAT8QpyA01BfN+4MworBXZ24x0BQVuROARraN5eXjvBADGN+2JaSM25//2Q=="},"images":{"fallback":{"src":"/static/cad30f55e5f5b2778695783477cc7ffd/c07d5/kubernetes-dns-cover.jpg","srcSet":"/static/cad30f55e5f5b2778695783477cc7ffd/7284f/kubernetes-dns-cover.jpg 750w,\n/static/cad30f55e5f5b2778695783477cc7ffd/29ba9/kubernetes-dns-cover.jpg 1080w,\n/static/cad30f55e5f5b2778695783477cc7ffd/2baac/kubernetes-dns-cover.jpg 1366w,\n/static/cad30f55e5f5b2778695783477cc7ffd/c07d5/kubernetes-dns-cover.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/cad30f55e5f5b2778695783477cc7ffd/57584/kubernetes-dns-cover.webp 750w,\n/static/cad30f55e5f5b2778695783477cc7ffd/984df/kubernetes-dns-cover.webp 1080w,\n/static/cad30f55e5f5b2778695783477cc7ffd/1e947/kubernetes-dns-cover.webp 1366w,\n/static/cad30f55e5f5b2778695783477cc7ffd/67855/kubernetes-dns-cover.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6664285714285714}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/cluster-dev-cloud-management/","title":"Discover Cluster.dev: A Simple Solution for Cloud Infrastructure Management ‚òÅÔ∏è","date":"2024-10-29 20:06:00"},"excerpt":"Manage Cloud Infrastructures Declaratively ‚òÅÔ∏è üìô Introduction Have you ever wished for a single tool to manage all your cloud infrastructure‚Ä¶","html":"<blockquote>\n<p><strong>Manage Cloud Infrastructures Declaratively ‚òÅÔ∏è</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìô Introduction</h2>\n<p>Have you ever wished for a single tool to manage all your cloud infrastructure needs?</p>\n<p><a href=\"https://cluster.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">Cluster.dev</a> is one of these tools that will change the way you handle cloud-native infrastructures.</p>\n<p>In this blog post, we will explore the incredible features of Cluster.dev and how it simplifies the deployment, testing, and distribution of cloud-native components.</p>\n<p>Get ready to discover Cluster.dev and how it can make your infrastructure management a breeze.</p>\n<h2 id=\"-when-do-i-need-clusterdev\" style=\"position:relative;\"><a href=\"#-when-do-i-need-clusterdev\" aria-label=\" when do i need clusterdev permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ùì When do I need Cluster.dev</h2>\n<p>If you have a common infrastructure pattern that contains multiple components stuck together, like a bunch of TF-modules or a set of K8s addons, you need to re-use this pattern inside your projects.\nIf you develop an infrastructure platform that you ship to other teams, and they need to launch new infrastructures from your template.\nIf you build a complex infrastructure that contains different technologies, and you need to perform integration testing to confirm the components' interoperability. After which you can promote the changes to the next environments.\nIf you are a software vendor and you need to deliver infrastructure deployment along with your software.</p>\n<h3 id=\"for-whom\" style=\"position:relative;\"><a href=\"#for-whom\" aria-label=\"for whom permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>For whom</h3>\n<p>The target audience is primarily infrastructure teams with a certain level of expertise, to whom Cluster.dev could simplify their daily routine: creating clusters, deploying services to the clusters, launching environments, sharing experience with development teams, etc.</p>\n<p>üí† <strong>Base concept diagrams</strong></p>\n<p>Stack templates are composed of <a href=\"https://docs.cluster.dev/units-overview/\" target=\"_blank\" rel=\"noopener noreferrer\">units</a> - Lego-like building blocks responsible for passing variables to a particular technology.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABoUlEQVR42pVTTUsCURR9mubX2GimtWgVQthC3LnJqUUL/4A7oZ2LWrZrM7v+g6s+lv4AIQgHIqGFq1CCIMFPyjIUTN+MzNzefY0gBTldOMxlzrmH8+68IeRnGUUXAUUgcO0jUPD84kFxmJzAtQvLuF8hAA5sHwH8b5PJPuJDG+8NppODNqXbyO1AYZlrFxbc+aPpNBd2Op1dlaqAGH+Op8CKTuglcnmoOFFryTCezfqwrdebEqWqUa3V9FJJUXXdAE2bXjDKvhWPR8h70ULC54K4Go1u4lC325M0TYPBcAj9fp8n1HX9CmWHsuy2nDBXyTuxpZTGei+9cqfVKnfb7dtWo/EwGo1OkZNxz1YNCVSXCd+R4mChbDMwE/v3V2ZmxpPLoiEbeGVXAipefj3OjkNEzq2Rk2yEnMsBAk0Pf881isOcssmybF9kjYIlhgDDugkvDpvcv8omCEJYFMUg63GfEQaRwc2wwRCaN00kEmFJkmKLTHHIbyYKzqXDq8KvVSaTwROQZDJ5lEqlbv5MaCYS5/rZbyjM9Tbz6WOnwdWQL7eLywXWHDQFAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"units\" title=\"\" src=\"/static/fcfca4177c21523518ad7da7dcee2417/c5bb3/units.png\" srcset=\"/static/fcfca4177c21523518ad7da7dcee2417/04472/units.png 170w,\n/static/fcfca4177c21523518ad7da7dcee2417/9f933/units.png 340w,\n/static/fcfca4177c21523518ad7da7dcee2417/c5bb3/units.png 680w,\n/static/fcfca4177c21523518ad7da7dcee2417/b12f7/units.png 1020w,\n/static/fcfca4177c21523518ad7da7dcee2417/b5a09/units.png 1360w,\n/static/fcfca4177c21523518ad7da7dcee2417/2cefc/units.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Templates define infrastructure patterns or even the whole platform.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACJElEQVR42s1TP2gTURj/LjHxzkQvuUwFEcRFKmQpJR0MoUTMoBAQT8c4BetWURyPDnYygwouQRwc1BNBUAxp0h4hJTgEKjUoNSbt3buXo9GmL/VQQfD5XmKwg046+MHjvu/7/d59fx/An0WA/0toww+0JEOvJI/ThjLZL0XGSU2ZpG8i3J6ilnKIVMNTTOc258H3lweA1mWg6yJQVhGlwq/SqBFioI85PaCBh3m8oKreITYgjlrgG+CMpzZ0f4L/jAXexRtKmEcC8GuZjIgRnkMmukEImXF33Ntu381gG9/f6m6da79vz5umOZfNZvfxewlq7IHcrCRsL9wSdpbuAvQXHwAp5UHPKTAx4VNVVbJt+6Jtdy5XlpcvvF1be4gxfoIQflQsls9aFio7jvOY8ZRer3d8o+ucniGrYfhUUOHz4lUAd+kUfKmegJbOeqF5hplTIZ1Oh7jebDanNyxr4SMhJ3kFpokKLFg+mUxGmJ5vW+aLK+T1kW/tzvWvlnNnV8ktXrKoJhJBjNCzTWdztV6ojPGeRaPRAP9WisXDHYRXsG0/ZyVzPqi67oVtI4Q6nTNmt3uepcMcvJl8wvSeCO9u7tXWV0I5q6EcrOUk0GclMJjf0ERuc+xaq843gW2Fzo8EtBr+zdqw6VIjCPRVAGiNkQxxqPMz8o+wAR6ED0/3s3vS0VR8LDWdOqbzbP9G1J9rFYvFLsXj8fK/fBYBWZYHQ/wBADkdi7pkRxIAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"infra\" title=\"\" src=\"/static/b2a8795612c0ea58cce80a519382a614/c5bb3/infra.png\" srcset=\"/static/b2a8795612c0ea58cce80a519382a614/04472/infra.png 170w,\n/static/b2a8795612c0ea58cce80a519382a614/9f933/infra.png 340w,\n/static/b2a8795612c0ea58cce80a519382a614/c5bb3/infra.png 680w,\n/static/b2a8795612c0ea58cce80a519382a614/b12f7/infra.png 1020w,\n/static/b2a8795612c0ea58cce80a519382a614/b5a09/infra.png 1360w,\n/static/b2a8795612c0ea58cce80a519382a614/2cefc/infra.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>üî• <strong>Features</strong></p>\n<ul>\n<li>Common variables, secrets, and templating for different technologies.</li>\n<li>Same GitOps Development experience for Terraform, Shell, Kubernetes.</li>\n<li>Could be used with any Cloud, On-premises or Hybrid scenarios.</li>\n<li>Encourage teams to follow technology best practices.</li>\n</ul>\n<p>üìå <strong>Differences between Cluster.dev and Terraform:</strong></p>\n<p>Cluster.dev improves on Terraform in a few key ways:</p>\n<ul>\n<li><strong>Relations:</strong> Terraform struggles with defining relationships between components, but Cluster.dev makes it easier, allowing you to trigger only what's needed.</li>\n<li><strong>Templating:</strong> Unlike Terraform, Cluster.dev supports templating, making it simpler to include or exclude modules like Jenkins Terraform.</li>\n<li><strong>External Tools:</strong> Terraform has limitations when using external tools, while Cluster.dev provides consistent support and integration for better control.</li>\n</ul>\n<p>Overall, Cluster.dev simplifies infrastructure management and addresses some limitations of Terraform.</p>\n<p>‚ùî <strong>Why Infrastructure Templating is Essential:</strong></p>\n<p>In modern cloud-native settings, infrastructure templating plays a crucial role in simplifying the processes of developing and maintaining infrastructure. Here's why it's vital:</p>\n<ul>\n<li>Streamlines infrastructure development by establishing clear functional boundaries.</li>\n<li>Facilitates testing the interoperability of components, even if they employ different technologies.</li>\n<li>Enables the sharing of infrastructure patterns among team members.</li>\n<li>Allows the integration of infrastructure into the product, enhancing overall efficiency.</li>\n<li>Facilitates the division of responsibilities between Platform and SRE teams.</li>\n<li>Supports a comprehensive GitOps approach for all infrastructure components.</li>\n</ul>\n<p>Now, let's explore the primary use cases of infrastructure templating and examine the benefits it brings.</p>\n<p>Templating significantly simplifies tasks for DevOps and SRE teams, especially in deploying and testing within complex environments. So, what comprises infrastructure templates? Let's explore the essential building blocks necessary for creating an infrastructure template, starting with infrastructure layering:</p>\n<ul>\n<li><strong>Networking Layer:</strong> Involves VPCs, Peerings, VPNs, Security Groups, and Routing.</li>\n<li><strong>Permissions Layer:</strong> Encompasses IAM roles and user policies.</li>\n<li><strong>Infra and OS Layer:</strong> Encompasses server instance provisioning and operating system settings.</li>\n<li><strong>Data Management Layer:</strong> Includes Relational and NoSQL databases, caches, file and object storages.</li>\n<li><strong>Application Layer:</strong> Involves container orchestration, continuous delivery of applications, business applications and workloads, and infrastructure applications.</li>\n<li><strong>Observability Layer:</strong> Encompasses metric logs, tracing applications, and storages.</li>\n<li><strong>Configuration Layer:</strong> Involves declarative configuration storage, infrastructure state storages, and secret storages.</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABPElEQVR42j2RCW7DIBBFff/TVW3VJLYx2Aa8gNcqm5TkdZy0RRohGOZvJDFGnHfEbkCnBm00lfEUhSLftWRpwTiNLMuCc5a2CezfHabUlMaiVEFXG9L9jiBYyQbovadrenRWPgGNrqiqirJoBNgwzRPTNGOtFVAv954QguyWQmuZ9ULUCPFEMgwDbdfStz3qoOVBQZ7noqAUAMd+f2AcXwo34s1NHzpWOZtclKWHp7L1e2VdV5KNqbY1vvZiUaHEqhHmLM1Ic0VV18zz/LLsLd41qJ0WoIyvN4fKCynD58cXnQhLLpcLp9OJ81n241nUDNLoiXFgjIFBaplWYoj0TaTzvdxF6Qe6NnI8/s6ezlyvV5L7/c7j8fivLYItq7hFUZU4CdxX8jmHDJNZcSHnxmKlZyTv2+3Gtv7mfwDjE711kwro9gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"layers\" title=\"\" src=\"/static/2fac83b032385e56f487120cb1fa55a2/c5bb3/layers.png\" srcset=\"/static/2fac83b032385e56f487120cb1fa55a2/04472/layers.png 170w,\n/static/2fac83b032385e56f487120cb1fa55a2/9f933/layers.png 340w,\n/static/2fac83b032385e56f487120cb1fa55a2/c5bb3/layers.png 680w,\n/static/2fac83b032385e56f487120cb1fa55a2/b12f7/layers.png 1020w,\n/static/2fac83b032385e56f487120cb1fa55a2/2bef9/layers.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>üìö <strong>Conclusion:</strong></p>\n<p>Cluster.dev is a great tool that simplifies the management of cloud-native infrastructures. It uses simple declarative manifests, or stack templates, to enable users to describe and deploy entire infrastructures with a single tool. The platform's support for various technologies, powerful templating engine, GitOps-first approach, and cross-platform compatibility make it a versatile and adaptable solution for managing complex infrastructures. By consolidating previously disconnected infrastructure components under one roof, Cluster.dev enables efficient, one-shot deployment of entire stacks, saving time and effort on operating tasks and allowing teams to focus on code development.</p>\n<p>If you found this article helpful, please don't forget to hit the <strong>Follow</strong> üëâ and <strong>Clap</strong> üëè buttons to help me write more articles like this.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/prometheus-manage-memory-overload/","title":"Prometheus Restart Troubles: Managing Memory Overload üß†","date":"2024-10-29 19:00:00"},"excerpt":"When Prometheus Can't Keep Up with the WAL üìà üìó Introduction Have you ever had Prometheus crash due to an out-of-memory error while trying‚Ä¶","html":"<blockquote>\n<p><strong>When Prometheus Can't Keep Up with the WAL üìà</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìó Introduction</h2>\n<p>Have you ever had Prometheus crash due to an out-of-memory error while trying to catch up by reading the Write-Ahead Log (WAL)? It's a frustrating problem, but let's check it out.</p>\n<p>Prometheus is great for monitoring, but when it restarts, it needs to process data from the Write-Ahead Log (WAL), which can be memory-intensive. This often leads to OOMKilled crashes, especially if Prometheus is already running close to its memory limits.</p>\n<p>The issue usually stems from either collecting too much data or running too close to memory limits. This has been a long-standing concern, with <a href=\"https://github.com/prometheus/prometheus/issues/6934\" target=\"_blank\" rel=\"noopener noreferrer\">reports dating back to an open issue since 2020 on GitHub</a>, highlighting the significant challenges faced by users in managing Prometheus' memory during WAL replay.</p>\n<h3 id=\"Ô∏è-the-problem\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-the-problem\" aria-label=\"Ô∏è the problem permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ÅâÔ∏è The Problem</h3>\n<br>\n<div style=\"width:100%;height:0;padding-bottom:100%;position:relative;\"><iframe src=\"https://giphy.com/embed/ka55CqnDNjQ7iIKtRa\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameborder=\"0\" class=\"giphy-embed\" allowfullscreen></iframe></div>\n<p>While upgrading <a href=\"https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack\" target=\"_blank\" rel=\"noopener noreferrer\">the Helm chart of kube-prometheus-stack</a>, we noticed several pods were not ready, including <code class=\"language-text\">prometheus-prometheus-operator-prometheus-0</code>, which showed a status of <code class=\"language-text\">3/4 Running</code> with a recent termination due to being OOMKilled.</p>\n<p>The logs revealed the root cause:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">ts</span><span class=\"token operator\">=</span><span class=\"token number\">2024</span>‚Äì06‚Äì14T09:49:10.383Z <span class=\"token assign-left variable\">caller</span><span class=\"token operator\">=</span>head.go:840 <span class=\"token assign-left variable\">level</span><span class=\"token operator\">=</span>info <span class=\"token assign-left variable\">component</span><span class=\"token operator\">=</span>tsdb <span class=\"token assign-left variable\">msg</span><span class=\"token operator\">=</span><span class=\"token string\">\"Deletion of corrupted mmap chunk files failed, discarding chunk files completely\"</span> <span class=\"token assign-left variable\">err</span><span class=\"token operator\">=</span><span class=\"token string\">\"cannot handle error: iterate on on-disk chunks: out of sequence m-mapped chunk for series ref 946594555, last chunk: [1718071739971, 1718075459971], new: [1718049629971, 1718053199971]\"</span></code></pre></div>\n<p>It seems like it's stuck in the running state, where the pod is not yet ready. Let's describe the pod to see what is wrong:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">State:       Running\n    Started:   Tue, <span class=\"token number\">14</span> Jun <span class=\"token number\">2024</span> <span class=\"token number\">10</span>:04:03 +0200\nLast State:  Terminated\n    Reason:    OOMKilled</code></pre></div>\n<p>Ah, there it is. Prometheus is indeed running, but it got terminated due to an OOMKill‚Ää‚Äî‚Äärunning out of memory. It seems Prometheus is in the midst of recovering from the Write Ahead Log (WAL), which might be causing the memory spike. This could stem from an error during recovery or a restart, where Prometheus doesn't have enough memory to write everything into the WAL.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 403px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAETUlEQVR42lVTWW8aVxid31apUp+aVKrUxk0fWqlK0lRVWvUhSZc0adLadb3U2Klt8LAMBowNwQYBJoAxywwwMCtzh2UAD8NObIMhTmwH04sfKlW6m66+c893vns+xL+zwEaXM4QaJFGQ1P5vkGj26iCSOpGEq1aIr2WiqICjEoP5HNMIG9VJNFritbWcuSqaq9n/hukAmIvAUs8aK8AgC+sSbfDvPnLHvrI5brcKW3TkOZIOa5moCn85SYXnyzyMM8uCqSIYOdrqTcf96VhecNVKESZhY0IrVu+tsHJPbZuogU1ybxFJ76PxwFzY+0cyNFdkdZAB4pWMIciTLl52ChUyR9fy20XhBeHTW713QXsRtX5dF62p0BJChjTEyymn6X7Q+ThPaSqiWR6D9QEGt+R7dqAAztGRo61aCsQN645nWGh1zfigU7CSEJzc0+TSmnm9ymRfbuUtMGdIroibnVc1tnZMF6TGQazAb6fCagZXU+E1PLCSCK6ClCERUCEcYUgldfdMrgWvp1nYgDmXeX2nSo1Gozf9djW3XQFYKqLJ8JTbZcP0i3tBr8ezHfTvBJzTiFf/NIA9fjI/Oz37O74zPS6yYD17078cjaqSv8yh1ayFwbVbG+qd7a3nS/Mej1On1Ths60HXX8jPE9d/+ezaTzc//O76+8aZHxoFU1PGIe3rflsGJuXq84Skwe/REft29w5KJd1+N5aIbMZ8fyPffPrBszsTz+7c+PXLT14s/wgFdxq5i3fDdj1bYrAyi5U5Y5nDJFqdJZdLrDaXWilQGgVgYfcs4rXP0KElLrrKRlYFYi2X1ucoY5bayKUNILGSTVvEFCaSRinjLYFAZuw2vZDQllhs1z6J8HFjiUNlYKjnLTBDRTTJHHqoxA/rVLuCvz3tvBm0z89OoJDhu7Oz087ZaXvQq799raQjqwgT1af352L+P+noYhVgLG0LUni1WRtHX8KqjeA8P393tY/qjVa704GHi7MuEViEYB0RmAl5pqiwKkdju1R6l0jhjHA5HELAxfk5EAGO48ViMRKJuJxOnucg+LBBJ/zzSGofjfmmHMb7UfeTOGEM0lyOjWeAMByOqQaDgc1mU6lUdrsNPnF0dAwvj9pSBayToedjb4ukelIzZ3f90yz7WTpFhpwNpQyDqlUlmUz6fD7I2e/3xxKGF92jAymz1ShskHtLiJAwRKLot2bvosvRaeYIUZAKAoyjGQbCeJ4/OeldKb8c9FrHbakkbMECN6XNMThonvQbnz6cnsE21497h7Gi1DvpHh225YNit9u9HMJSjd6edg/y3gKjL3D2smCFXmjAxoAt+ejzj3774uPvr73nt5ub/RNZkbvHR/VatdlQBqevAZdu1cu10l6RXauKJmh7aBjYPBCcgpof3r65cP/u01s3WJIINVvFfLjAWmsVut9t9Lqv5DzOESslzqAAS0UYu7UCTAeZdZh2dHf+X8slnBOXkWHmAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cardinality\" title=\"\" src=\"/static/4457980b0539bcef67ad94fef9940f92/045fd/cardinality.png\" srcset=\"/static/4457980b0539bcef67ad94fef9940f92/04472/cardinality.png 170w,\n/static/4457980b0539bcef67ad94fef9940f92/9f933/cardinality.png 340w,\n/static/4457980b0539bcef67ad94fef9940f92/045fd/cardinality.png 403w\" sizes=\"(max-width: 403px) 100vw, 403px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>One potential solution could be allocating more memory to Prometheus and analyzing why the WAL is getting clogged up. Essentially, we need to investigate what changed to suddenly cause this spike in memory usage in our once serene environment.</p>\n<p>The issue persisted due to the WAL replay process requiring 2‚Äì3 times more memory than the running Prometheus instance. Despite running smoothly with around 30Gi of memory usage, the WAL replay process demanded over 50+Gi, ultimately leading to OOMKilled crashes during startup. Simply increasing the RAM limit wasn't a viable solution, as the excessive memory usage occurred specifically during the replay phase.</p>\n<p>This problem aligns with a longstanding issue on GitHub since 2020, where users have consistently reported challenges managing Prometheus' memory during WAL replay. We encountered similar difficulties during our upgrade process, highlighting the critical need for resolution.</p>\n<h3 id=\"-understanding-memory-overheads-in-wal-replay\" style=\"position:relative;\"><a href=\"#-understanding-memory-overheads-in-wal-replay\" aria-label=\" understanding memory overheads in wal replay permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí° Understanding Memory Overheads in WAL Replay</h3>\n<p>In the past, WAL replay often caused significant overhead, leading to unexpected out-of-memory (OOM) situations. For instance, if your Prometheus was already running at 70% of its memory limit and the overhead during replay surged by 300%, it could easily lead to crashes. Additionally, increased CPU usage during replay, especially in low CPU environments like Kubernetes, could slow down processes like garbage collection, resulting in slower memory release.</p>\n<p>However, recent benchmarks of Prometheus versions at Google show a different picture. While there's been a noticeable 2x increase in CPU usage during replay, the memory overhead, including heap and working sets, is only around 1‚Äì5%. This raises the question: are the reported OOM issues symptoms of a larger problem, with the replay OOM merely surfacing it?</p>\n<p>Currently, two prevalent scenarios appear:</p>\n<ol>\n<li><strong>Excessive Data Collection</strong>: If your Prometheus setup scrapes too many series or samples, it's prone to OOM crashes during replay, regardless of the memory overhead.</li>\n<li><strong>Running Close to Memory Limits</strong>: Even a slight overhead during replay can trigger OOM crashes if Prometheus is already running near its memory limit, such as at 95%.</li>\n</ol>\n<p>These issues often revolve around cardinality‚Ää‚Äî‚Ääthe combination of all label values per metric. High cardinality metrics, like those tracking multiple URLs or response codes, can quickly escalate memory usage. In short, much of Prometheus' memory woes can be attributed to cardinality.</p>\n<h3 id=\"-how-does-remote-write-work\" style=\"position:relative;\"><a href=\"#-how-does-remote-write-work\" aria-label=\" how does remote write work permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ How Does Remote Write Work?</h3>\n<p>The remote write reads data from Prometheus' <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\" rel=\"noopener noreferrer\">write ahead log</a>.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 66.47058823529413%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB9ElEQVR42o1T7XLTMBD0Q+ZdYKZ9nfKfJ+B38g+GklJIGju2ZTfEH7Ily7Kk5STXAYYwxTM3kke6vb3dU4QrnzEOVavQtD2OaY40Y4iPGXJWousHcDHiX190FdBapM8SScqQpgnatoVSA4qCoa4r9FK9DuicCxEA9USJHE3D0XKBXqgQQo7oOglBLJ2ZKMm+ztAZAys6sCxFkzxBJd8xHL5B5QmG4x6m47B6hB0krBR/Ay6smqYhrbJQWcseGSvw43SCnXQoEsKaSxe0IdAZ0L3EhaG/xDkHK4oAoKnyicDquoYlPX0YAlzuWtqbaZoB6QzEmHSCo9zIC34+nwOg9dWJoR2HUHEB8+DhnPYTARVUuK6q0IkiCZ7zDKws0Sc7RJ6FT/Ch6TK8huToROuk9UWOvqdkRcYIMf/zFkZJGAKtH+9R7b5CPn5C5A+32y3iOH5haMJFX6jvOnK1Q0UdeCBflDEW9HOLhn4VxL7nMGRm5FtIknnWZpcnGHJQEptxHEOLilZNbJ2dx0SfS+jdl+D2f42NrzRR214fvwYQApTE1psjHz5CfHgfDMQfPrtfLl/GwScTSLzf4VQWeLj/jJu3b/Du7i6MUZbEKFke7uC3nKsMF9BhGLDebHA4xFivN1itVri5vaVnmGK/f6LYB4OuPb2ffzTryGFZsqsAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"wal\" title=\"\" src=\"/static/ae8f0b6c864a690e0f78a8be41523357/c5bb3/wal.png\" srcset=\"/static/ae8f0b6c864a690e0f78a8be41523357/04472/wal.png 170w,\n/static/ae8f0b6c864a690e0f78a8be41523357/9f933/wal.png 340w,\n/static/ae8f0b6c864a690e0f78a8be41523357/c5bb3/wal.png 680w,\n/static/ae8f0b6c864a690e0f78a8be41523357/b12f7/wal.png 1020w,\n/static/ae8f0b6c864a690e0f78a8be41523357/d9b5d/wal.png 1224w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Data generated by scrape is written to the WAL, so this essentially gives us a 2- to 3-hour buffer on disk of data for remote write. The RW system now has a subroutine that reads the WAL and passes the data back to remote write.</p>\n<p>Remote write still has a small in-memory buffer, and the routine reading the WAL pauses where it is if it's not able to append new data to the buffer. This means we no longer drop data if the buffer is full, and the buffer doesn't need to be large enough to handle a longer outage.</p>\n<p>As long as the remote endpoint isn't down for hours, remote write no longer loses data (with some caveats, like Prometheus restarts), since the WAL is truncated every two hours or so.</p>\n<h3 id=\"Ô∏è-issue-resolutions\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-issue-resolutions\" aria-label=\"Ô∏è issue resolutions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Issue Resolutions</h3>\n<p>If you've never experienced this issue before (lucky you!), here's a handy solution I found effective. Since Prometheus may not be up and running to utilize PromQL for detecting potential issues, we need an alternative method to identify high cardinality. One approach is to get hands-on with some kubectl exec magic:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">kubectl <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token parameter variable\">-n</span> monitoring pods/prometheus-prometheus-kube-prometheus-prometheus-0 -- <span class=\"token function\">sh</span></code></pre></div>\n<p>Then, run the Prometheus TSDB analysis:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">/prometheus $ promtool tsdb analyze <span class=\"token builtin class-name\">.</span></code></pre></div>\n<p>This analysis will provide insights into metrics with high cardinality, like <code class=\"language-text\">haproxy_server_http_responses_total</code>, which might be causing memory issues. In such cases, updating or optimizing the problematic metric, such as haproxy, can alleviate memory strain.</p>\n<p>Alternatively, consider increasing Prometheus' memory allocation or deploying it to a specific node group with ample memory resources.</p>\n<p>Here are some additional strategies to mitigate memory overhead and OOM crashes:</p>\n<ul>\n<li><strong>Verify Memory Overhead</strong>: Ensure that the memory overhead during replay is within acceptable limits (e.g., 10‚Äì15%). Running Prometheus close to its memory limit is risky due to dynamic garbage collection and limited room for unexpected cardinality spikes or queries.</li>\n<li><strong>Optimize Storage and Scraping</strong>: Regularly optimize Prometheus' storage, scraping, and remote write configurations to reduce memory usage. Upgrading to newer releases can often provide optimizations in this regard.</li>\n<li><strong>Automate Recovery from OOM</strong>: Implement auto-recovery mechanisms to handle OOM crash loops, such as automatically deleting the Write-Ahead Log (WAL) on OOM events. This ensures smoother recovery from memory-related issues.</li>\n<li><strong>Implement Scraping Limits</strong>: Consider introducing forceful scrape limits to prevent Prometheus from scraping targets when memory usage exceeds a certain threshold. This proactive approach can help avoid memory-intensive situations and potential OOM crashes.</li>\n</ul>\n<p>By implementing these strategies, you can effectively manage Prometheus' memory challenges and ensure smooth operation in your monitoring environment.</p>\n<h3 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h3>\n<p>Dealing with Prometheus OOM errors during WAL replay can be challenging. By understanding the root causes, such as excessive data collection and high cardinality metrics, and implementing solutions like optimizing storage and scraping configurations, increasing memory allocations, and setting up auto-recovery mechanisms, you can mitigate these issues.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}