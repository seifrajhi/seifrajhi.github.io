{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/prometheus-manage-memory-overload/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>When Prometheus Can't Keep Up with the WAL üìà</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìó Introduction</h2>\n<p>Have you ever had Prometheus crash due to an out-of-memory error while trying to catch up by reading the Write-Ahead Log (WAL)? It's a frustrating problem, but let's check it out.</p>\n<p>Prometheus is great for monitoring, but when it restarts, it needs to process data from the Write-Ahead Log (WAL), which can be memory-intensive. This often leads to OOMKilled crashes, especially if Prometheus is already running close to its memory limits.</p>\n<p>The issue usually stems from either collecting too much data or running too close to memory limits. This has been a long-standing concern, with <a href=\"https://github.com/prometheus/prometheus/issues/6934\" target=\"_blank\" rel=\"noopener noreferrer\">reports dating back to an open issue since 2020 on GitHub</a>, highlighting the significant challenges faced by users in managing Prometheus' memory during WAL replay.</p>\n<h3 id=\"Ô∏è-the-problem\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-the-problem\" aria-label=\"Ô∏è the problem permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ÅâÔ∏è The Problem</h3>\n<br>\n<div style=\"width:100%;height:0;padding-bottom:100%;position:relative;\"><iframe src=\"https://giphy.com/embed/ka55CqnDNjQ7iIKtRa\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameborder=\"0\" class=\"giphy-embed\" allowfullscreen></iframe></div>\n<p>While upgrading <a href=\"https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack\" target=\"_blank\" rel=\"noopener noreferrer\">the Helm chart of kube-prometheus-stack</a>, we noticed several pods were not ready, including <code class=\"language-text\">prometheus-prometheus-operator-prometheus-0</code>, which showed a status of <code class=\"language-text\">3/4 Running</code> with a recent termination due to being OOMKilled.</p>\n<p>The logs revealed the root cause:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token assign-left variable\">ts</span><span class=\"token operator\">=</span><span class=\"token number\">2024</span>‚Äì06‚Äì14T09:49:10.383Z <span class=\"token assign-left variable\">caller</span><span class=\"token operator\">=</span>head.go:840 <span class=\"token assign-left variable\">level</span><span class=\"token operator\">=</span>info <span class=\"token assign-left variable\">component</span><span class=\"token operator\">=</span>tsdb <span class=\"token assign-left variable\">msg</span><span class=\"token operator\">=</span><span class=\"token string\">\"Deletion of corrupted mmap chunk files failed, discarding chunk files completely\"</span> <span class=\"token assign-left variable\">err</span><span class=\"token operator\">=</span><span class=\"token string\">\"cannot handle error: iterate on on-disk chunks: out of sequence m-mapped chunk for series ref 946594555, last chunk: [1718071739971, 1718075459971], new: [1718049629971, 1718053199971]\"</span></code></pre></div>\n<p>It seems like it's stuck in the running state, where the pod is not yet ready. Let's describe the pod to see what is wrong:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">State:       Running\n    Started:   Tue, <span class=\"token number\">14</span> Jun <span class=\"token number\">2024</span> <span class=\"token number\">10</span>:04:03 +0200\nLast State:  Terminated\n    Reason:    OOMKilled</code></pre></div>\n<p>Ah, there it is. Prometheus is indeed running, but it got terminated due to an OOMKill‚Ää‚Äî‚Äärunning out of memory. It seems Prometheus is in the midst of recovering from the Write Ahead Log (WAL), which might be causing the memory spike. This could stem from an error during recovery or a restart, where Prometheus doesn't have enough memory to write everything into the WAL.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 403px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAETklEQVR42j1UWW8aVxSe39Y+9KWLKlVtErUPkaomVaSmah8aJVGjpG6b2E2Mi+M4xgxmWMbBNg7YMUsAwwAzhmEWZmUblmHYHcxix3GITS+21Kuro6tP59O53znfvVDArU/HF4U9g5SEZdIkJU0gXhwkEs4kL5CVC1AgjHzMKOCwkrb4X85A6ZhZYUwl3qRlUU1Gtcz/216WUEV8Uc/YVAmpiLYCYw74ft/Bv99wXW/lNxhsAaIwExPT468f0piuLFirMqpK9qpk5ViHj9oLUPGcuFUrYmzCyYafO7w/YOrPBueVuryWCs9DVAROBGejnr/J8KySXgEVVAmtCkiIJ7f4iltUySxby7kU0Un4zQ7vDamthx0/1mVHKvwUIsNG4vW0234rtHU/RxtVGa1IqCqYQyz+ItffkKoy7+pUYq1aSkpYbK4/rZEl2Ha7U1gjATm5a8xRyzpEb99cbOVWK6JdlWyq7Gh3amytR+cLjTJeENxU1MARBjq6jAefJ3eX5BSSCOohfs9Cpcw30Vf/+rytggPcucyvdDRqPB4fDzta1g30p6LLPE/vbG9YzfO7Qa/X4woF3KGtGchrngpa7j/QzU4/+Svunp40WXScvBuejcda4XWJg2vZVRY3ra0uuV+uP53X7Xi2TPDy5oZ1d/sf6M6lz+9e/vzulU9vfvaxZebXRt7eVHFQ9nDQBhKq58MTkkhgZ4XAnDtumEnuBDzWZHQt7p+DbnzzydT1y1PXLt27+vXmszsVydZp5EYfzjp1ucgipTRS4ixlDlGYpQy5WEqbsuRigV7WJEvU8wTyOh+zkQU+ZuBiBnHPmKVWMrQ1Q6HZlFlKPJdJVCYRmbQUeE9RDAjnbhMTsMIiXudDSEjagL1U2drIO4A8TbarnPFNlXhTp1tq/OTt/vFh5/3JAAg5Ox2dHO+/P24fDeonRxodNUBsDKEiOjwwzcTmNcmaZpxBmtCatUn2GejaZI1Gp2fjybnRaLXb+xPkpE8E5wF5hQg+jnge0Zg+wyA+ivYQVJwVx+fM0WgkSRKO44qiYBjmdrt4ngP4mwaTCOigVASO+x+5LL/FXj0gcEuI4XPcniCJp6cT8tHR0fr6+tzcHIh4HO8eHACw28mrYPjhBYjGYIk0PDLOOreftUoBlkmR4a1GtQSSNE1LJpN+vz8ajQ6HwwvZvW6lIGw08i/I3aeQlECwGPwT6tVvb3aaOUIWlbwE8hiW9fn8HMcPBoML/cN+66BdKIrrRQ5uAm8Dcgh9GLBN3Z55bFmzdfvduFLoD/oH3bZaKfZ7vbPTD4D57m2vnPPmWXOec5ZEB3h5YDQT8r3vvvzj6le/fPFRwIk2h4NKtXJw0K3VtGZdPXx7JHFUu16qFXeVtBF8FSXeDDwDnAfIqcgCdPvat3O3bkxdu5wmiXCzpeSwfNpRU5nDXmPQ2y/ncA5fLPJIVV5Vz90KvoqyYAPXjvl0/wFaJpw38r5EpwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cardinality\" title=\"\" src=\"/static/4457980b0539bcef67ad94fef9940f92/045fd/cardinality.png\" srcset=\"/static/4457980b0539bcef67ad94fef9940f92/04472/cardinality.png 170w,\n/static/4457980b0539bcef67ad94fef9940f92/9f933/cardinality.png 340w,\n/static/4457980b0539bcef67ad94fef9940f92/045fd/cardinality.png 403w\" sizes=\"(max-width: 403px) 100vw, 403px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>One potential solution could be allocating more memory to Prometheus and analyzing why the WAL is getting clogged up. Essentially, we need to investigate what changed to suddenly cause this spike in memory usage in our once serene environment.</p>\n<p>The issue persisted due to the WAL replay process requiring 2‚Äì3 times more memory than the running Prometheus instance. Despite running smoothly with around 30Gi of memory usage, the WAL replay process demanded over 50+Gi, ultimately leading to OOMKilled crashes during startup. Simply increasing the RAM limit wasn't a viable solution, as the excessive memory usage occurred specifically during the replay phase.</p>\n<p>This problem aligns with a longstanding issue on GitHub since 2020, where users have consistently reported challenges managing Prometheus' memory during WAL replay. We encountered similar difficulties during our upgrade process, highlighting the critical need for resolution.</p>\n<h3 id=\"-understanding-memory-overheads-in-wal-replay\" style=\"position:relative;\"><a href=\"#-understanding-memory-overheads-in-wal-replay\" aria-label=\" understanding memory overheads in wal replay permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí° Understanding Memory Overheads in WAL Replay</h3>\n<p>In the past, WAL replay often caused significant overhead, leading to unexpected out-of-memory (OOM) situations. For instance, if your Prometheus was already running at 70% of its memory limit and the overhead during replay surged by 300%, it could easily lead to crashes. Additionally, increased CPU usage during replay, especially in low CPU environments like Kubernetes, could slow down processes like garbage collection, resulting in slower memory release.</p>\n<p>However, recent benchmarks of Prometheus versions at Google show a different picture. While there's been a noticeable 2x increase in CPU usage during replay, the memory overhead, including heap and working sets, is only around 1‚Äì5%. This raises the question: are the reported OOM issues symptoms of a larger problem, with the replay OOM merely surfacing it?</p>\n<p>Currently, two prevalent scenarios appear:</p>\n<ol>\n<li><strong>Excessive Data Collection</strong>: If your Prometheus setup scrapes too many series or samples, it's prone to OOM crashes during replay, regardless of the memory overhead.</li>\n<li><strong>Running Close to Memory Limits</strong>: Even a slight overhead during replay can trigger OOM crashes if Prometheus is already running near its memory limit, such as at 95%.</li>\n</ol>\n<p>These issues often revolve around cardinality‚Ää‚Äî‚Ääthe combination of all label values per metric. High cardinality metrics, like those tracking multiple URLs or response codes, can quickly escalate memory usage. In short, much of Prometheus' memory woes can be attributed to cardinality.</p>\n<h3 id=\"-how-does-remote-write-work\" style=\"position:relative;\"><a href=\"#-how-does-remote-write-work\" aria-label=\" how does remote write work permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÑ How Does Remote Write Work?</h3>\n<p>The remote write reads data from Prometheus' <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\" rel=\"noopener noreferrer\">write ahead log</a>.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 66.47058823529413%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB5klEQVR42o1TW27bMBDULX2XFvB10u9eoCdwPwLUhtPWgWS9bb1JihJJTXepGE0BJw0BYiVod3ZmZxXgzrF2Qd1ptJ3AOckQpznCc4I0KzGIEYOc8NYJ7gI6h7hUiOIMSRyh6zpoPSLPMzRNDaH0xwCXZVkBjUHb9OjaAX0vIaT2V6oJYlCQxHKxhgrc/xku1sLJAVkSo41O0NEvjOFP6CyCjp9hRQ83T3CjglPybUCWFSeJ7zwrgTTPcb1c4MwMOEtkrI94UcHRjX8Bl9eALLXve2QEspBcQ51LAmuaBo7mydcS81vu7d0D0jMMmeQM1c4ImFlVVR7QcXdi6KbRd7yBlWXpFfCzoYY5NW7q2isZhw5lGiMtCggaT8AsuOBCjIyxvDNw5KihaObZs2rbFkIITNNExij/PjAByuM5Nk8/UP8+Qj09IuCPh8MBYRiuLhOQ1QrNC8gwDKiJjZTSN82y7N8ZUlxk768jxgFLYDAGXl02sOSg0tozYqY+Ut7i1jUxVYH5tPduf3BtBLEcoWlFOPKZSb4ixmyGOj5Cffu6bsA9QJZ6W2pfTCDR8wlVWeC432P7+RO+PDygul6RniMUJJtz8Krm3T9Fk9zdboc4Tih+x2azwXa79e5G0dmPiHPu/Xp/ADqn7Ii5/yOKAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"wal\" title=\"\" src=\"/static/ae8f0b6c864a690e0f78a8be41523357/c5bb3/wal.png\" srcset=\"/static/ae8f0b6c864a690e0f78a8be41523357/04472/wal.png 170w,\n/static/ae8f0b6c864a690e0f78a8be41523357/9f933/wal.png 340w,\n/static/ae8f0b6c864a690e0f78a8be41523357/c5bb3/wal.png 680w,\n/static/ae8f0b6c864a690e0f78a8be41523357/b12f7/wal.png 1020w,\n/static/ae8f0b6c864a690e0f78a8be41523357/d9b5d/wal.png 1224w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Data generated by scrape is written to the WAL, so this essentially gives us a 2- to 3-hour buffer on disk of data for remote write. The RW system now has a subroutine that reads the WAL and passes the data back to remote write.</p>\n<p>Remote write still has a small in-memory buffer, and the routine reading the WAL pauses where it is if it's not able to append new data to the buffer. This means we no longer drop data if the buffer is full, and the buffer doesn't need to be large enough to handle a longer outage.</p>\n<p>As long as the remote endpoint isn't down for hours, remote write no longer loses data (with some caveats, like Prometheus restarts), since the WAL is truncated every two hours or so.</p>\n<h3 id=\"Ô∏è-issue-resolutions\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-issue-resolutions\" aria-label=\"Ô∏è issue resolutions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Issue Resolutions</h3>\n<p>If you've never experienced this issue before (lucky you!), here's a handy solution I found effective. Since Prometheus may not be up and running to utilize PromQL for detecting potential issues, we need an alternative method to identify high cardinality. One approach is to get hands-on with some kubectl exec magic:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">kubectl <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token parameter variable\">-n</span> monitoring pods/prometheus-prometheus-kube-prometheus-prometheus-0 -- <span class=\"token function\">sh</span></code></pre></div>\n<p>Then, run the Prometheus TSDB analysis:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">/prometheus $ promtool tsdb analyze <span class=\"token builtin class-name\">.</span></code></pre></div>\n<p>This analysis will provide insights into metrics with high cardinality, like <code class=\"language-text\">haproxy_server_http_responses_total</code>, which might be causing memory issues. In such cases, updating or optimizing the problematic metric, such as haproxy, can alleviate memory strain.</p>\n<p>Alternatively, consider increasing Prometheus' memory allocation or deploying it to a specific node group with ample memory resources.</p>\n<p>Here are some additional strategies to mitigate memory overhead and OOM crashes:</p>\n<ul>\n<li><strong>Verify Memory Overhead</strong>: Ensure that the memory overhead during replay is within acceptable limits (e.g., 10‚Äì15%). Running Prometheus close to its memory limit is risky due to dynamic garbage collection and limited room for unexpected cardinality spikes or queries.</li>\n<li><strong>Optimize Storage and Scraping</strong>: Regularly optimize Prometheus' storage, scraping, and remote write configurations to reduce memory usage. Upgrading to newer releases can often provide optimizations in this regard.</li>\n<li><strong>Automate Recovery from OOM</strong>: Implement auto-recovery mechanisms to handle OOM crash loops, such as automatically deleting the Write-Ahead Log (WAL) on OOM events. This ensures smoother recovery from memory-related issues.</li>\n<li><strong>Implement Scraping Limits</strong>: Consider introducing forceful scrape limits to prevent Prometheus from scraping targets when memory usage exceeds a certain threshold. This proactive approach can help avoid memory-intensive situations and potential OOM crashes.</li>\n</ul>\n<p>By implementing these strategies, you can effectively manage Prometheus' memory challenges and ensure smooth operation in your monitoring environment.</p>\n<h3 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h3>\n<p>Dealing with Prometheus OOM errors during WAL replay can be challenging. By understanding the root causes, such as excessive data collection and high cardinality metrics, and implementing solutions like optimizing storage and scraping configurations, increasing memory allocations, and setting up auto-recovery mechanisms, you can mitigate these issues.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":5,"rawMarkdownBody":"\n> **When Prometheus Can't Keep Up with the WAL üìà**\n\n## üìó Introduction\n\nHave you ever had Prometheus crash due to an out-of-memory error while trying to catch up by reading the Write-Ahead Log (WAL)? It's a frustrating problem, but let's check it out.\n\nPrometheus is great for monitoring, but when it restarts, it needs to process data from the Write-Ahead Log (WAL), which can be memory-intensive. This often leads to OOMKilled crashes, especially if Prometheus is already running close to its memory limits.\n\nThe issue usually stems from either collecting too much data or running too close to memory limits. This has been a long-standing concern, with [reports dating back to an open issue since 2020 on GitHub](https://github.com/prometheus/prometheus/issues/6934), highlighting the significant challenges faced by users in managing Prometheus' memory during WAL replay.\n\n### ‚ÅâÔ∏è The Problem\n\n<br>\n\nhttps://giphy.com/gifs/strangerthings-netflix-stranger-things-ka55CqnDNjQ7iIKtRa\n\nWhile upgrading [the Helm chart of kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack), we noticed several pods were not ready, including `prometheus-prometheus-operator-prometheus-0`, which showed a status of `3/4 Running` with a recent termination due to being OOMKilled.\n\nThe logs revealed the root cause:\n\n```shell\nts=2024‚Äì06‚Äì14T09:49:10.383Z caller=head.go:840 level=info component=tsdb msg=\"Deletion of corrupted mmap chunk files failed, discarding chunk files completely\" err=\"cannot handle error: iterate on on-disk chunks: out of sequence m-mapped chunk for series ref 946594555, last chunk: [1718071739971, 1718075459971], new: [1718049629971, 1718053199971]\"\n```\n\nIt seems like it's stuck in the running state, where the pod is not yet ready. Let's describe the pod to see what is wrong:\n\n```shell\nState:       Running\n    Started:   Tue, 14 Jun 2024 10:04:03 +0200\nLast State:  Terminated\n    Reason:    OOMKilled\n```\n\nAh, there it is. Prometheus is indeed running, but it got terminated due to an OOMKill‚Ää‚Äî‚Äärunning out of memory. It seems Prometheus is in the midst of recovering from the Write Ahead Log (WAL), which might be causing the memory spike. This could stem from an error during recovery or a restart, where Prometheus doesn't have enough memory to write everything into the WAL.\n\n![cardinality](./cardinality.png)\n\nOne potential solution could be allocating more memory to Prometheus and analyzing why the WAL is getting clogged up. Essentially, we need to investigate what changed to suddenly cause this spike in memory usage in our once serene environment.\n\nThe issue persisted due to the WAL replay process requiring 2‚Äì3 times more memory than the running Prometheus instance. Despite running smoothly with around 30Gi of memory usage, the WAL replay process demanded over 50+Gi, ultimately leading to OOMKilled crashes during startup. Simply increasing the RAM limit wasn't a viable solution, as the excessive memory usage occurred specifically during the replay phase.\n\nThis problem aligns with a longstanding issue on GitHub since 2020, where users have consistently reported challenges managing Prometheus' memory during WAL replay. We encountered similar difficulties during our upgrade process, highlighting the critical need for resolution.\n\n### üí° Understanding Memory Overheads in WAL Replay\n\nIn the past, WAL replay often caused significant overhead, leading to unexpected out-of-memory (OOM) situations. For instance, if your Prometheus was already running at 70% of its memory limit and the overhead during replay surged by 300%, it could easily lead to crashes. Additionally, increased CPU usage during replay, especially in low CPU environments like Kubernetes, could slow down processes like garbage collection, resulting in slower memory release.\n\nHowever, recent benchmarks of Prometheus versions at Google show a different picture. While there's been a noticeable 2x increase in CPU usage during replay, the memory overhead, including heap and working sets, is only around 1‚Äì5%. This raises the question: are the reported OOM issues symptoms of a larger problem, with the replay OOM merely surfacing it?\n\nCurrently, two prevalent scenarios appear:\n1. **Excessive Data Collection**: If your Prometheus setup scrapes too many series or samples, it's prone to OOM crashes during replay, regardless of the memory overhead.\n2. **Running Close to Memory Limits**: Even a slight overhead during replay can trigger OOM crashes if Prometheus is already running near its memory limit, such as at 95%.\n\nThese issues often revolve around cardinality‚Ää‚Äî‚Ääthe combination of all label values per metric. High cardinality metrics, like those tracking multiple URLs or response codes, can quickly escalate memory usage. In short, much of Prometheus' memory woes can be attributed to cardinality.\n\n### üîÑ How Does Remote Write Work?\n\nThe remote write reads data from Prometheus' [write ahead log](https://en.wikipedia.org/wiki/Write-ahead_logging).\n\n![wal](./wal.png)\n\nData generated by scrape is written to the WAL, so this essentially gives us a 2- to 3-hour buffer on disk of data for remote write. The RW system now has a subroutine that reads the WAL and passes the data back to remote write.\n\nRemote write still has a small in-memory buffer, and the routine reading the WAL pauses where it is if it's not able to append new data to the buffer. This means we no longer drop data if the buffer is full, and the buffer doesn't need to be large enough to handle a longer outage.\n\nAs long as the remote endpoint isn't down for hours, remote write no longer loses data (with some caveats, like Prometheus restarts), since the WAL is truncated every two hours or so.\n\n### üõ†Ô∏è Issue Resolutions\n\nIf you've never experienced this issue before (lucky you!), here's a handy solution I found effective. Since Prometheus may not be up and running to utilize PromQL for detecting potential issues, we need an alternative method to identify high cardinality. One approach is to get hands-on with some kubectl exec magic:\n\n```sh\nkubectl exec -it -n monitoring pods/prometheus-prometheus-kube-prometheus-prometheus-0 -- sh\n```\n\nThen, run the Prometheus TSDB analysis:\n\n```sh\n/prometheus $ promtool tsdb analyze .\n```\n\nThis analysis will provide insights into metrics with high cardinality, like `haproxy_server_http_responses_total`, which might be causing memory issues. In such cases, updating or optimizing the problematic metric, such as haproxy, can alleviate memory strain.\n\nAlternatively, consider increasing Prometheus' memory allocation or deploying it to a specific node group with ample memory resources.\n\nHere are some additional strategies to mitigate memory overhead and OOM crashes:\n\n- **Verify Memory Overhead**: Ensure that the memory overhead during replay is within acceptable limits (e.g., 10‚Äì15%). Running Prometheus close to its memory limit is risky due to dynamic garbage collection and limited room for unexpected cardinality spikes or queries.\n- **Optimize Storage and Scraping**: Regularly optimize Prometheus' storage, scraping, and remote write configurations to reduce memory usage. Upgrading to newer releases can often provide optimizations in this regard.\n- **Automate Recovery from OOM**: Implement auto-recovery mechanisms to handle OOM crash loops, such as automatically deleting the Write-Ahead Log (WAL) on OOM events. This ensures smoother recovery from memory-related issues.\n- **Implement Scraping Limits**: Consider introducing forceful scrape limits to prevent Prometheus from scraping targets when memory usage exceeds a certain threshold. This proactive approach can help avoid memory-intensive situations and potential OOM crashes.\n\nBy implementing these strategies, you can effectively manage Prometheus' memory challenges and ensure smooth operation in your monitoring environment.\n\n### üîö Conclusion\n\nDealing with Prometheus OOM errors during WAL replay can be challenging. By understanding the root causes, such as excessive data collection and high cardinality metrics, and implementing solutions like optimizing storage and scraping configurations, increasing memory allocations, and setting up auto-recovery mechanisms, you can mitigate these issues.\n\n**Thank You üñ§**\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1173},"frontmatter":{"id":"e3781588b27a4b0373bffe42","path":"/blog/prometheus-manage-memory-overload/","humanDate":"Oct 29, 2024","fullDate":"2024-10-29","title":"Prometheus Restart Troubles: Managing Memory Overload üß†","keywords":["Prometheus","memory overload","WAL","monitoring","kubernetes"],"excerpt":"Explore strategies to manage memory overload in Prometheus, ensuring stability and performance even when dealing with large Write-Ahead Logs (WAL).","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACtUlEQVR42jWTuW8TYRDF/Y9RAA09HT0SggYJFCEkJA4hEQQFFKQACXGEHMghhpAY30d8JL7wkcRO7M36xFdsb7xOYoP9Y9aCYjSfvp3vvXkzb03hwg6Fcp5IKkIymySxmyC9n5FzSnKazEGGVE7OuTQ/95LTc2InQb5S4PCXilI7RKkeotZUtgu7mApyOeo0cFrMRDx2QvYNPGtfcKx+xv1thaDtO751C66vZlJBH3GfE+/3VX73j9CbVXq1In/0Lox0VMEy7Ul3Z70mJ/0OAyP3WrjkgVPCJ2Dr5kXCQpQI+znRWvK9yaDbQJdolBVK+T2q6j79ZoVCVRHA0gExYd10Wgm5bXis3/DZ1wk4N4gFPfgdG6SjITYl26TLKcnKEvGQl6F+xKmQGARavcSBNGfKySw4PRYJHSYnGiPpMBXwErIJqHWNqJBsOazEpEtD2vh/SK3xbqR3JLWR1qfzNGWK+zhTW5i3PRLuaV4OOvno+8EH7waf/D9YlJiX83LYhSXqYyXixRLzsRb10G4XmehtITkSyQK4I4DBvfgU1JYM4UpvE8jFiCsxAtkI9vQWfjWLX+pcOxGpC2OXcMv9QjRBRu3wqzVgcNzj0OhwKnmoT+VORAJDjXZLY8XTQSn2YDzgzL3KmeUtE1nIZNgXqRonAw2n+h7X7jPubr8kVU9SrVcwZcsF/vzbsDFgY5MP52vMLpSZmddQxD7dFzO03z2nPfeAs/GQ026d494Ry0v3mJu7wuPXD8iKmrLYyHQgqzYYR+KrsQx4JOBPFho8XaxxZ/6YqvhSf/MIbekV/YWXjCcjxlJrWO3a1ZtcvHCZ8+cuEXA4aPUamLbkT6nUi5TElEVxe72lEsupzJoVHHGFZrdCftNK3m6mKOMpSW1ZalVZwP3ZJ1y/fYsbMzO4/G5SlTx/Abzt9sGRXOHxAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/dee8a66c7ee1ff012bb686e835e8e69c/f9343/prometheus-cover.png","srcSet":"/static/dee8a66c7ee1ff012bb686e835e8e69c/6f128/prometheus-cover.png 750w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/dc9a9/prometheus-cover.png 1080w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/cb4db/prometheus-cover.png 1366w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/f9343/prometheus-cover.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/dee8a66c7ee1ff012bb686e835e8e69c/fdac4/prometheus-cover.webp 750w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/0f929/prometheus-cover.webp 1080w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/fc20e/prometheus-cover.webp 1366w,\n/static/dee8a66c7ee1ff012bb686e835e8e69c/24ca0/prometheus-cover.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.55625}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/kubernetes-dns-coredns-externaldns/","title":"Kubernetes & DNS: A Guide to CoreDNS and ExternalDNS in AWS EKS clusterüê≥","date":"2024-10-29 19:06:00"},"excerpt":"The ABCs of Kubernetes DNS üê≥ üî• Introduction Kubernetes, an open-source platform for automating containerized applications, relies on‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/docker-image-optimization/","title":"Docker Image Optimization: A Toolbox of efficient Tricks","date":"2024-10-29 18:30:00"},"excerpt":"A Toolbox of Simple Tricks for Docker Image Optimization ‚ò∏ Introduction Docker has revolutionized the way we package and deploy applications‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}