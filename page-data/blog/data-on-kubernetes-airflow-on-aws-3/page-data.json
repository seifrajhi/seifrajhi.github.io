{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/data-on-kubernetes-airflow-on-aws-3/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Automate and orchestrate data workflows in Kubernetes with Amazon managed Apache Airflow</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî∑ Introduction</h2>\n<p>Welcome back to this blog series on Data on Kubernetes! In this third part, we're going to focus on how to manage workflows effectively using job schedulers and orchestrators. We'll be looking at tools specifically designed for batch workloads, scientific computing, machine learning workflows, and parallel tasks.</p>\n<p>One of the tools we'll explore is <a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Managed Workflows for Apache Airflow MWAA</a>, a great service that helps manage data pipelines, machine learning workflows, and batch processing.</p>\n<h2 id=\"deep-dive-into-job-schedulers-and-workflow-orchestrators\" style=\"position:relative;\"><a href=\"#deep-dive-into-job-schedulers-and-workflow-orchestrators\" aria-label=\"deep dive into job schedulers and workflow orchestrators permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Deep Dive into Job Schedulers and Workflow Orchestrators</h2>\n<p>Nowadays, in the modern <a href=\"https://www.forbes.com/sites/googlecloud/2020/05/20/how-the-world-became-data-driven-and-whats-next/\" target=\"_blank\" rel=\"noopener noreferrer\">data-driven IT world</a>, the management and automation of data workflows are becoming more and more important. This is where Job Schedulers and Batch-Oriented Workflow Orchestrators come into play.</p>\n<p>These platforms are designed to manage and automate various data processes in a systematic and efficient manner. Consider the example of an ETL (Extract, Transform, Load) process, a common scenario in data workflows. In this process, data is extracted from various sources, transformed into a suitable format, and then loaded into a data warehouse for further analysis. Managing this process manually can be challenging and prone to errors.</p>\n<p>This is where Job Schedulers and Workflow Orchestrators prove their worth. They automate these tasks, ensuring that the ETL process runs smoothly and efficiently. Furthermore, they enhance the portability and scalability of our workflows, which is important when dealing with large volumes of data or complex machine learning models.</p>\n<p>Apart from ETL processes, there are other scenarios where Job Schedulers and Workflow Orchestrators can be beneficial. For instance, in machine learning pipelines, where data preprocessing, model training, model evaluation, and model deployment need to be executed in a specific order. Another example could be data synchronization tasks between different systems, which require precise timing and error handling.</p>\n<p>There are several tools available that can help manage these workflows. <a href=\"https://airflow.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Airflow</a> is a platform designed to programmatically author, schedule, and monitor workflows. <a href=\"https://www.kubeflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubeflow</a> is a Kubernetes-native platform for developing, orchestrating, deploying, and running scalable and portable machine learning workloads. <a href=\"https://argoproj.github.io/workflows/\" target=\"_blank\" rel=\"noopener noreferrer\">Argo Workflows</a> is an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes.</p>\n<p>In this blog, we will focus specifically on <a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Managed Workflows for Apache Airflow MWAA</a>. This service makes it easier to set up and operate end-to-end data pipelines in the cloud at scale. Airflow is an open-source workflow management platform. Workflows are defined with <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target=\"_blank\" rel=\"noopener noreferrer\">DAGs</a>, Configuration as code written in Python.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 64.70588235294117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABzklEQVR42o1T23KrMBDj//+xczopAWIM4WYb8EXVmpBpmzycnRAb1mi10lLgT4TNwcwT1xVmmTEMA3Zn4MyCzRqkGPK5dL6Qjr+UjidFejw7D+yrQVOVMNPI/QrnHJahR68VJq4HIE8nAYnwJLBMA+K+HYCSOJMSkS+M44AQIvfHldLBQPaezCfmBWS1C+x4x6AqzJ0iqENhmNBKYTXzwXDf0dR1ZhZCgDEGWmts28acz4COUjiel30gM8v7mUWS31FIG9W1xHjvMqAnSNu28N4/dT31yXt28Gz7TRSODDrdYpnngxWBHMVfrc1SiAQCKIVW0bNXuKsaie3hLPSQLQMGmqBuNV211G6kwyPF76CaCp7u9n2Py+WS217E9baBZq6tr3TfvjIkBemDv/hoKWKepqyZhKwigQDLCMXHuSezF8A/IfpYzpy0LYUiWxVzcsFHcUuZRhbNztMUv6/ZECnwArhxFJS60ekquydDXtH1e99honFaNdnljowddZ66G77+fWDUNwKGNwzDzoF2eSSi3361FZnzq80OO7dmwK65ov76pK5XkvevgO/iHOyfIRMhgLouUX5+YBCG/wv4LkRX+QiWu6bjJea+zfp+A777+EhiJv7NAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"DAG\" title=\"\" src=\"/static/3c64588cebd9c2c9bbdb7b2c0c2bee89/c5bb3/dag.png\" srcset=\"/static/3c64588cebd9c2c9bbdb7b2c0c2bee89/04472/dag.png 170w,\n/static/3c64588cebd9c2c9bbdb7b2c0c2bee89/9f933/dag.png 340w,\n/static/3c64588cebd9c2c9bbdb7b2c0c2bee89/c5bb3/dag.png 680w,\n/static/3c64588cebd9c2c9bbdb7b2c0c2bee89/0f67e/dag.png 921w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/nerd-for-tech/airflow-mwaa-automating-etl-for-a-data-warehouse-f5e50d14713c\">source</a></div>\n<p>Apache Airflow at a high level has the following components talking to each other:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 44.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABX0lEQVR42oVSy1LCQBDM/5/0rn6Dyk2Q0uIiJws5Cag8Y0myhLz3mXZ2YxAoLKdqarObmd7p7vWSJEardYM0TWCjqir8F8ZocM4hlcJwOIDvr6C1xvvHGzxFhyt/Cbs2YUH3gY/3QnAUXEJqhcHLM/zPFaQUWAdreFRJBQLUUjfj1ITVwSqlhFBmr7L+YiyEt40j3LauEW03u98Pj11HpYnJZITO/Z2jWhQ5FvM5wiiGogm73Q7G41diKLFczuGV3I76hSzPkFNyXtA+AIs2SLP6LCF92YY58DjeYjabohTKyRDY3iwlDRWCkChr8zt4GAYEWEJXNQnGmBP82Cyrl1C6Jrp3bo2pAY+cFVKTgxrmD8ctYF4KoqyJ7sgxrCrjLneAttGm0sY1nJ2f4+Lyandr43KTxhikhX02Ev3+E6YkgZQcC6vhgZc/E7U7bfR6vRqQmpsLm7RV5sTk9rV8AyRgt0EMYflVAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"AWS DAG\" title=\"\" src=\"/static/64140cea93542af0f952ab25119ad0a2/c5bb3/aws-dag.png\" srcset=\"/static/64140cea93542af0f952ab25119ad0a2/04472/aws-dag.png 170w,\n/static/64140cea93542af0f952ab25119ad0a2/9f933/aws-dag.png 340w,\n/static/64140cea93542af0f952ab25119ad0a2/c5bb3/aws-dag.png 680w,\n/static/64140cea93542af0f952ab25119ad0a2/1d69c/aws-dag.png 750w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://aws.amazon.com/blogs/machine-learning/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/\">source</a></div>\n<h2 id=\"main-concepts-of-apache-airflow\" style=\"position:relative;\"><a href=\"#main-concepts-of-apache-airflow\" aria-label=\"main concepts of apache airflow permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Main Concepts of Apache Airflow</h2>\n<p>Before starting this demo and the operation of MWAA, it's important to refresh briefly some key Airflow concepts. For more deep dive explanations, please refer to <a href=\"https://airflow.apache.org/docs/apache-airflow/stable/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Airflow official documentation</a>.</p>\n<ul>\n<li>\n<p>üåê <strong>Webserver</strong>: This provides a control interface for users and maintainers. The Airflow UI is a Flask + Gunicorn setup that lists DAGs, their run history, schedule, and pause/start options. It's a central place from where we can manage the Airflow pipelines and also handles the APIs.</p>\n</li>\n<li>\n<p>üóÑÔ∏è <strong>Metadata Database</strong>: Airflow uses a database supported by the SQLAlchemy Library, such as PostgreSQL or MySQL. This database powers the UI and acts as the backend for the worker and scheduler. It stores configurations, such as variables, connections, user information, roles, and policies. It also stores DAG-related metadata such as schedule intervals, tasks, statistics from various runs, etc.</p>\n</li>\n<li>\n<p>üï∞Ô∏è <strong>Scheduler</strong>: This is a daemon, built using the Python daemon library. It schedules &#x26; delegates tasks on the worker node via the executor. It also takes care of other housekeeping tasks like concurrency checks, dependency checks, callbacks, retries, etc. The three main components of the scheduler are:</p>\n<ul>\n<li>SchedulerJob</li>\n<li>DagFileProcessor</li>\n<li>Executor</li>\n</ul>\n</li>\n<li>\n<p>üë∑ <strong>Worker</strong>: These are the workhorses of Airflow. They are the actual nodes where tasks are executed.</p>\n</li>\n<li>\n<p>üîß <strong>Executor</strong>: Executors are the \"workstations\" for \"tasks\". The Executor acts as a middleman to handle resource allocation and distribute task completion. Executors run inside the scheduler. There are many options available in Airflow for executors, including:</p>\n<ul>\n<li>Sequential Executor: Default executor, runs one task at a time.</li>\n<li>Debug Executor: A debug tool, runs tasks by queuing them.</li>\n<li>Local Executor: Runs multiple tasks concurrently on a single machine.</li>\n<li>Dask Executor: Executes tasks concurrently across multiple machines.</li>\n<li>Celery Executor: Scales out the number of workers in parallel.</li>\n<li>Kubernetes Executor: Runs each task in its own Kubernetes pod.</li>\n</ul>\n</li>\n<li>\n<p>üì® <strong>Message Broker (optional)</strong>: A message broker is needed in distributed setups, where the CeleryExecutor is used to manage communication between the Scheduler and the Workers. The message broker, such as RabbitMQ or Redis, helps to pass task information from the Scheduler to the Workers. For MWAA, the Celery Executor is used.</p>\n</li>\n</ul>\n<h3 id=\"sequence-of-actions\" style=\"position:relative;\"><a href=\"#sequence-of-actions\" aria-label=\"sequence of actions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Sequence of Actions</h3>\n<ol>\n<li>The scheduler initiates DAGs based on triggers, which could be scheduled or external.</li>\n<li>The scheduler loads the tasks/steps within the DAG and determines the dependencies.</li>\n<li>Tasks that are ready to run are placed in the queue by the scheduler.</li>\n<li>Workers retrieve these tasks from the queue and execute them.</li>\n<li>Upon completion of a task, a worker updates the task's status.</li>\n<li>The overall status of the DAG is determined based on the statuses of the individual tasks.</li>\n</ol>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 111.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE3ElEQVR42kWU3W/aZhTGudnlpN3uL5z2J0za1VRpV5O2NVuzXnRS1XWq2qZt2jVptvUjbZMUSAgQCOH7w8YGbPAHtrEN/HZstu4Vhxe95n38nOec82TW6zVJbNaaIIoI4xVta8TDy6M0Hl2d8LB2xJF2RbxcYts2lm1hWf9HcpbgZCIBmM1m6YHneWSVIQf1DsXRgAOtzMPyWx5IPO2dUbAHGIbByVmJ3qBPu9uh3enQVxSKpRJhGJJJwPL5HPlcPn1wrGjsVC8pNVus41g4r0k//2ahazqNTj/9vVovP56rqroBTFJIKJummTIs6BP2qh003SYI5nLmby6vVuk+Go24sb/DvfNDfnp4h+3Hv3Ov8Ia7r54TS7aZVeARr1Ie6XKjJWM3YmnBMhRNFz5zd/6RScIi177k1stdvrj2FV9++zX382+4UDushFxmf2+f7Rd7+AIaeXMMx8PzA7kY4E48AmeB6zqi3ViAXcJFyHq5wjZMViIJ8nstJBJGy4Th3UqF7dIOc8dgNJ0wmY3x5yYLzyLwp9jjKY4plTQNhsLCNBR8byq7yszUqA7rPDl/zX75HS29S2buTgmXHroA7rWyPG8es3P5jsqkIW/1JVXRMAohEGZegC9a+/OJpOcJiTEHgzO2PuxyPbvLW+VCqmwO8SIfxzfYbR9xu3DAg+obygngKsBbTCmoVcqaVF6p0uh3cc0Jrq3jWjrv1XOeNd6y1zmiZgnDkd7HdiZE/gzN7DOxFAHyUgZIqL7GVuEZ3x8+4IeTR9yvv8O356KfI1Ko5K7K/HZR5NdCng+d1gZQHTRR+nUUpYM60gRocyHZvdiiOKlz3DjlZTPL69oJvdYVve4Vtmj2qFTmsx9v88l3t/jm1XsyxlhlMuoz0rqMRz2GUk3Hd0Q2h2Xs4rsGcWBLwyxYLGb0O5c0rkpUKmdMBg12iud8vn2XT7fucO3wmMxYVwSsx9QYoA77dLUhM99FnxoMunVOjg8pFXPoShdvZkjrGFgznSQzT4pS79TYPz9j9zRHpdfaMDx8fcCTx/dRJHVXLjiBy8gYkc8ec5o75uTkkGL2jLk+o9UoUymfclHK0W1V8Vyp+MIWXTXCwNoA5nPvef3yBbreQ5VedCXFVbzRMY4s4tCSZnZZhDPKwvayWqDdrNAWwEB6No5sYT6WYXCSogyYjBWmEwVN66ONh2n/uYHBfvMDfzSP2Gt/4M9unkeVQ+qjBpFjSbpGOgBLAYuCGXNpo1CYZsYjVQxgykKmIvBlQgIzre7E0/nlfI/rRzv8nH3KjdxTtk6fkTdrIA0eL6St8NMWixbyAmEYhS4ZczrGmtuEAhjJG4i9NGJ52LH6lEZ1sq0iRbVGfSadINp6hiV62almsYAlTK2pJoRmZGwZ+KExFcoWbUn/74r02uUpNaUtze4TSywXC7noE1hzmW1paMfGkf+n4WysT9NUfH8ufiguM0/cQlbVHnKzdMBNSfWvQVk0ETe3HZkkF9dzMTWXqYQfCri4Tpy4jaxkT7w0WSng2ItwFuKDXkjXcTjpDBiK6eoTg+NsnoJMQ6lS5bx0wdl5mcFQZ2q70rdJmuIDcseVTFPA5CtarsVx1umerLFp02i2abY7NCSqV3XqrQ61eoOrRoNWt0e1Vk8BE5dOTPc/R/8H8gRP+dlENQEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Airflow\" title=\"\" src=\"/static/7d6f32586aff58da6a85271562123550/c5bb3/airflow.png\" srcset=\"/static/7d6f32586aff58da6a85271562123550/04472/airflow.png 170w,\n/static/7d6f32586aff58da6a85271562123550/9f933/airflow.png 340w,\n/static/7d6f32586aff58da6a85271562123550/c5bb3/airflow.png 680w,\n/static/7d6f32586aff58da6a85271562123550/b12f7/airflow.png 1020w,\n/static/7d6f32586aff58da6a85271562123550/6f2be/airflow.png 1029w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-more-airflow-terminology\" style=\"position:relative;\"><a href=\"#-more-airflow-terminology\" aria-label=\" more airflow terminology permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• More Airflow Terminology</h3>\n<ul>\n<li><strong>Operators</strong>: They are the fundamental elements of a DAG. They outline the actual work that the DAG will carry out. The nature of the task is determined by the operators. They are represented by a Python class that serves as a task type template. They are idempotent.</li>\n<li>\n<ul>\n<li>BashOperator‚Ää-‚ÄäExecutes a bash command.</li>\n<li>PythonOperator‚Ää-‚ÄäRuns a Python function.</li>\n<li>SparkSubmitOperator‚Ää-‚ÄäExecutes spark-submit.</li>\n</ul>\n</li>\n<li><strong>Task</strong>: A task is an instance of an operator or sensor.</li>\n<li><strong>Plugins</strong>: They offer a convenient way to write, share, and activate custom runtime behavior.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">__init__.py\n                <span class=\"token operator\">|</span>-- airflow_plugin.py\nhooks/\n                <span class=\"token operator\">|</span>-- __init__.py\n                <span class=\"token operator\">|</span>-- airflow_hook.py\noperators/\n                <span class=\"token operator\">|</span>-- __init__.py\n                <span class=\"token operator\">|</span>-- airflow_operator.py\nsensors/\n                <span class=\"token operator\">|</span>-- __init__.py\n                <span class=\"token operator\">|</span>-- airflow_sensor.py</code></pre></div>\n<blockquote>\n<p>Note: In MWAA, we don't have direct access to the runtime where tasks are being processed. We can use <code class=\"language-text\">requirements.txt</code> to install available Python modules. For unavailable or custom modules, we can create a zip of the packages locally, upload it to S3, and use it as needed.</p>\n</blockquote>\n<ul>\n<li><strong>Hooks</strong>: Provide a way to connect your DAG to your environment. They serve as an interface for interacting with external systems. For example, we can establish an S3 connection and use S3 Hooks to retrieve the connection information and perform our task. There are various hooks available (HTTP, Hive, Slack, MySQL), and more are continuously being added by the community.</li>\n<li><strong>Sensors</strong>: They are special operators used to monitor (or poll) long-running tasks, files, database rows, S3 keys, other DAGs/tasks, etc.</li>\n<li><strong>XComs</strong>: XComs (cross-communication) are designed to facilitate communication between tasks. We use <code class=\"language-text\">xcom_push</code> and <code class=\"language-text\">xcom_pull</code> to store and retrieve variables, respectively.</li>\n</ul>\n<p>Tasks transition from one state to another during the execution of a DAG. Initially, the Airflow scheduler determines if it's time for a task to run and whether all other dependencies for the task have been met. At this point, the task enters the scheduled state. When a task is assigned to an executor, it enters the queued state. When the executor picks up the task and a worker begins executing the task, the task enters the <strong>running</strong> state.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9NDQf/xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAQUCHP8A/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGRAAAwEBAQAAAAAAAAAAAAAAAREhACBh/9oACAEBAAE/IXVoIYNKmHnH/9oADAMBAAIAAwAAABAgD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAAIDAQEAAAAAAAAAAAAAAAERACExQWH/2gAIAQEAAT8QYdjuwwwg5re+RUQ1HsZhvbVwGp//2Q=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"States\" title=\"\" src=\"/static/358174cd60668bd1771416ec0f939da5/7bf67/states.jpg\" srcset=\"/static/358174cd60668bd1771416ec0f939da5/651be/states.jpg 170w,\n/static/358174cd60668bd1771416ec0f939da5/d30a3/states.jpg 340w,\n/static/358174cd60668bd1771416ec0f939da5/7bf67/states.jpg 680w,\n/static/358174cd60668bd1771416ec0f939da5/990cb/states.jpg 1020w,\n/static/358174cd60668bd1771416ec0f939da5/c44b8/states.jpg 1360w,\n/static/358174cd60668bd1771416ec0f939da5/b17f8/states.jpg 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-demo-mwaa-airflow-dag-on-eks\" style=\"position:relative;\"><a href=\"#-demo-mwaa-airflow-dag-on-eks\" aria-label=\" demo mwaa airflow dag on eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Demo: MWAA Airflow DAG on EKS</h2>\n<p>A Directed Acyclic Graph (DAG) is a graphical representation of a workflow in Airflow. It organizes tasks in a manner that clearly illustrates the relationships and dependencies between each task. The context for executing tasks is contained within the DAGs. In MWAA, DAGs are stored in Amazon S3. When a new DAG file is introduced, it takes approximately one minute for Amazon MWAA to begin utilizing the new file.</p>\n<p>Amazon Managed Workflows for Apache Airflow MWAA is a managed service that simplifies the orchestration of Apache Airflow, making it more straightforward to establish and manage comprehensive data pipelines in the cloud at a large scale. With Managed Workflows, you have the ability to employ Airflow and Python to construct workflows, without the need to handle the underlying infrastructure required for scalability, availability, and security.</p>\n<h3 id=\"provision-the-infrastructure\" style=\"position:relative;\"><a href=\"#provision-the-infrastructure\" aria-label=\"provision the infrastructure permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Provision the Infrastructure</h3>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/data-on-eks.git\n<span class=\"token builtin class-name\">cd</span> data-on-eks/schedulers/terraform/managed-airflow-mwaa\n<span class=\"token function\">chmod</span> +x install.sh\n./install.sh</code></pre></div>\n<p>The following components are provisioned in your environment:</p>\n<ul>\n<li>A VPC with 3 Private Subnets and 3 Public Subnets.</li>\n<li>Internet gateway for Public Subnets and NAT Gateway for Private Subnets.</li>\n<li>EKS Cluster with one managed node group.</li>\n<li>K8S metrics server and cluster autoscaler.</li>\n<li>A MWAA environment in version 2.2.2.</li>\n<li>A S3 bucket with DAG code.</li>\n</ul>\n<p>After a few minutes, the script will finish and you can run the below commands:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">aws eks <span class=\"token parameter variable\">--region</span> eu-west-1 update-kubeconfig <span class=\"token parameter variable\">--name</span> managed-airflow-mwaa\nkubectl get namespaces</code></pre></div>\n<p>You should see output similar to the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">default           Active    8m\nemr-mwaa          Active    4m\nkube-node-lease   Active    9m\nkube-public       Active    9m\nkube-system       Active    9m\nmwaa              Active    3m</code></pre></div>\n<h3 id=\"log-into-apache-airflow-ui\" style=\"position:relative;\"><a href=\"#log-into-apache-airflow-ui\" aria-label=\"log into apache airflow ui permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Log into Apache Airflow UI</h3>\n<ol>\n<li>Open the Environments page on the Amazon MWAA console.</li>\n<li>Choose an environment.</li>\n<li>Under the Details section, click the link for the Airflow UI.</li>\n</ol>\n<h3 id=\"trigger-the-dag-workflow-to-execute-job-in-eks\" style=\"position:relative;\"><a href=\"#trigger-the-dag-workflow-to-execute-job-in-eks\" aria-label=\"trigger the dag workflow to execute job in eks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Trigger the DAG Workflow to Execute Job in EKS</h3>\n<ol>\n<li>In the Airflow UI, enable the example DAG <code class=\"language-text\">kubernetes_pod_example</code> and then trigger it.</li>\n</ol>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 22.941176470588236%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAArUlEQVR42qWQwQ7EIAhE/f8P3WTTVkVFbRVn0fba05K8zAQjGTDeE5z3mDoREYwB1aF6Iw9v1XvXv35hPt8dISbYOZgSKBUkLgiJwZzVZ0Qll4p2NbTWcSm3b+pvnUFabzDOWWRmlJzB5QTxiVKyUpC1N5meAmGjHc4eiHaDjQHknXIoms45hWDWihq7nnWt91bzrQ+5TyJ9nWAMeRgrJWuwNVC0Wa+Kf2rdWgQ/wOSH2qJliM0AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"UI\" title=\"\" src=\"/static/5eac917d55b4aa9d8d88c469ca802548/c5bb3/ui.png\" srcset=\"/static/5eac917d55b4aa9d8d88c469ca802548/04472/ui.png 170w,\n/static/5eac917d55b4aa9d8d88c469ca802548/9f933/ui.png 340w,\n/static/5eac917d55b4aa9d8d88c469ca802548/c5bb3/ui.png 680w,\n/static/5eac917d55b4aa9d8d88c469ca802548/b12f7/ui.png 1020w,\n/static/5eac917d55b4aa9d8d88c469ca802548/b5a09/ui.png 1360w,\n/static/5eac917d55b4aa9d8d88c469ca802548/29007/ui.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"verify-that-the-pod-was-executed-successfully\" style=\"position:relative;\"><a href=\"#verify-that-the-pod-was-executed-successfully\" aria-label=\"verify that the pod was executed successfully permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Verify that the Pod was Executed Successfully</h3>\n<p>After it runs and completes successfully, use the following command to verify the pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">kubectl get pods <span class=\"token parameter variable\">-n</span> mwaa</code></pre></div>\n<p>You should see output similar to the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">NAME                                             READY   STATUS      RESTARTS   AGE\nmwaa-pod-test.4bed823d645844bc8e6899fd858f119d   <span class=\"token number\">0</span>/1     Completed   <span class=\"token number\">0</span>          25s</code></pre></div>\n<h2 id=\"Ô∏è-key-takeaways\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-key-takeaways\" aria-label=\"Ô∏è key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóùÔ∏è Key Takeaways</h2>\n<p>Automating and organizing data tasks is very important for managing work and resources well in Kubernetes. This is where Job Schedulers are useful. They run jobs that are done either once or many times, making sure tasks are finished when needed. On the other hand, Batch-Oriented Workflow Orchestrators give more control. They allow for complex job scheduling, including the order of tasks and their dependencies.</p>\n<p><strong>Stay tuned for next blogs in this series üéâ</strong></p>\n<p><strong>References:</strong></p>\n<ul>\n<li><a href=\"https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-eks-example.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-eks-example.html</a></li>\n<li><a href=\"https://medium.com/apache-airflow/what-we-learned-after-running-airflow-on-kubernetes-for-2-years-0537b157acfd\" target=\"_blank\" rel=\"noopener noreferrer\">https://medium.com/apache-airflow/what-we-learned-after-running-airflow-on-kubernetes-for-2-years-0537b157acfd</a></li>\n<li><a href=\"https://medium.com/@binayalenka/airflow-architecture-667f1cc613e8\" target=\"_blank\" rel=\"noopener noreferrer\">https://medium.com/@binayalenka/airflow-architecture-667f1cc613e8</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow</a></li>\n</ul>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":7,"rawMarkdownBody":"\n> **Automate and orchestrate data workflows in Kubernetes with Amazon managed Apache Airflow**\n\n## üî∑ Introduction\n\nWelcome back to this blog series on Data on Kubernetes! In this third part, we're going to focus on how to manage workflows effectively using job schedulers and orchestrators. We'll be looking at tools specifically designed for batch workloads, scientific computing, machine learning workflows, and parallel tasks.\n\nOne of the tools we'll explore is [Amazon Managed Workflows for Apache Airflow MWAA](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html), a great service that helps manage data pipelines, machine learning workflows, and batch processing.\n\n## Deep Dive into Job Schedulers and Workflow Orchestrators\n\nNowadays, in the modern [data-driven IT world](https://www.forbes.com/sites/googlecloud/2020/05/20/how-the-world-became-data-driven-and-whats-next/), the management and automation of data workflows are becoming more and more important. This is where Job Schedulers and Batch-Oriented Workflow Orchestrators come into play.\n\nThese platforms are designed to manage and automate various data processes in a systematic and efficient manner. Consider the example of an ETL (Extract, Transform, Load) process, a common scenario in data workflows. In this process, data is extracted from various sources, transformed into a suitable format, and then loaded into a data warehouse for further analysis. Managing this process manually can be challenging and prone to errors.\n\nThis is where Job Schedulers and Workflow Orchestrators prove their worth. They automate these tasks, ensuring that the ETL process runs smoothly and efficiently. Furthermore, they enhance the portability and scalability of our workflows, which is important when dealing with large volumes of data or complex machine learning models.\n\nApart from ETL processes, there are other scenarios where Job Schedulers and Workflow Orchestrators can be beneficial. For instance, in machine learning pipelines, where data preprocessing, model training, model evaluation, and model deployment need to be executed in a specific order. Another example could be data synchronization tasks between different systems, which require precise timing and error handling.\n\nThere are several tools available that can help manage these workflows. [Apache Airflow](https://airflow.apache.org/) is a platform designed to programmatically author, schedule, and monitor workflows. [Kubeflow](https://www.kubeflow.org/) is a Kubernetes-native platform for developing, orchestrating, deploying, and running scalable and portable machine learning workloads. [Argo Workflows](https://argoproj.github.io/workflows/) is an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes.\n\nIn this blog, we will focus specifically on [AWS Managed Workflows for Apache Airflow MWAA](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html). This service makes it easier to set up and operate end-to-end data pipelines in the cloud at scale. Airflow is an open-source workflow management platform. Workflows are defined with [DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph), Configuration as code written in Python.\n\n![DAG](./dag.png)\n<div class=\"image-title\"><a href=\"https://medium.com/nerd-for-tech/airflow-mwaa-automating-etl-for-a-data-warehouse-f5e50d14713c\">source</a></div>\n\nApache Airflow at a high level has the following components talking to each other:\n\n![AWS DAG](./aws-dag.png)\n<div class=\"image-title\"><a href=\"https://aws.amazon.com/blogs/machine-learning/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/\">source</a></div>\n\n## Main Concepts of Apache Airflow\n\nBefore starting this demo and the operation of MWAA, it's important to refresh briefly some key Airflow concepts. For more deep dive explanations, please refer to [Apache Airflow official documentation](https://airflow.apache.org/docs/apache-airflow/stable/index.html).\n\n- üåê **Webserver**: This provides a control interface for users and maintainers. The Airflow UI is a Flask + Gunicorn setup that lists DAGs, their run history, schedule, and pause/start options. It's a central place from where we can manage the Airflow pipelines and also handles the APIs.\n- üóÑÔ∏è **Metadata Database**: Airflow uses a database supported by the SQLAlchemy Library, such as PostgreSQL or MySQL. This database powers the UI and acts as the backend for the worker and scheduler. It stores configurations, such as variables, connections, user information, roles, and policies. It also stores DAG-related metadata such as schedule intervals, tasks, statistics from various runs, etc.\n- üï∞Ô∏è **Scheduler**: This is a daemon, built using the Python daemon library. It schedules & delegates tasks on the worker node via the executor. It also takes care of other housekeeping tasks like concurrency checks, dependency checks, callbacks, retries, etc. The three main components of the scheduler are:\n  \n    - SchedulerJob\n    - DagFileProcessor\n    - Executor\n\n- üë∑ **Worker**: These are the workhorses of Airflow. They are the actual nodes where tasks are executed.\n- üîß **Executor**: Executors are the \"workstations\" for \"tasks\". The Executor acts as a middleman to handle resource allocation and distribute task completion. Executors run inside the scheduler. There are many options available in Airflow for executors, including:\n  - Sequential Executor: Default executor, runs one task at a time.\n  - Debug Executor: A debug tool, runs tasks by queuing them.\n  - Local Executor: Runs multiple tasks concurrently on a single machine.\n  - Dask Executor: Executes tasks concurrently across multiple machines.\n  - Celery Executor: Scales out the number of workers in parallel.\n  - Kubernetes Executor: Runs each task in its own Kubernetes pod.\n- üì® **Message Broker (optional)**: A message broker is needed in distributed setups, where the CeleryExecutor is used to manage communication between the Scheduler and the Workers. The message broker, such as RabbitMQ or Redis, helps to pass task information from the Scheduler to the Workers. For MWAA, the Celery Executor is used.\n\n### Sequence of Actions\n\n1. The scheduler initiates DAGs based on triggers, which could be scheduled or external.\n2. The scheduler loads the tasks/steps within the DAG and determines the dependencies.\n3. Tasks that are ready to run are placed in the queue by the scheduler.\n4. Workers retrieve these tasks from the queue and execute them.\n5. Upon completion of a task, a worker updates the task's status.\n6. The overall status of the DAG is determined based on the statuses of the individual tasks.\n\n![Airflow](./airflow.png)\n\n### üî• More Airflow Terminology\n\n- **Operators**: They are the fundamental elements of a DAG. They outline the actual work that the DAG will carry out. The nature of the task is determined by the operators. They are represented by a Python class that serves as a task type template. They are idempotent.\n- \n  - BashOperator‚Ää-‚ÄäExecutes a bash command.\n  - PythonOperator‚Ää-‚ÄäRuns a Python function.\n  - SparkSubmitOperator‚Ää-‚ÄäExecutes spark-submit.\n- **Task**: A task is an instance of an operator or sensor.\n- **Plugins**: They offer a convenient way to write, share, and activate custom runtime behavior.\n\n```shell\n__init__.py\n                |-- airflow_plugin.py\nhooks/\n                |-- __init__.py\n                |-- airflow_hook.py\noperators/\n                |-- __init__.py\n                |-- airflow_operator.py\nsensors/\n                |-- __init__.py\n                |-- airflow_sensor.py\n```\n\n> Note: In MWAA, we don't have direct access to the runtime where tasks are being processed. We can use `requirements.txt` to install available Python modules. For unavailable or custom modules, we can create a zip of the packages locally, upload it to S3, and use it as needed.\n\n- **Hooks**: Provide a way to connect your DAG to your environment. They serve as an interface for interacting with external systems. For example, we can establish an S3 connection and use S3 Hooks to retrieve the connection information and perform our task. There are various hooks available (HTTP, Hive, Slack, MySQL), and more are continuously being added by the community.\n- **Sensors**: They are special operators used to monitor (or poll) long-running tasks, files, database rows, S3 keys, other DAGs/tasks, etc.\n- **XComs**: XComs (cross-communication) are designed to facilitate communication between tasks. We use `xcom_push` and `xcom_pull` to store and retrieve variables, respectively.\n\nTasks transition from one state to another during the execution of a DAG. Initially, the Airflow scheduler determines if it's time for a task to run and whether all other dependencies for the task have been met. At this point, the task enters the scheduled state. When a task is assigned to an executor, it enters the queued state. When the executor picks up the task and a worker begins executing the task, the task enters the **running** state.\n\n![States](./states.jpg)\n\n## üöÄ Demo: MWAA Airflow DAG on EKS\n\nA Directed Acyclic Graph (DAG) is a graphical representation of a workflow in Airflow. It organizes tasks in a manner that clearly illustrates the relationships and dependencies between each task. The context for executing tasks is contained within the DAGs. In MWAA, DAGs are stored in Amazon S3. When a new DAG file is introduced, it takes approximately one minute for Amazon MWAA to begin utilizing the new file.\n\nAmazon Managed Workflows for Apache Airflow MWAA is a managed service that simplifies the orchestration of Apache Airflow, making it more straightforward to establish and manage comprehensive data pipelines in the cloud at a large scale. With Managed Workflows, you have the ability to employ Airflow and Python to construct workflows, without the need to handle the underlying infrastructure required for scalability, availability, and security.\n\n### Provision the Infrastructure\n\n```bash\ngit clone https://github.com/seifrajhi/data-on-eks.git\ncd data-on-eks/schedulers/terraform/managed-airflow-mwaa\nchmod +x install.sh\n./install.sh\n```\n\nThe following components are provisioned in your environment:\n\n- A VPC with 3 Private Subnets and 3 Public Subnets.\n- Internet gateway for Public Subnets and NAT Gateway for Private Subnets.\n- EKS Cluster with one managed node group.\n- K8S metrics server and cluster autoscaler.\n- A MWAA environment in version 2.2.2.\n- A S3 bucket with DAG code.\n\nAfter a few minutes, the script will finish and you can run the below commands:\n\n```bash\naws eks --region eu-west-1 update-kubeconfig --name managed-airflow-mwaa\nkubectl get namespaces\n```\n\nYou should see output similar to the following:\n\n```shell\ndefault           Active    8m\nemr-mwaa          Active    4m\nkube-node-lease   Active    9m\nkube-public       Active    9m\nkube-system       Active    9m\nmwaa              Active    3m\n```\n\n### Log into Apache Airflow UI\n\n1. Open the Environments page on the Amazon MWAA console.\n2. Choose an environment.\n3. Under the Details section, click the link for the Airflow UI.\n\n### Trigger the DAG Workflow to Execute Job in EKS\n\n1. In the Airflow UI, enable the example DAG `kubernetes_pod_example` and then trigger it.\n\n![UI](./ui.png)\n\n### Verify that the Pod was Executed Successfully\n\nAfter it runs and completes successfully, use the following command to verify the pod:\n\n```bash\nkubectl get pods -n mwaa\n```\n\nYou should see output similar to the following:\n\n```shell\nNAME                                             READY   STATUS      RESTARTS   AGE\nmwaa-pod-test.4bed823d645844bc8e6899fd858f119d   0/1     Completed   0          25s\n```\n\n## üóùÔ∏è Key Takeaways\n\nAutomating and organizing data tasks is very important for managing work and resources well in Kubernetes. This is where Job Schedulers are useful. They run jobs that are done either once or many times, making sure tasks are finished when needed. On the other hand, Batch-Oriented Workflow Orchestrators give more control. They allow for complex job scheduling, including the order of tasks and their dependencies.\n\n**Stay tuned for next blogs in this series üéâ**\n\n**References:**\n\n- https://docs.aws.amazon.com/mwaa/latest/userguide/mwaa-eks-example.html\n- https://medium.com/apache-airflow/what-we-learned-after-running-airflow-on-kubernetes-for-2-years-0537b157acfd\n- https://medium.com/@binayalenka/airflow-architecture-667f1cc613e8\n- https://awslabs.github.io/data-on-eks/docs/blueprints/job-schedulers/aws-managed-airflow\n\n**Thank You üñ§**\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1641},"frontmatter":{"id":"ecbbb9c429bf769b8d2c8e53","path":"/blog/data-on-kubernetes-airflow-on-aws-3/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Data on Kubernetes: Part 3‚Ää-‚ÄäManaging Workflows with Job Schedulers and Batch-Oriented Workflow Orchestrators","keywords":["Apache Airflow","AWS Kubernetes EKS","Workflow Orchestration","AWS","Amazon Managed Workflows for Apache Airflow MWAA"],"excerpt":"Explore how to automate and orchestrate data workflows in Kubernetes using Amazon managed Apache Airflow MWAA.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC3ElEQVR42h2Sy25aVxSGeZm+RB+g6qDTPoE7atVEVaWogypKM0ir3hQ3ahMZDBgCGMLhficHMD6QA7bhYC4GDA7U5uYQIHaMo/TyZZvB1pb21vr+/19r6bS4mcXLHFezPgdKjIhDj5r0UFFl0iEXcsBJQQ6SS/rIxiS6VZXnPgf1vQzVQopXpw2W8z7X02MqwUfo1JCRt6+7XEw6+J/qCTlMK0jCY2M3KmF7/BC/Vc8z45+r/710iFzci8+6QVmJo8Q8THo1ltMW+9Iv6ApBA2/nZ8wGR0ScZpKepwRuwNtG9nciq+Kk174SiEtWdiLulcOIy0whFSS8baZTUfj33YxyRI+uHLdwOT9lMWyRFY5ioshvMwigaRW5vr9DwG4knwwQdG6RFsBcwkdcCEddW6REmt5RgXeXI0rBx+jSlgfMRy3+uxoDFzQ1RcTRkw67xW0QTizkRO9kyUzYsI4sIDei3q0nVIoprhd9ZsPmylQpvIEuY7zLuKeRCduJuo1En20KiEGoG8TbNstxg1q3w6bSJ5bNYCvl6alhBi8cRI0OHvzwI1PRruWbAVrchG7H/jOlrB/Jso751+/Z+Ok71u9/y293b4t+ioHNB6j1MypHXdonLTKam/GBnfOuhpJMUMzJq4QXYksK3pspe/5g0FCwBjzc23hEajfF4rzP++VNC+acDSb87hoy7R+yOJAZVEOMD20kE1lKxTz8M+GN2JDppIfqFlNu5Ly8v57iDUrc+2udz2/d5n/Oab9QsOtNZDMRmgdplL87OA9bjIZDOi9HWEsjkkUR9bwpgG2O83YavjvoavIWl6Map90qfeHi4y/X+OLhfT5dW+OjTz4jEPOyt5sQxWX2uici9jGddpNcrY2mFalXxcl7qbq+oSXdEg5TFq7GVU6aGmo5gyQANr8L6bkfp+xn+bpJVhvz9eYrDhsNDFGVyWmN2VmdYUuhFH+CVohST5tour/iAyiKypnteG1QAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/9763f373b00b538b6d6ca67a345c230c/f9343/dok3.png","srcSet":"/static/9763f373b00b538b6d6ca67a345c230c/6f128/dok3.png 750w,\n/static/9763f373b00b538b6d6ca67a345c230c/dc9a9/dok3.png 1080w,\n/static/9763f373b00b538b6d6ca67a345c230c/cb4db/dok3.png 1366w,\n/static/9763f373b00b538b6d6ca67a345c230c/f9343/dok3.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/9763f373b00b538b6d6ca67a345c230c/fdac4/dok3.webp 750w,\n/static/9763f373b00b538b6d6ca67a345c230c/0f929/dok3.webp 1080w,\n/static/9763f373b00b538b6d6ca67a345c230c/fc20e/dok3.webp 1366w,\n/static/9763f373b00b538b6d6ca67a345c230c/24ca0/dok3.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.55625}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/goldilocks-kubernetes-resource-allocation/","title":"Goldilocks: The Secret to Setting Kubernetes Resource Requests and Limits Just Right","date":"2024-10-25 14:34:00"},"excerpt":"Optimizing Kubernetes Resource Allocation ‚ò∏Ô∏è Introduction Kubernetes resource requests and limits are two of the most important concepts in‚Ä¶","html":"<blockquote>\n<p><strong>Optimizing Kubernetes Resource Allocation</strong></p>\n</blockquote>\n<h2 id=\"Ô∏è-introduction\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-introduction\" aria-label=\"Ô∏è introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ò∏Ô∏è Introduction</h2>\n<p>Kubernetes resource requests and limits are two of the most important concepts in Kubernetes, but they can also be some of the most difficult to set correctly. If your requests are too low, your pods will be evicted and your applications will fail. If your requests are too high, you'll be wasting resources and money.</p>\n<p><a href=\"https://github.com/FairwindsOps/goldilocks\" target=\"_blank\" rel=\"noopener noreferrer\">Goldilocks</a> is a utility that can help you identify a starting point for setting your resource requests and limits. It uses the Kubernetes Vertical Pod Autoscaler (VPA) to analyze the current resource usage of your pods and provide recommendations.</p>\n<p>In this post, we'll explain what Goldilocks is and how it works. We'll also show you how to use Goldilocks to set your resource requests and limits just right.</p>\n<h3 id=\"how-can-this-help-with-resource-settings\" style=\"position:relative;\"><a href=\"#how-can-this-help-with-resource-settings\" aria-label=\"how can this help with resource settings permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How can this help with resource settings</h3>\n<p>By using the Kubernetes <a href=\"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Vertical Pod Autoscaler</a> in recommendation mode, we can see a suggestion for resource requests on each of our apps. This tool creates a VPA for each workload in a namespace and then queries them for information.</p>\n<p>Once your VPAs are in place, you'll see recommendations appear in the Goldilocks dashboard:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 57.05882352941176%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABrklEQVR42pWSW2+bQBCF+f8/pa+VWvUpUlIl6kvbNJEs10rFZfEusHcWQ8BwOl4nTSL7IX34AM05MzrMbrK++o70boP8D4PIOdLfGbJNjqHtsIwT5sfxv0hWn69we3EDtn7AoDywoyG+x2Q6jDq8H/IPtUWibz6ivvgAl6VwLEXYMsxGAk6dZbFHTjSvMTUVks3qDl++XaPYFmCCgdccYefR9e1Z+scucqIdalIgMU2NzBgI67BVDiVhpz38fo44+n7G0k6zWiGvNdx49Lj9kzYv8EoiCVqiAyUms/FdRFqPRltUysa6n14woYelA9MuRE+tDBwdhpvn48CWHjERNUoypHkJSSbBORw1vx1IScIOWVaglhq80VDGox1fD5QNxZ5io6gkipJDH9IJgbYbqD7+G9jS70kakBUllHYQ0sBQUv86YbAGdFnifg57LLVBE7rj3qgW3/sXKt+CKUrn/FGPngl2oR3S+hL78xLq/itaXcNWDFYUCEpgcBK9bdCbt3SkBcmjJ+rPHkt+zuge/rhEdv0J1cMKfP0L1eYeY8MocoXFiFOofk471CbB8BffeUYXxWSj9wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"goldilocks\" title=\"\" src=\"/static/f33bec6050370a9e359e76d5cc147d72/c5bb3/goldilocks.png\" srcset=\"/static/f33bec6050370a9e359e76d5cc147d72/04472/goldilocks.png 170w,\n/static/f33bec6050370a9e359e76d5cc147d72/9f933/goldilocks.png 340w,\n/static/f33bec6050370a9e359e76d5cc147d72/c5bb3/goldilocks.png 680w,\n/static/f33bec6050370a9e359e76d5cc147d72/b12f7/goldilocks.png 1020w,\n/static/f33bec6050370a9e359e76d5cc147d72/b5a09/goldilocks.png 1360w,\n/static/f33bec6050370a9e359e76d5cc147d72/29007/goldilocks.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"installation\" style=\"position:relative;\"><a href=\"#installation\" aria-label=\"installation permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Installation</h3>\n<h4 id=\"requirements\" style=\"position:relative;\"><a href=\"#requirements\" aria-label=\"requirements permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Requirements</h4>\n<ul>\n<li><code class=\"language-text\">kubectl</code></li>\n<li>Vertical Pod Autoscaler configured in the cluster</li>\n<li>Some workloads with pods (Goldilocks will monitor any workload controller that includes a PodSpec template (spec.template.spec.containers[] to be specific). This includes Deployments, DaemonSets, and StatefulSets among others.)</li>\n<li><code class=\"language-text\">metrics-server</code> (a requirement of VPA)</li>\n<li><code class=\"language-text\">golang 1.17+</code></li>\n</ul>\n<h4 id=\"installing-vertical-pod-autoscaler\" style=\"position:relative;\"><a href=\"#installing-vertical-pod-autoscaler\" aria-label=\"installing vertical pod autoscaler permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Installing Vertical Pod Autoscaler</h4>\n<p>There are multiple ways to install VPA for use with Goldilocks:</p>\n<ul>\n<li>Install using the <code class=\"language-text\">hack/vpa-up.sh</code> script from the <a href=\"https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/hack/vpa-up.sh\" target=\"_blank\" rel=\"noopener noreferrer\">vertical-pod-autoscaler repository</a>.</li>\n<li>Install using the <a href=\"https://github.com/FairwindsOps/charts/tree/master/stable/vpa\" target=\"_blank\" rel=\"noopener noreferrer\">Fairwinds VPA Helm Chart</a>.</li>\n</ul>\n<blockquote>\n<p><strong>Important Note about VPA:</strong>\nThe full VPA install includes the updater and the admission webhook for VPA. Goldilocks only requires the recommender. An admission webhook can introduce unexpected results in a cluster if not planned for properly. If installing VPA using the Goldilocks chart and the VPA sub-chart, only the VPA recommender will be installed. See the <a href=\"https://github.com/FairwindsOps/charts/tree/master/stable/vpa\" target=\"_blank\" rel=\"noopener noreferrer\">VPA chart</a> and the Goldilocks <a href=\"https://github.com/FairwindsOps/charts/blob/master/stable/goldilocks/values.yaml\" target=\"_blank\" rel=\"noopener noreferrer\">values.yaml</a> for more information.\nVPA does not require the use of Prometheus, but it is supported. The use of Prometheus may provide more accurate results.</p>\n</blockquote>\n<h4 id=\"installation-1\" style=\"position:relative;\"><a href=\"#installation-1\" aria-label=\"installation 1 permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Installation</h4>\n<p>First, make sure you satisfy the requirements above.</p>\n<p><strong>Method 1‚Ää-‚ÄäHelm (preferred):</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">helm repo <span class=\"token function\">add</span> fairwinds-stable https://charts.fairwinds.com/stable\nkubectl create namespace goldilocks\nhelm <span class=\"token function\">install</span> goldilocks <span class=\"token parameter variable\">--namespace</span> goldilocks fairwinds-stable/goldilocks</code></pre></div>\n<p><strong>Method 2‚Ää-‚ÄäManifests:</strong></p>\n<p>The <a href=\"https://github.com/FairwindsOps/goldilocks/tree/master/hack/manifests\" target=\"_blank\" rel=\"noopener noreferrer\">hack/manifests</a> directory contains collections of Kubernetes YAML definitions for installing the controller and dashboard components in cluster.</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\"><span class=\"token function\">git</span> clone https://github.com/FairwindsOps/goldilocks.git\n<span class=\"token builtin class-name\">cd</span> goldilocks\nkubectl create namespace goldilocks\nkubectl <span class=\"token parameter variable\">-n</span> goldilocks apply <span class=\"token parameter variable\">-f</span> hack/manifests/controller\nkubectl <span class=\"token parameter variable\">-n</span> goldilocks apply <span class=\"token parameter variable\">-f</span> hack/manifests/dashboard</code></pre></div>\n<h4 id=\"enable-namespace\" style=\"position:relative;\"><a href=\"#enable-namespace\" aria-label=\"enable namespace permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Enable Namespace</h4>\n<p>Pick an application namespace and label it like so in order to see some data:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">kubectl label ns goldilocks goldilocks.fairwinds.com/enabled<span class=\"token operator\">=</span>true</code></pre></div>\n<p>After that, you should start to see VPA objects in that namespace.</p>\n<h4 id=\"viewing-the-dashboard\" style=\"position:relative;\"><a href=\"#viewing-the-dashboard\" aria-label=\"viewing the dashboard permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Viewing the Dashboard</h4>\n<p>The default installation creates a ClusterIP service for the dashboard. You can access via port forward:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">kubectl <span class=\"token parameter variable\">-n</span> goldilocks port-forward svc/goldilocks-dashboard <span class=\"token number\">8080</span>:80</code></pre></div>\n<p>Then open your browser to <a href=\"http://localhost:8080\" target=\"_blank\" rel=\"noopener noreferrer\">http://localhost:8080</a>.</p>\n<h3 id=\"how-accurate-is-goldilocks\" style=\"position:relative;\"><a href=\"#how-accurate-is-goldilocks\" aria-label=\"how accurate is goldilocks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How Accurate is Goldilocks?</h3>\n<p>The Goldilocks open-source software is based entirely on the underlying VPA project, specifically the Recommender. In our experience, Goldilocks is a good starting point for setting your resource requests and limits. But every environment is different, and Goldilocks isn't a replacement for tuning your applications to your specific use cases.</p>\n<h2 id=\"-closing-thoughts\" style=\"position:relative;\"><a href=\"#-closing-thoughts\" aria-label=\" closing thoughts permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìù Closing Thoughts</h2>\n<p>So this is how Goldilocks can be used by Developers, SRE, and DevOps teams to calculate the right estimates for all their workloads and also from the FinOps perspective. Please feel free to share your experience while implementing this in your clusters.</p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/data-on-kubernetes-ai-ml-5/","title":"Data on Kubernetes: Part 5‚Ää-‚ÄäMaking AI/ML simpler","date":"2024-10-25 13:06:00"},"excerpt":"Using Kubernetes to deploy and scale AI/ML models efficiently üìà üìö Introduction Welcome back to our 'Data on Kubernetes' series. In this‚Ä¶","html":"<blockquote>\n<p><strong>Using Kubernetes to deploy and scale AI/ML models efficiently üìà</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>Welcome back to our 'Data on Kubernetes' series. In this fifth part, we're focusing on AI and machine learning (ML). We'll discuss how Kubernetes is transforming the use of <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a> software.</p>\n<p>The more we rely on <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a>, the more complex it becomes to manage these systems. Kubernetes offers a helping hand with features that allow AI/ML software to automatically adjust their size, update without stopping, and repair themselves. This keeps AI/ML systems always on and working smoothly.</p>\n<p>In this blog, we'll discover the tools <a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, and their integration with Kubernetes to simplify the process, all within the AWS cloud ecosystem.</p>\n<h2 id=\"-kubernetes-making-aiml-easier\" style=\"position:relative;\"><a href=\"#-kubernetes-making-aiml-easier\" aria-label=\" kubernetes making aiml easier permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Kubernetes: Making AI/ML Easier</h2>\n<p>When we use AI and ML, things can get pretty tricky. It's exciting but can also be a bit overwhelming with all the pieces you need to keep track of and there's a lot to manage. That's exactly why Kubernetes comes in‚Ää‚Äî‚Ääit's like having a smart assistant that helps keep everything in order.</p>\n<p>Kubernetes is great because it can fix itself. If something goes wrong with an AI program, Kubernetes doesn't wait around; it jumps right in to fix the problem.</p>\n<p>This means if an AI program stops working, Kubernetes doesn't just stand by. It jumps into action, fixing things up or starting fresh so that there's no downtime. This means you can rely on your AI and ML to work non-stop.</p>\n<p>But that's not all. Kubernetes is also smart about using resources. Sometimes AI needs more resources to work on big tasks, and sometimes it needs less.</p>\n<p>Kubernetes is smart enough to adjust on the fly, giving them more resources when they're busy and scaling back when they're not. This way, you're not wasting any energy or money, and everything runs super efficiently.</p>\n<h3 id=\"Ô∏è-ray-and-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-ray-and-kubernetes\" aria-label=\"Ô∏è ray and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Ray and Kubernetes</h3>\n<p>When working with AI and ML tasks, we require tools that can manage extensive workloads efficiently.</p>\n<p><a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> is such a tool, designed for <a href=\"https://www.python.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Python</a>, a widely-used programming language. It simplifies the execution of large-scale projects on multiple computers simultaneously.</p>\n<p>Ray serves as a comprehensive toolkit for deep learning, an AI branch that equips computers with cognitive abilities.</p>\n<p><a href=\"https://ray-project.github.io/q4-2021-docs-hackathon/0.4/ray-overview/what-and-why-ray/\" target=\"_blank\" rel=\"noopener noreferrer\">Developed by experts at UC Berkeley</a>, Ray's objective is to streamline large computing projects. It consists of Ray Core, the primary component for distributing tasks across computers, and Ray Ecosystem, a suite of specialized tools for various purposes, including project optimization and decision-making algorithms.</p>\n<p>While similar to Dask, a tool for executing Python code concurrently in different locations and processing multiple data-related tasks simultaneously, Ray stands out.</p>\n<p>It doesn't merely replicate existing tools like NumPy and Pandas; it's engineered to be versatile, catering to a wide array of projects beyond data science. Its adaptability has led major projects such as spaCy, Hugging Face, and XGBoost to incorporate Ray, enhancing their efficiency and intelligence.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAChElEQVR42mNgYGAQio+PMsjMzNcEsrmAmB2IOczszWRMdE2UzFSkZUyUlWX1xMW5geJsQMwDxAIMMhacDAwynPX19TJd9SUSxgwMrGD5/3/+ZN24dHrt6Yu3Th179Iiz9Pp/KQaGJN77axVnPTpgHciQ9F9Ktnm5snLVCiOZK/+FbMTYndmZGBJmhq6S6xROktq9Z8/RPXuOrNxz+1vVyRc/ihiePHn2euLlj1Yu847p6m18rhjcttrHKS5P78ia7OorF9bXfv32Y+fNV+/2zDn/9EDKyf8J9s7OHmYiPLahfnPU45ymS5+/eKPg8amydRuXlyxvnrCohQEMzOr5GIwjRRiSN4oDecxaWlpCTR3T27dv37P51fWzjy/cuf9q4em7jzceuxZ/bMOcpqubZueJWxaLWViAvM3A8GQVw+T5jca3/IPSZjHsPnN77e4zN1ftP3dnxY7Tt9btPXtn9Z5z99bsPXNz9dKjN9ZN2HV5/ab9pzZsO3Vzw65jl9fvOnx2345jF/duO3pp7ebDF9cA8frNh6+s2Xnq1obNhy9PYrj9/v//a6/+/Lv++u//G6///L/8/Me/Mw8+/7rx7Ou/g3c+fl184e33Pecf/jn36OPfk7ef/zh6/en3k7fffjt1583v03ff/gdhIP/v3Y///198/PUmw/Slm1VBeNbyrWr989crLN+82+Hk8ePdbZuOuGnverrNaO/Tyoe9kXFTmuaGFMx53lM752rN3Llz9yxfvbFh2a6zUos37FWft2YXWP+UJVvkGWAAGP1MIPrA8eOa9y8ebYw/8dyb8+D3hZyHf2WvW6DtZldxxDJ9yueOrElPc+bMmLx69ep1Gcj6YAAASclQlNbtCaIAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ray\" title=\"\" src=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png\" srcset=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/04472/ray.png 170w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/9f933/ray.png 340w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png 680w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b12f7/ray.png 1020w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b5a09/ray.png 1360w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/2cefc/ray.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead\">Source</a></div>\n<p>The diagram above shows that at a high level, the Ray ecosystem consists of:</p>\n<ul>\n<li>The core Ray system</li>\n<li>Scalable libraries for machine learning (both native and third party)</li>\n<li>Tools for <a href=\"https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208\" target=\"_blank\" rel=\"noopener noreferrer\">launching clusters on any cluster or cloud provider</a></li>\n</ul>\n<p>KubeRay amplifies Ray's capabilities by integrating it with Kubernetes, a platform that orchestrates numerous computers working in unison. This integration combines Ray's user-friendly Python interface with Kubernetes' robust and dependable framework.</p>\n<p>The result is a great system capable of developing, operating, and maintaining large-scale projects easily. It's akin to an advanced system that scales with your requirements, self-repairs, and autonomously performs maintenance and updates.</p>\n<p>The use of Kubernetes is advantageous for Ray. It acts as a command center for computers, ensuring smooth operation and task management. Kubernetes excels with both tools, adeptly managing numerous tasks without confusion.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABdElEQVR42lWR2Y6bQBBF+f9PykNe82ZrNmuc8Tjx2INNQ4MBs/V2Ug0ZK2npiKu69O1SVeKcw3l/x4fAZAxKZeR5LhRkSjGO4+z9+29kHAfS9JNCa4L4SZBAxFhwM0FQVUMhoefPE3VVIsW7fyc4+n7g8HFC62IJzDrH+WYXOs+ld2S953gT/ddTQ6z7uZ4PC1mkt0wuoK4D4zQRT3KUkJ+7PW+/P8hG2Mtrh/RC2kqQrmYKXfKr7DnmJanKhYJzrtnXBus8qhloTJCOpcPX2vFSDGzKiafSzvpFj6yynh+7s5DylLU8aMNKjazVcOe5NNJT4PH9JiNadLKrLc+VhF6FSgKvntcWHiTk++Mb39Zb1qJjLT6+bb0Q2NSeXeOw1nLJFO2tW2YYt+zjducNg201Q6O51IHVwRCMDH2T0rU129PE/lRTlTlN284B8V48UcesJIr/cAZnDaPM5NrZWXd1L19LI0vpBos109zZ150Y+qX/APzJZbQoEulJAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay\" title=\"\" src=\"/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png\" srcset=\"/static/a78dd992dd6340928c4606a616df4d61/04472/kuberay.png 170w,\n/static/a78dd992dd6340928c4606a616df4d61/9f933/kuberay.png 340w,\n/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png 680w,\n/static/a78dd992dd6340928c4606a616df4d61/b12f7/kuberay.png 1020w,\n/static/a78dd992dd6340928c4606a616df4d61/b5a09/kuberay.png 1360w,\n/static/a78dd992dd6340928c4606a616df4d61/29007/kuberay.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\">Source</a></div>\n<h3 id=\"-pytorch-and-kubernetes\" style=\"position:relative;\"><a href=\"#-pytorch-and-kubernetes\" aria-label=\" pytorch and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• PyTorch and Kubernetes</h3>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> is an open-source machine learning library for Python, widely used for applications such as natural language processing, computer vision, and deep learning.</p>\n<p>What makes PyTorch special is its ease of use, flexibility, and dynamic computational graph, which allows for quick prototyping and experimentation. Researchers like it because it's designed to work well with Python and makes deep learning straightforward.</p>\n<h4 id=\"pytorch-and-tensorflow\" style=\"position:relative;\"><a href=\"#pytorch-and-tensorflow\" aria-label=\"pytorch and tensorflow permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch and TensorFlow</h4>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> and <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a> are both strong in deep learning, but they're good at different things. TensorFlow, made by Google, is great for getting big machine learning models ready for use and for training them on many computers at once.</p>\n<p>PyTorch is more about being easy to work with and changing things as you go, which is really helpful when you're still figuring things out.</p>\n<h4 id=\"pytorch-with-ray-and-kuberay\" style=\"position:relative;\"><a href=\"#pytorch-with-ray-and-kuberay\" aria-label=\"pytorch with ray and kuberay permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch with Ray and KubeRay</h4>\n<p>When PyTorch works together with Ray or KubeRay, it gets even better at deep learning jobs. Ray and KubeRay help spread out the work over many computers, making things faster and more reliable.</p>\n<p>This integration facilitates the distribution of computational tasks across multiple nodes, enabling faster processing times and resilience against individual node failures.</p>\n<p>They also make better use of cloud services, saving money. This combination means developers can spend more time being creative with their machine learning models, while the tough tech stuff is taken care of.</p>\n<h4 id=\"pytorch-meets-kubernetes\" style=\"position:relative;\"><a href=\"#pytorch-meets-kubernetes\" aria-label=\"pytorch meets kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch meets Kubernetes</h4>\n<p>By using PyTorch with Kubernetes (also known as k8s), you get all the perks of Kubernetes' smart way of managing containers. Kubernetes helps set up, grow, and manage containers over lots of computers.</p>\n<p>This means PyTorch applications can handle big, complicated jobs better, grow more easily, and keep running smoothly. Developers can then put more energy into making their models, without worrying too much about the tech behind it.</p>\n<h2 id=\"Ô∏è-hands-on-ray-on-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-hands-on-ray-on-kubernetes\" aria-label=\"Ô∏è hands on ray on kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Hands-on: Ray on Kubernetes</h2>\n<p>Our setup consists of a Kubernetes cluster in Amazon EKS, which hosts our AI applications. The recommended method for installing Ray onto a Kubernetes cluster is through KubeRay, a Kubernetes Operator. The KubeRay Operator allows you to create and manage Ray clusters in a Kubernetes-native way by defining clusters as a custom resource called a RayCluster.</p>\n<p>The installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob, and RayService as documented <a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n<p>This RayCluster resource describes the desired state of a Ray cluster. The KubeRay Operator manages the cluster lifecycle, autoscaling, and monitoring functions.</p>\n<p>Hence, we use the KubeRay Operator for our Ray cluster installation.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 63.52941176470588%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABYUlEQVR42pWSi46CQAxF+f8/JBEkJoSHyhsVedQ5XcvK6ia7k1zHdNrTzh08Ebk53f+raZpU8zzfWc/44LmfWf6w8jyX/X4vURTpnqapHI9HybJM6rqW2425ZNkAXTc9qKpKk9q2FZoTL8tSLpeLXK9XjQM7n89yOp2kaZrPQBJJ2u12EgSBTgK4KAo9W5ZlMzVwmrzE34FMM46jiuloYNOgrutkGAa9ahzHkiSJnhN7A/Z9r12ZChjFeGUwfATAVFyTPEDucWzKbyABAAZB+MMUNKKQnRv8XJsrO/pMBxJ5DIrwjB3RgMkQNpBrltCEm5gc4wvIFRBTGcxeHQtohIADZJFHE/xj5ytYgXiBJwbEcF45DEMtMOM5N884831fDoeDAqlZgbwuQKagk02CaEbMPh97fWoAcTOaUb8CzUNk/zEaWdxEjBx7KIB81M/axXMJhbOkepU7XPXb2ae4Y5UPzzXwRaCIkHEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay operator\" title=\"\" src=\"/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png\" srcset=\"/static/0fe3a262cf7937c74bc1df905770e678/04472/kuberay-op.png 170w,\n/static/0fe3a262cf7937c74bc1df905770e678/9f933/kuberay-op.png 340w,\n/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png 680w,\n/static/0fe3a262cf7937c74bc1df905770e678/b12f7/kuberay-op.png 1020w,\n/static/0fe3a262cf7937c74bc1df905770e678/c61d0/kuberay-op.png 1145w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/sage-ai/demystifying-the-process-of-building-a-ray-cluster-110c67914a99\">Source</a></div>\n<p>KubeRay Operator managing the Ray Cluster lifecycle. Now let us deploy the infrastructure. Start by cloning the repo and change the working directory.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/data-on-eks\n<span class=\"token builtin class-name\">cd</span> data-on-eks/ai-ml/ray/terraform</code></pre></div>\n<p>Next, we can use the shell script <code class=\"language-text\">install.sh</code> to run the <code class=\"language-text\">terraform init</code> and <code class=\"language-text\">apply</code> commands.</p>\n<p>Update <code class=\"language-text\">variables.tf</code> to change the region. Also, we can update any other input variables or make any other changes to the terraform template.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">./install.sh</code></pre></div>\n<p>And now, we can run the PyTorch benchmark.</p>\n<p>We deploy a Ray Cluster with its own configuration for Karpenter workers. Different jobs can have different requirements for Ray Cluster such as a different version of Ray libraries or EC2 instance configuration such as making use of Spot market or GPU instances.</p>\n<p>To deploy the Ray cluster run below commands:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">cd</span> examples/pytorch\nterraform init\nterraform plan\nterraform apply -auto-approve</code></pre></div>\n<p>Once running, we can forward the port for the server:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl port-forward service/pytorch-kuberay-head-svc <span class=\"token parameter variable\">-n</span> pytorch <span class=\"token number\">8266</span>:8265</code></pre></div>\n<p>We can then submit the job for PyTorch benchmark workload:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">python job/pytorch_submit.py</code></pre></div>\n<p>You can open <a href=\"http://localhost:8266\" target=\"_blank\" rel=\"noopener noreferrer\">http://localhost:8266</a> to monitor the progress of the PyTorch benchmark.</p>\n<p>The <code class=\"language-text\">pytorch_submit.py</code> script is a benchmarking for evaluating the performance of training and tuning a PyTorch model on a distributed system using Ray.</p>\n<p>It measures the time taken to train a model and to find the best hyperparameters through tuning, providing insights into the efficiency of distributed machine learning workflows.</p>\n<h2 id=\"-key-takeaways\" style=\"position:relative;\"><a href=\"#-key-takeaways\" aria-label=\" key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Key Takeaways</h2>\n<p>Kubernetes makes working with AI/ML simpler. It scales AI/ML models easily and keeps them running smoothly. With Kubernetes, Ray, and PyTorch work better in the cloud, making AI/ML systems more reliable and easier to manage.</p>\n<br>\n<br>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìªüß° References:</strong></p>\n<ul>\n<li><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://docs.ray.io/en/latest/cluster/kubernetes/index.html</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws\" target=\"_blank\" rel=\"noopener noreferrer\">https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay\" target=\"_blank\" rel=\"noopener noreferrer\">https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay</a></li>\n</ul>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}