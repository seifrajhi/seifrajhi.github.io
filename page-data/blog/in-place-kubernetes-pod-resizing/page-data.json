{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/in-place-kubernetes-pod-resizing/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>In-Place Pod Resizing in Action ‚öôÔ∏è</strong></p>\n</blockquote>\n<h2 id=\"-kick-off\" style=\"position:relative;\"><a href=\"#-kick-off\" aria-label=\" kick off permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üéå Kick-off</h2>\n<p>Welcome to this exciting deep dive into one of the newest features in Kubernetes management‚Ää‚Äî‚Ääthe <strong>In-Place Pod Resource Resizing</strong> feature!</p>\n<p>In-place pod resource resizing is a new feature in Kubernetes that allows you to resize the CPU and memory resources allocated to a pod without restarting it. This can be a major advantage for many applications, as it can help to improve performance and efficiency without causing any downtime.</p>\n<p>In this blog post, we will take a deep dive into the in-place pod resource resizing feature. We will discuss how it works, the benefits it offers, and how to use it. We will also show you how to resize a pod in-place in action.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 500px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2UlEQVR42o1U70tTURi+f0L/QR/6AwoiiH5Anywiir4V9CGCMNA+RIJgDQwLEkQ/GELQF3XVQsyy2pRS04U/WqiRP2aIosvNbXduc7vb3b333Kdzzr3nbs4te8eBu/ec87zP+z7veyRUMdM0YZhkn5/5TPqrZlIFJGjEcK6wb1nNQi5kodsBTNsvgElJ4H2AAmg6sYGbM69wdrQTJz530NXOv28H3uDHTshhS8rYSqXMeJyMgsYJNw57n6B5YQiT8jq2cimEckn442tw/fLiyKfHuPXdA5XonHU4n64AaNhpTvgx/LILIVO1GZsWbRNOihldxY1pN65P9eBrbBUXxp87ZyXBjpvbDczOWi5CUDAIF0cYYULRNBIFBYNbCzg3+gyH+hvRMPfOKYEEYhd0aQmoqwd2d2Gwy7Z/M2ag2Z1Ey2uadlznvg0licv+Fzg23AbJfQfDkWAJoGCwtgbMzzv1ZEw6B3dxtC6Ma0/juPIoiuN3w9wnroRzaV7jrF5wBLVS1mnknh5AliEa4PeWhlP3I2h/m0afX4FnPIu2/hRO3otg5Y/GrxNzb2cURUmlgNpaIBbj6TL7OKNwQAY09jOPL3N5CpzF6YYI+r8pVo/SNJjKJWW2AfN5YGiIntBoHayNjoE0rrZEMTCZhS+Qg5euD9MKLjVHOWteM1JpUgT8yAgtStimbyKcMHCGsqntlPF+iqpKweq7EpxhSNb3NMdeQKFya6tVR6awbiU+E1Rx0RVFzcNtnKer5sE2ppZVu4WqzbIIs7IC9PUVm9x2L4c0+BdUTNC1tKk5D8e/HwfBcnER6O52WsekwOV5sYbHgYDiQC4HNDU5qbNAbIeULEterUii6vMlQFkLra9bY9jbC8TjQKHABUMmAwQCgMsF7OygkjJS+VvoHAgGLaHGxgCfD6inY+nxAKurxYn63wd2XzqsFMkkF+sgk6ruMFAGUA7O/lepHwP8C7nF1i4Jf/YXAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"resize\" title=\"\" src=\"/static/ae6469f85d6a2da77b247cfb4f4a4f7c/0b533/resize.png\" srcset=\"/static/ae6469f85d6a2da77b247cfb4f4a4f7c/04472/resize.png 170w,\n/static/ae6469f85d6a2da77b247cfb4f4a4f7c/9f933/resize.png 340w,\n/static/ae6469f85d6a2da77b247cfb4f4a4f7c/0b533/resize.png 500w\" sizes=\"(max-width: 500px) 100vw, 500px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"-exploring-the-long-awaited-in-place-pod-resource-resizing-in-kubernetes-127\" style=\"position:relative;\"><a href=\"#-exploring-the-long-awaited-in-place-pod-resource-resizing-in-kubernetes-127\" aria-label=\" exploring the long awaited in place pod resource resizing in kubernetes 127 permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìò Exploring the Long-Awaited In-Place Pod Resource Resizing in Kubernetes 1.27</h3>\n<p>Like many, I have been waiting for the ability to resize Kubernetes pods without restarting them for several years. This feature is now available in <a href=\"https://kubernetes.io/docs/setup/release/notes/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes 1.27</a>, and I am excited to try it out.</p>\n<p>This feature, called in-place pod resource resizing, allows you to change the CPU and memory resources allocated to a pod without having to restart it. This is a major advantage for many applications, as it can help to improve performance and efficiency without causing any downtime.</p>\n<p>The way in-place pod resource resizing works is by making the pod spec resources mutable. This means that Kubernetes can update the underlying c-group allocation in-place. This is particularly useful in the case of scaling pods vertically, such as with Kubernetes' built-in <a href=\"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Vertical Pod Autoscaler (VPA)</a>.</p>\n<p>Vertical scaling proves indispensable in various use cases, especially with stateful database workloads experiencing bursty traffic, where service disruptions can be costly. Additionally, <a href=\"https://www.youtube.com/watch?v=jjfa1cVJLwc&#x26;t=818s\" target=\"_blank\" rel=\"noopener noreferrer\">an exciting talk at KubeCon North America 2022</a> showcased the utilization of this in-place feature with eBPF, adding to its versatility.</p>\n<p>In this blog post, I will show you how to try out in-place pod resource resizing. I will also discuss the new changes you'll see in the pod spec. There are many ways to do this; this is just one simple example.</p>\n<h3 id=\"-a-hands-on-guide-to-in-place-pod-resource-resizing-in-kubernetes\" style=\"position:relative;\"><a href=\"#-a-hands-on-guide-to-in-place-pod-resource-resizing-in-kubernetes\" aria-label=\" a hands on guide to in place pod resource resizing in kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèó A Hands-On Guide to In-Place Pod Resource Resizing in Kubernetes</h3>\n<p>The new feature is introduced under the name <strong>InPlacePodVerticalScaling</strong>.</p>\n<p>Let's now start a test pod. Let's say that your application can safely change the amount of CPUs without restarting, but changing the amount of memory requires a restart. For example, a pod running a database has no problem with a CPU count change while running, but decreasing the amount of memory would cause unexpected behavior.</p>\n<p>To reflect this in the pod YAML, you need to set the <code class=\"language-text\">restartPolicy</code> to <code class=\"language-text\">RestartContainer</code> for the memory resource. Otherwise, the default behavior will be to attempt to update the resource in-place.</p>\n<p>Here is an example of a pod YAML that you can use:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 68.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABRUlEQVR42qVT226CQBQURE2Dt1JBUJEFqtxBoib9///aTGdjtH1oUpGHk2x298zOmZkdbJNEGuERAzuFtk4x6FsfQSEtUWEW1Bhty/6AflzLKLtiKk79wVQt/ELaYY2J32AWnWGKth/gSpTSCSssg5IjF9Dc7HGorV/QlQ3SI6AVnTCPr5jFFxhKSydh8YKd3NbPAvKyjA4NxLGFlX1hdTzD8HKMNjnMfQkrrqlvBY17zwGS4ZKNW+qoRjf9ggxzjHcFdI4/9DLorMEvKf4FnBJEfDZI0hZeROCkhagumPOBhXpk3yFOd4Y7jmbuSujcHLopjE1Gk3JMyFSN30HDVE7YvCGzNzY/DpQhTvJjTheGKhoMOFZCGVDeGDkv5lC5PKbo4eGEdwJqHFd/1oA/c+imUi3soIIyx6I5Vtx0y979I7gZvgFYAGQTxnq6lgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"pod\" title=\"\" src=\"/static/a36b0cd2c702a14531a42b8b9bd867dc/c5bb3/pod.png\" srcset=\"/static/a36b0cd2c702a14531a42b8b9bd867dc/04472/pod.png 170w,\n/static/a36b0cd2c702a14531a42b8b9bd867dc/9f933/pod.png 340w,\n/static/a36b0cd2c702a14531a42b8b9bd867dc/c5bb3/pod.png 680w,\n/static/a36b0cd2c702a14531a42b8b9bd867dc/b12f7/pod.png 1020w,\n/static/a36b0cd2c702a14531a42b8b9bd867dc/b5a09/pod.png 1360w,\n/static/a36b0cd2c702a14531a42b8b9bd867dc/29007/pod.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>To apply the YAML and ensure it's running, follow these steps:</p>\n<ol>\n<li><strong>Save the YAML content to a file</strong>, for example, <code class=\"language-text\">test-pod.yaml</code>.</li>\n<li><strong>Apply the YAML</strong> to create the pod in your Kubernetes cluster.</li>\n<li><strong>Wait until the pod is ready and running</strong>:</li>\n</ol>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 26.47058823529412%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAv0lEQVR42pWQyRKCQAxEB0FgQD2poIKIIOJQIqKAu///T1htXM4uh65KKsnrJMwORK2FMVhfoGEvINkC8pA0EGBW/Jckmme6X9Wt8ARzdgKfltAmJbrJFYaXkcn8byhT3azWvS34pIBOUkZLqOMcrDd/AfsRxdE7/sGgHZ1vZngEH6ZohQcYwf55+gPeGCQE34D7FWQnRdNd09AXKPeLmxHsqDmHRtuZXg7FydCOLtBoU+6unrlKtY6ff/6hFeMO45iAlE/LdCgAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"apply\" title=\"\" src=\"/static/ad7d776e6114215c74b4b2b8c0165d6a/c5bb3/apply.png\" srcset=\"/static/ad7d776e6114215c74b4b2b8c0165d6a/04472/apply.png 170w,\n/static/ad7d776e6114215c74b4b2b8c0165d6a/9f933/apply.png 340w,\n/static/ad7d776e6114215c74b4b2b8c0165d6a/c5bb3/apply.png 680w,\n/static/ad7d776e6114215c74b4b2b8c0165d6a/b12f7/apply.png 1020w,\n/static/ad7d776e6114215c74b4b2b8c0165d6a/b5a09/apply.png 1360w,\n/static/ad7d776e6114215c74b4b2b8c0165d6a/29007/apply.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Under the pod spec, one of the new and notable features to observe is the <code class=\"language-text\">resizePolicy</code>. This section defines how the pod's resources can be resized and specifies the behavior for different resource types, such as memory and CPU.</p>\n<p>For memory resources, the <code class=\"language-text\">resizePolicy</code> indicates that changes to the memory allocation require a restart of the container. This is denoted by setting the <code class=\"language-text\">restartPolicy</code> to <code class=\"language-text\">RestartContainer</code> for the \"memory\" resource. In contrast, for CPU resources, a restart is not necessary during resizing, as indicated by the <code class=\"language-text\">restartPolicy</code> set to <code class=\"language-text\">NotRequired</code> for the \"cpu\" resource.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 32.94117647058823%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAoElEQVR42q3Ryw6CMBQE0EYBDWAIGlBEKPIoAsVWjPr/XzaWunANYTGZ3cncXGIWFUhwAzkulF3awYoarE/NMuCecngphxG1IGH9z1zQTTiCrEeo4iQdiFq6GtfOxbdxizi/IyuFbjd/wmcfWJceZsx1JoGbc4uCCdT1AyWT8Ku3Bu1MKnyATeU00FAPOdDf2WM7VMArX3Cvg15JAjYJ/AIUirIjczW/sgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"pod-resize\" title=\"\" src=\"/static/8ad89fa2a863c446eae064582084b46f/c5bb3/pod-resize.png\" srcset=\"/static/8ad89fa2a863c446eae064582084b46f/04472/pod-resize.png 170w,\n/static/8ad89fa2a863c446eae064582084b46f/9f933/pod-resize.png 340w,\n/static/8ad89fa2a863c446eae064582084b46f/c5bb3/pod-resize.png 680w,\n/static/8ad89fa2a863c446eae064582084b46f/b12f7/pod-resize.png 1020w,\n/static/8ad89fa2a863c446eae064582084b46f/b5a09/pod-resize.png 1360w,\n/static/8ad89fa2a863c446eae064582084b46f/29007/pod-resize.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Under the status field, new and noteworthy information includes the <code class=\"language-text\">allocatedResources</code> and <code class=\"language-text\">resources</code> fields. <code class=\"language-text\">allocatedResources</code> reflects the current resource allocation to the pod's containers, while <code class=\"language-text\">resources</code> represents the desired or to-be-updated resource specifications.</p>\n<p>This distinction provides real-time insights into resource utilization and allows effective monitoring and optimization of the pod's resource management. These fields are nested under <code class=\"language-text\">containerStatuses</code>, presenting a comprehensive view of each container's status and resource usage. Understanding this information is vital for fine-tuning resource allocation in the Kubernetes environment.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 45.88235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAv0lEQVR42qWSyw6CMBREiwQfAQ0xgDTIW6HI00T//78MzXgLG7fCYtJFb05mMsN4Lj77WyGZK6R2EZIpeeX8/inNryTb8mr00w5e0iLIOjhJA4M/QAfLxDwx2lGDrHgiLAa4aQ/zWq8BltLgFeyoxi5sYeUv+ijXAY9hAxXbUs68EtpS2K9DN25hUvSNvwI2AeeGEOY9gvuAQzyA2lrnUCcAT1s41LJOblXsxUDNV7sTcGkuZ5rO6faeXC6BqmRf48HjPEXUmagAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cpu-mem\" title=\"\" src=\"/static/6080ec0104832e08d9f74b51e2eee444/c5bb3/cpu-mem.png\" srcset=\"/static/6080ec0104832e08d9f74b51e2eee444/04472/cpu-mem.png 170w,\n/static/6080ec0104832e08d9f74b51e2eee444/9f933/cpu-mem.png 340w,\n/static/6080ec0104832e08d9f74b51e2eee444/c5bb3/cpu-mem.png 680w,\n/static/6080ec0104832e08d9f74b51e2eee444/b12f7/cpu-mem.png 1020w,\n/static/6080ec0104832e08d9f74b51e2eee444/b5a09/cpu-mem.png 1360w,\n/static/6080ec0104832e08d9f74b51e2eee444/29007/cpu-mem.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h3 id=\"cpu-resize\" style=\"position:relative;\"><a href=\"#cpu-resize\" aria-label=\"cpu resize permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>CPU Resize</h3>\n<p>To begin, let's adjust the CPU limits of the pod, increasing them from 2 to 3. We'll accomplish this using the command line by applying a patch:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 19.411764705882355%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAiElEQVR42n2P0QrDIAxF+w1anTWaDhu3l/3/38ldtBRayvZwuJcYjmT6iG9+nTEbZb5irVV6d7e3X0wipdUq2LaCI0speL0reGWIbGDOyLwg5U7U+c4j+LswUmj8JFBakFdC75EWkJKZhiDGMGS6O/Z6Jv2gz29CPakZY/fzNI9uTv0fZ5lzDl+eW2XxMn/ZYQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"check\" title=\"\" src=\"/static/b28b0ae003f90c89860865a77272f8e6/c5bb3/check.png\" srcset=\"/static/b28b0ae003f90c89860865a77272f8e6/04472/check.png 170w,\n/static/b28b0ae003f90c89860865a77272f8e6/9f933/check.png 340w,\n/static/b28b0ae003f90c89860865a77272f8e6/c5bb3/check.png 680w,\n/static/b28b0ae003f90c89860865a77272f8e6/b12f7/check.png 1020w,\n/static/b28b0ae003f90c89860865a77272f8e6/b5a09/check.png 1360w,\n/static/b28b0ae003f90c89860865a77272f8e6/29007/check.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Upon inspecting the pod now (using <code class=\"language-text\">kubectl get pod testinplace -o yaml</code>), it is probable (though not guaranteed) that you will observe the appearance of the <code class=\"language-text\">resize</code> field. Additionally, you will notice the pod spec resources displaying the updated value, while the pod status resources retain the previous value.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 84.70588235294117%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAABC0lEQVR42q2USW6EMBBF+wihBmNoBps56c79L2fpp0wvsscskBEST7/8n/14R5eaicEkUBFI4fOIA6dnq/gyIN8BdCpp7AVLJPRPg3IhkIhT4xmvjTB0DsRcBhThlFP9rBWO1SGECOdcCVBS7QSvvcI6K1TVxr6e8mE/p4oykLAvin0/MI4jiOgasPWc/svgc9yc8nLCrpUUR4W1bbAPlKVgZIOkKQjmQKc6S8wptWQPPx5uM6G7w8MMrB3jbaWEobaPeo+Hx0LmoZ4NF5WSE+ZCfo/KxrZyXFnKU+z88r1mD209doQSDzMwDnY5WMvbLAh9bddYQULv1TxkDH2DOC2Y5wXe+0vHL+/9H+4UqAaHbBuMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"resized\" title=\"\" src=\"/static/ae92bf0867b1c10086bd3265e9a817db/c5bb3/resized.png\" srcset=\"/static/ae92bf0867b1c10086bd3265e9a817db/04472/resized.png 170w,\n/static/ae92bf0867b1c10086bd3265e9a817db/9f933/resized.png 340w,\n/static/ae92bf0867b1c10086bd3265e9a817db/c5bb3/resized.png 680w,\n/static/ae92bf0867b1c10086bd3265e9a817db/b12f7/resized.png 1020w,\n/static/ae92bf0867b1c10086bd3265e9a817db/b5a09/resized.png 1360w,\n/static/ae92bf0867b1c10086bd3265e9a817db/29007/resized.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>The duration for the pod to transition from <code class=\"language-text\">resize: InProgress</code> to <code class=\"language-text\">resize: complete</code> may vary depending on various factors. The time taken for the resizing process to finish could be different for different pods or environments.</p>\n<p>In case you encounter a different flag, such as <code class=\"language-text\">resize: Infeasible</code>, it indicates that the resizing process is not feasible. To address this, check your node's available resources to ensure they are sufficient to accommodate the requested changes. Insufficient resources may prevent the successful completion of the resizing operation.</p>\n<h3 id=\"memory-resize\" style=\"position:relative;\"><a href=\"#memory-resize\" aria-label=\"memory resize permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Memory Resize</h3>\n<p>Continuing with memory adjustments, let's raise the limits from 1G to 2G:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 16.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAjUlEQVR42lWOSw6DMAxEuUP8DRQSCGXR+98Oaeqki7aL0Ywta/ym157v5TCoGMwcpt0N7hkiOrKqRuYh7q4SWZCIkBKB6Ktpq+W+rhOt7XiGn+eBo9WRS1nRYt7C1zLjsWaUfcFWQzHnOZ67/hcy063eP1KQcBzIoGBO8Bxk+iEzk+GeO7EMyl7W97+Fb7Q9TX7byQ4KAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"mem-resize\" title=\"\" src=\"/static/20cf12e2b44f781630e3999632ef162b/c5bb3/mem-resize.png\" srcset=\"/static/20cf12e2b44f781630e3999632ef162b/04472/mem-resize.png 170w,\n/static/20cf12e2b44f781630e3999632ef162b/9f933/mem-resize.png 340w,\n/static/20cf12e2b44f781630e3999632ef162b/c5bb3/mem-resize.png 680w,\n/static/20cf12e2b44f781630e3999632ef162b/b12f7/mem-resize.png 1020w,\n/static/20cf12e2b44f781630e3999632ef162b/b5a09/mem-resize.png 1360w,\n/static/20cf12e2b44f781630e3999632ef162b/29007/mem-resize.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>The process remains identical to the previous step, involving the use of the <code class=\"language-text\">resize</code> tag and inspecting the pod status. To ensure that the resizing process is complete, verify the corresponding field or resource status. In my setup, this typically takes approximately 15 seconds to 1 minute, although it's essential to note that there's currently a bug that might cause it to take longer.</p>\n<p>After confirming the successful resizing completion, proceed to verify whether the restart occurred based on the flag we set earlier. The restart behavior should align with the specified flag, where memory changes triggered a container restart, while CPU changes did not necessitate a restart. This validation ensures that the In-Place Pod Resource Resizing feature is functioning as expected, allowing for seamless resource updates without unnecessary container disruptions.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 16.470588235294116%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAfklEQVR42n2NQQ7DIAwE8xCg1GAMhJAiIfX//0LaUtpDL+lh5dHKHm8t34c7CDdLsORgjIHRGnrmzXqx+uFPf5Ut1DbkeIA8Q2pDKOeUWziRNUkifMxr2dL36R/pFkodUs916JgR9gzigNyfU5oQUkLpHT7t4BTBOUMrdSl8AUPjU9xl/pwRAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"checking\" title=\"\" src=\"/static/1cb62436359cdcff026dd877b0073ee3/c5bb3/checking.png\" srcset=\"/static/1cb62436359cdcff026dd877b0073ee3/04472/checking.png 170w,\n/static/1cb62436359cdcff026dd877b0073ee3/9f933/checking.png 340w,\n/static/1cb62436359cdcff026dd877b0073ee3/c5bb3/checking.png 680w,\n/static/1cb62436359cdcff026dd877b0073ee3/b12f7/checking.png 1020w,\n/static/1cb62436359cdcff026dd877b0073ee3/b5a09/checking.png 1360w,\n/static/1cb62436359cdcff026dd877b0073ee3/29007/checking.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Indeed, the process is complete! It's exciting to have experienced the new In-Place Pod Resource Resizing feature firsthand. The ability to resize resources without disrupting the pod is a game-changer, especially for stateful applications that require vertical pod autoscaling.</p>\n<h2 id=\"-takeaway\" style=\"position:relative;\"><a href=\"#-takeaway\" aria-label=\" takeaway permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Takeaway</h2>\n<p>In this blog post, we have explored the new in-place pod resource resizing feature in Kubernetes. We have seen how this feature can be used to resize pods without having to restart them.</p>\n<p>In-place pod resource resizing is a powerful feature that can be used to improve the performance and efficiency of Kubernetes applications. It can also be used to reduce downtime and improve the overall reliability of Kubernetes clusters.</p>\n<p>I hope this blog post has been helpful.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":5,"rawMarkdownBody":"\n> **In-Place Pod Resizing in Action ‚öôÔ∏è**\n\n## üéå Kick-off\n\nWelcome to this exciting deep dive into one of the newest features in Kubernetes management‚Ää‚Äî‚Ääthe **In-Place Pod Resource Resizing** feature!\n\nIn-place pod resource resizing is a new feature in Kubernetes that allows you to resize the CPU and memory resources allocated to a pod without restarting it. This can be a major advantage for many applications, as it can help to improve performance and efficiency without causing any downtime.\n\nIn this blog post, we will take a deep dive into the in-place pod resource resizing feature. We will discuss how it works, the benefits it offers, and how to use it. We will also show you how to resize a pod in-place in action.\n\n![resize](./resize.png)\n\n### üìò Exploring the Long-Awaited In-Place Pod Resource Resizing in Kubernetes 1.27\n\nLike many, I have been waiting for the ability to resize Kubernetes pods without restarting them for several years. This feature is now available in [Kubernetes 1.27](https://kubernetes.io/docs/setup/release/notes/), and I am excited to try it out.\n\nThis feature, called in-place pod resource resizing, allows you to change the CPU and memory resources allocated to a pod without having to restart it. This is a major advantage for many applications, as it can help to improve performance and efficiency without causing any downtime.\n\nThe way in-place pod resource resizing works is by making the pod spec resources mutable. This means that Kubernetes can update the underlying c-group allocation in-place. This is particularly useful in the case of scaling pods vertically, such as with Kubernetes' built-in [Vertical Pod Autoscaler (VPA)](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler).\n\nVertical scaling proves indispensable in various use cases, especially with stateful database workloads experiencing bursty traffic, where service disruptions can be costly. Additionally, [an exciting talk at KubeCon North America 2022](https://www.youtube.com/watch?v=jjfa1cVJLwc&t=818s) showcased the utilization of this in-place feature with eBPF, adding to its versatility.\n\nIn this blog post, I will show you how to try out in-place pod resource resizing. I will also discuss the new changes you'll see in the pod spec. There are many ways to do this; this is just one simple example.\n\n### üèó A Hands-On Guide to In-Place Pod Resource Resizing in Kubernetes\n\nThe new feature is introduced under the name **InPlacePodVerticalScaling**.\n\nLet's now start a test pod. Let's say that your application can safely change the amount of CPUs without restarting, but changing the amount of memory requires a restart. For example, a pod running a database has no problem with a CPU count change while running, but decreasing the amount of memory would cause unexpected behavior.\n\nTo reflect this in the pod YAML, you need to set the `restartPolicy` to `RestartContainer` for the memory resource. Otherwise, the default behavior will be to attempt to update the resource in-place.\n\nHere is an example of a pod YAML that you can use:\n\n![pod](./pod.png)\n\nTo apply the YAML and ensure it's running, follow these steps:\n\n1. **Save the YAML content to a file**, for example, `test-pod.yaml`.\n2. **Apply the YAML** to create the pod in your Kubernetes cluster.\n3. **Wait until the pod is ready and running**:\n\n![apply](./apply.png)\n\nUnder the pod spec, one of the new and notable features to observe is the `resizePolicy`. This section defines how the pod's resources can be resized and specifies the behavior for different resource types, such as memory and CPU.\n\nFor memory resources, the `resizePolicy` indicates that changes to the memory allocation require a restart of the container. This is denoted by setting the `restartPolicy` to `RestartContainer` for the \"memory\" resource. In contrast, for CPU resources, a restart is not necessary during resizing, as indicated by the `restartPolicy` set to `NotRequired` for the \"cpu\" resource.\n\n![pod-resize](./pod-resize.png)\n\nUnder the status field, new and noteworthy information includes the `allocatedResources` and `resources` fields. `allocatedResources` reflects the current resource allocation to the pod's containers, while `resources` represents the desired or to-be-updated resource specifications.\n\nThis distinction provides real-time insights into resource utilization and allows effective monitoring and optimization of the pod's resource management. These fields are nested under `containerStatuses`, presenting a comprehensive view of each container's status and resource usage. Understanding this information is vital for fine-tuning resource allocation in the Kubernetes environment.\n\n![cpu-mem](./cpu-mem.png)\n\n### CPU Resize\n\nTo begin, let's adjust the CPU limits of the pod, increasing them from 2 to 3. We'll accomplish this using the command line by applying a patch:\n\n![check](./check.png)\n\nUpon inspecting the pod now (using `kubectl get pod testinplace -o yaml`), it is probable (though not guaranteed) that you will observe the appearance of the `resize` field. Additionally, you will notice the pod spec resources displaying the updated value, while the pod status resources retain the previous value.\n\n![resized](./resized.png)\n\nThe duration for the pod to transition from `resize: InProgress` to `resize: complete` may vary depending on various factors. The time taken for the resizing process to finish could be different for different pods or environments.\n\nIn case you encounter a different flag, such as `resize: Infeasible`, it indicates that the resizing process is not feasible. To address this, check your node's available resources to ensure they are sufficient to accommodate the requested changes. Insufficient resources may prevent the successful completion of the resizing operation.\n\n### Memory Resize\n\nContinuing with memory adjustments, let's raise the limits from 1G to 2G:\n\n![mem-resize](./mem-resize.png)\n\nThe process remains identical to the previous step, involving the use of the `resize` tag and inspecting the pod status. To ensure that the resizing process is complete, verify the corresponding field or resource status. In my setup, this typically takes approximately 15 seconds to 1 minute, although it's essential to note that there's currently a bug that might cause it to take longer.\n\nAfter confirming the successful resizing completion, proceed to verify whether the restart occurred based on the flag we set earlier. The restart behavior should align with the specified flag, where memory changes triggered a container restart, while CPU changes did not necessitate a restart. This validation ensures that the In-Place Pod Resource Resizing feature is functioning as expected, allowing for seamless resource updates without unnecessary container disruptions.\n\n![checking](./checking.png)\n\nIndeed, the process is complete! It's exciting to have experienced the new In-Place Pod Resource Resizing feature firsthand. The ability to resize resources without disrupting the pod is a game-changer, especially for stateful applications that require vertical pod autoscaling.\n\n## üîö Takeaway\n\nIn this blog post, we have explored the new in-place pod resource resizing feature in Kubernetes. We have seen how this feature can be used to resize pods without having to restart them.\n\nIn-place pod resource resizing is a powerful feature that can be used to improve the performance and efficiency of Kubernetes applications. It can also be used to reduce downtime and improve the overall reliability of Kubernetes clusters.\n\nI hope this blog post has been helpful.\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1156},"frontmatter":{"id":"53ee158b46c1beb798048aa1","path":"/blog/in-place-kubernetes-pod-resizing/","humanDate":"Oct 28, 2024","fullDate":"2024-10-28","title":"In-Place Kubernetes Pod Resource Resizing Feature: A Deep Dive üîç","keywords":["Kubernetes","pod resizing","SRE","k8s","DevOps","AWS"],"excerpt":"Explore the in-place resizing feature for Kubernetes pods, allowing dynamic resource adjustments without downtime, enhancing application performance and resource management.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAACkklEQVR42g2Qa0tTAQBAL/Ryyx5Wbrvubm27c7vb7tzddqdz5lzOR2kPU1MrsaKkIK1MMCGMykdSZIQ96N2HyIqQCiGKiFKJijLoQ1QQBEEEBUH9gNN+wDkcjjDU20ThqhXYQ/momoeh/S1sWhfH6RKprCqlpXktoXiEgCKheCV6OjfwfvIqvqhKjmQhkNIZHe6ku6uV4pgf4fvHCY4daqU0XYBbNjHU1cTJ7iYUnwO/Nw8tEUR224modqpX6sw8v8jwYDtixEdlY5qatUk2rEugxDWO9m5H+Pllkg/PznN4d5pUMJeSApmdG1O4/TLBoJtQQiOcgTXFTH9/N9MvxrE4RcJxlea6chLxMJY8E7EiP/dvnUD49v4Bj8aGOdbTzI6GKPmKHavdRFkyRMAnYzQY8HokNJ/I5dPd3DjfR7bNQv2WGswZ0YKcHJZ7HDQ2lTMy0o/w9uk1Ht0coKN9DaLTyoryKGpYJpT5UVmlE4nkU6QrWGWJvj2NXDrewZzZszg72svNi32UJzXaNlexdVs9PQf3Ijy+fZxbF3rp2rmaVFmYoriCM+KiuCxKa53O+oQHb6Y6z2amblUJn19cZ/BIB/9+vObr1BXadtXS0lZBolRnZHQQ4dRAO7U1MWK6h3RVMZquUlEdIbayANFlJlWoIFmWYTYtwbggm7EzB/jz5hy/Zu7w5O4g3kQYY+5SbGo+U5MPEQ7ua6Yw7sXutqIE5MxDB8lkmERFFK/qQA86kaxmbA4JQ1YWoriEdCrIq4kTTI0PoFWUMHfxQjoPtPP39yeEhtZq5Lgf0e/CkLMIQ7YRURKRAy5sThMmcRmmvFysLomlGdA434gwL4uXj0eZGDuCsyiIGvIy/eAC76bu8R8eklb9xgFFMwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/07c324a8fbb2073dd11932a738d0da1b/6bffe/resizing-cover.png","srcSet":"/static/07c324a8fbb2073dd11932a738d0da1b/402d0/resizing-cover.png 750w,\n/static/07c324a8fbb2073dd11932a738d0da1b/6bffe/resizing-cover.png 825w","sizes":"100vw"},"sources":[{"srcSet":"/static/07c324a8fbb2073dd11932a738d0da1b/5e135/resizing-cover.webp 750w,\n/static/07c324a8fbb2073dd11932a738d0da1b/63ce3/resizing-cover.webp 825w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.44969696969696976}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/kubernetes-pod-priority-preemption/","title":"Kubernetes Pod Priority and Preemption: How to Ensure Your Critical Pods Get the Resources They Need üöÄ","date":"2024-10-28 20:30:00"},"excerpt":"A Guide to Managing Pod Priorities üìä üóØ Introduction When running a Kubernetes cluster, it's important to ensure that your critical pods‚Ä¶","html":"<blockquote>\n<p><strong>A Guide to Managing Pod Priorities üìä</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóØ Introduction</h2>\n<p>When running a Kubernetes cluster, it's important to ensure that your critical pods get the resources they need to function properly. This can be difficult to do if your cluster is under heavy load or if there are not enough resources available.</p>\n<p>One way to ensure that your critical pods get the resources they need is to use Kubernetes pod priority and preemption. Pod priority allows you to assign a numerical value to each pod, indicating its importance. Pods with higher priority will be scheduled before pods with lower priority, even if the lower-priority pods have been waiting longer.</p>\n<p>Pod preemption allows the scheduler to evict lower-priority pods from nodes in order to make room for higher-priority pods. This can be a useful way to ensure that your critical pods are always running, even if your cluster is under heavy load.</p>\n<p>In this tutorial, you will learn about pod priority, preemption, and pod PriorityClass. You will also learn how to use these features to ensure that your critical pods always get the resources they need.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.294117647058826%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqElEQVR42mN4biyFQCbSz81kn5vKPjeWeWks+GFrzNWvX668+/Twz8/p+z8WLnhTvvRt2dJ3FUtfbLz04v6vnwwvjKVAyEgSROqLvVDjeaHJ/8JY8oW22McWoxtfnt/99n3H7a9qKc+UEp5opjxRTXoiF/NMO+HohjN3QZqfm8i80BF61d/wYtf6Fz11LwriXtirv9AV+9qp+vTjqZvf/+fNehbdftur7olB9jOn8scZk+9b598Maz7B8NxY+p2RxCsz2ScXT7369/vlt8+v/vx8vX31M02JL/WSXz8vm7D3d0DNrQVbNoU3X5ONfR7acn/upk1RbTdVYs8zvDWWOm0sl2ZufHjdyreXzzw9fvDFpTOvt658piXxJkLq46cdEw/+N8l7Ft56MaztcUz/16Cmx7GdF5yqX2gmXWT4ayrRqqMUmpb9+sjeJwd3vzxz9OrWDdfTw99oCr2yFHh+bvqVz2+b2pdlFi/NrduzvKkjvOWZd+S8wMQlhokHGfabqy5RE7/cWfP63+8Xr59++PDq+IWz4Y6ON/SlPlmIPVhtv/eg9685Yu8mi3dkZc+01gyrvRUV2RzunVo0aSLDVjPVl5r87+tyXjy8/fLpg9dfPz5+8yLe3XWhvtJ7c4nXi0ynT8u53m1+rc9p87INu/ZdbVzwwDX3uF/u6v557QwfjCVfgiP5hY3Ki2i3Fy3FTyc2vzWTXW+ifEFH4mGZXefM5VU1/aUVHT2Tl06cvXbijJUdXdPbu2a2961keGks+QqCDMReaQu+UuN+qS34yVjigKniCV2Zt+FqU+Ysqm6ZWV7f39a7qGfyyu7JK4Coa9Ky7snLGB6ayiKQmdxDc4X7ZvIvTWVWmqmfM1Z4bic7v7+tbfLq3inLOicu6ZiwGIiADBCasAQA8jJtysFkkuQAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"evicted\" title=\"\" src=\"/static/278f1480db9dd8ddcdccf94ce99836f5/c5bb3/evicted.png\" srcset=\"/static/278f1480db9dd8ddcdccf94ce99836f5/04472/evicted.png 170w,\n/static/278f1480db9dd8ddcdccf94ce99836f5/9f933/evicted.png 340w,\n/static/278f1480db9dd8ddcdccf94ce99836f5/c5bb3/evicted.png 680w,\n/static/278f1480db9dd8ddcdccf94ce99836f5/b12f7/evicted.png 1020w,\n/static/278f1480db9dd8ddcdccf94ce99836f5/c1b63/evicted.png 1200w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-kubernetes-pod-priority-enhancing-scheduling-efficiency\" style=\"position:relative;\"><a href=\"#-kubernetes-pod-priority-enhancing-scheduling-efficiency\" aria-label=\" kubernetes pod priority enhancing scheduling efficiency permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Kubernetes Pod Priority: Enhancing Scheduling Efficiency</h2>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/\" target=\"_blank\" rel=\"noopener noreferrer\">Pod priority</a> is a Kubernetes scheduling feature that allows you to assign a numerical value to each pod, indicating its importance. Pods with higher priority will be scheduled before pods with lower priority, even if the lower-priority pods have been waiting longer.</p>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption\" target=\"_blank\" rel=\"noopener noreferrer\">Pod preemption</a> is a feature that allows the Kubernetes scheduler to evict lower-priority pods from nodes in order to make room for higher-priority pods. This can be useful for ensuring that your critical pods are always running, even if your cluster is under heavy load.</p>\n<p>There are two main concepts related to pod priority:</p>\n<ul>\n<li><strong><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass\" target=\"_blank\" rel=\"noopener noreferrer\">Pod priority class</a></strong>: A pod priority class is a non-namespaced object that defines a mapping from a name to the integer value of the priority. The higher the value, the higher the priority.</li>\n<li><strong><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#non-preempting-priority-class\" target=\"_blank\" rel=\"noopener noreferrer\">Pod preemption policy</a></strong>: The preemption policy determines whether or not Kubernetes will preempt lower-priority pods to make room for higher-priority pods. The default preemption policy is PreemptLowerPriority, which means that Kubernetes will preempt lower-priority pods if there are no resources available for higher-priority pods.</li>\n</ul>\n<h2 id=\"-pod-preemption-ensuring-high-priority-task-execution\" style=\"position:relative;\"><a href=\"#-pod-preemption-ensuring-high-priority-task-execution\" aria-label=\" pod preemption ensuring high priority task execution permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ° Pod Preemption: Ensuring High-Priority Task Execution</h2>\n<p>Within the Kubernetes ecosystem, the concept of Pod preemption emerges as a strategic feature designed to uphold optimal resource utilization. This functionality empowers Kubernetes to elegantly oust lower-priority pods from nodes whenever the scheduling queue harbors higher-priority counterparts demanding resources that are currently unavailable.</p>\n<h2 id=\"-kubernetes-pod-priority-class-fine-tuning-priority-assignment\" style=\"position:relative;\"><a href=\"#-kubernetes-pod-priority-class-fine-tuning-priority-assignment\" aria-label=\" kubernetes pod priority class fine tuning priority assignment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üéØ Kubernetes Pod Priority Class: Fine-Tuning Priority Assignment</h2>\n<p>In the intricate dance of Kubernetes resource allocation, the Kubernetes Pod Priority Class takes center stage. This indispensable construct facilitates the allocation of specific priorities to pods, enabling meticulous control over task execution sequences.</p>\n<p>By harnessing the PriorityClass object (which operates outside the bounds of namespacing), administrators can seamlessly designate priorities for pods. The cornerstone of this assignment lies in the 'Value' parameter‚Ää-‚Ääa numerical indicator that effectively steers the order of execution. The range for this value spans from 1 to 1,000,000,000 (one billion), with a simple principle: the larger the value, the more pronounced the priority bestowed upon the pod.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> scheduling.k8s.io/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PriorityClass\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> high<span class=\"token punctuation\">-</span>priority\n<span class=\"token key atrule\">value</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1000000</span>\n<span class=\"token key atrule\">preemptionPolicy</span><span class=\"token punctuation\">:</span> Never\n<span class=\"token key atrule\">globalDefault</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n<span class=\"token key atrule\">description</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"This priority class for backends\"</span></code></pre></div>\n<p>The name of the priorityclass (priorityClassName) will be used in the pod specification to set the priority. If you don't want the priority class to preempt the pods, you can set <code class=\"language-text\">PreemptionPolicy: Never</code>. By default, Priorityclasss use PreemptLowerPriority policy.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> test\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n        <span class=\"token key atrule\">imagePullPolicy</span><span class=\"token punctuation\">:</span> IfNotPresent\n    <span class=\"token key atrule\">priorityClassName</span><span class=\"token punctuation\">:</span> high<span class=\"token punctuation\">-</span>priority</code></pre></div>\n<h2 id=\"-shielding-critical-system-pods-in-kubernetes-high-priorityclasses\" style=\"position:relative;\"><a href=\"#-shielding-critical-system-pods-in-kubernetes-high-priorityclasses\" aria-label=\" shielding critical system pods in kubernetes high priorityclasses permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ° Shielding Critical System Pods in Kubernetes: High PriorityClasses</h2>\n<p>Securing pivotal pods from preemption within the Kubernetes ecosystem is a vital concern. To address this, Kubernetes has introduced two preconfigured high-priority classes, tailored to safeguard system-critical operations.</p>\n<ul>\n<li><strong>system-node-critical</strong>: This priority class is endowed with a numerical value of 2000001000. It is exclusively designated for static pods that play a crucial role in the system, such as etcd, kube-apiserver, kube-scheduler, and Controller Manager. The utilization of this priority class ensures that these fundamental components are shielded from preemption.</li>\n<li><strong>system-cluster-critical</strong>: Boasting a priority value of 2000000000, this priority class serves as the bastion for essential Addon Pods. Noteworthy components like coredns, calico controller, metrics server, and more align with this priority class. By aligning with the system-cluster-critical class, these Addon Pods are granted a robust shield against preemption, preserving the integrity of your Kubernetes cluster.</li>\n</ul>\n<h2 id=\"Ô∏è-kubernetes-pod-priority--preemption-how-it-all-works\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kubernetes-pod-priority--preemption-how-it-all-works\" aria-label=\"Ô∏è kubernetes pod priority  preemption how it all works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è Kubernetes Pod Priority &#x26; Preemption: How It All Works</h2>\n<p>When it comes to orchestrating the intricate ballet of Kubernetes pod allocation, the dynamic duo of Pod Priority and Preemption takes center stage. Let's delve into the inner workings of this process to uncover the magic behind efficient resource utilization.</p>\n<ul>\n<li><strong>Assigning Priority via PriorityClassName</strong>: Picture this‚Ää-‚Ääyou've deployed a pod furnished with a PriorityClassName. As the pod takes its place in the Kubernetes environment, the priority admission controller steps in. This controller deftly extracts the priority value associated with the PriorityClassName, setting the stage for what's to come.</li>\n<li><strong>Scheduling Order Based on Priority</strong>: In the bustling queue of pending pods, the scheduler deftly orchestrates their sequence based on their assigned priorities. Here, the golden rule prevails: high-priority pods claim their rightful spot ahead of their lower-priority counterparts.</li>\n<li><strong>Preemption Logic Takes the Stage</strong>: But what if the spotlight shines on a high-priority pod without a suitable stage? In other words, if no nodes flaunt the resources required to host this eager pod, the preemption logic enters the scene. Like a seasoned theater director, the scheduler orchestrates the graceful eviction (preemption) of a low-priority pod from its node.</li>\n<li><strong>A Graceful Ballet of Eviction</strong>: As the curtain falls on the evicted pod's performance, it bows out with a gracious default termination time of 30 seconds. Yet, there's room for customization‚Ää-‚Ääif pods come prepared with a terminationGracePeriodSeconds specified for preStop container Lifecycle Hooks, this interval supersedes the default 30 seconds.</li>\n<li><strong>Scheduling Continues with Flexibility</strong>: But what if the stars fail to align even after preemption? Fear not, for Kubernetes is adept at adaptation. If scheduling constraints persist, the scheduler graciously adjusts its strategy, making room for the ensemble of lower-priority pods to claim their spotlight.</li>\n</ul>\n<h2 id=\"Ô∏è-throttling-and-quality-of-service-qos-in-kubernetes-a-deep-dive\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-throttling-and-quality-of-service-qos-in-kubernetes-a-deep-dive\" aria-label=\"Ô∏è throttling and quality of service qos in kubernetes a deep dive permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚õ≥Ô∏è Throttling and Quality of Service (QoS) in Kubernetes: A Deep Dive</h2>\n<p>At the core of efficient resource management in Kubernetes lies the concept of throttling‚Ää-‚Ääa strategic technique that curbs the processing speed of specific resources, such as network bandwidth or CPU usage. The essence of throttling is to create a balanced environment where high-priority tasks can progress while allowing room for other tasks to function, albeit at a reduced pace. In the realm of containers, resources can be broadly classified into compressible (throttle-able) and incompressible (non-throttle-able) categories, depending on whether they can be regulated.</p>\n<h3 id=\"understanding-throttlings-impact-on-pod-deployment\" style=\"position:relative;\"><a href=\"#understanding-throttlings-impact-on-pod-deployment\" aria-label=\"understanding throttlings impact on pod deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Understanding Throttling's Impact on Pod Deployment</h3>\n<p>In the context of Kubernetes pods, throttling plays a significant role in shaping resource allocation and task execution. Imagine a scenario where a pod is fervently consuming a substantial chunk of a node's memory‚Ää-‚Ääthis situation could impede the scheduling of new pods, potentially leading to deployment issues. Unlike CPUs, which can be slowed down to manage resource contention, memory lacks a similar mechanism. Consequently, an overzealous pod hogging memory can thwart the deployment of new pods, creating a bottleneck.</p>\n<h3 id=\"tackling-throttling-challenges\" style=\"position:relative;\"><a href=\"#tackling-throttling-challenges\" aria-label=\"tackling throttling challenges permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Tackling Throttling Challenges</h3>\n<p>To circumvent these challenges and ensure seamless pod deployment, Kubernetes offers several strategies:</p>\n<ul>\n<li><strong>Resource Management with LimitRange and ResourceQuota</strong>: Kubernetes provides tools like LimitRange and ResourceQuota to maintain control over resource allocation. These mechanisms enable administrators to rein in pods that exceed their allocated limits, preventing resource overutilization.</li>\n<li><strong>Precise Resource Requests and Limits</strong>: Crafting accurate resource requests and limits for containers can mitigate potential resource contention issues. By defining these parameters judiciously, you optimize resource utilization and foster smoother pod deployment.</li>\n<li><strong>Node Upgrades for Enhanced Capability</strong>: Upgrading the capabilities of your nodes can alleviate resource constraints and enhance the overall performance of your cluster. This proactive measure fortifies your infrastructure to accommodate varying resource demands.</li>\n</ul>\n<h3 id=\"diving-into-quality-of-service-qos-and-its-implications\" style=\"position:relative;\"><a href=\"#diving-into-quality-of-service-qos-and-its-implications\" aria-label=\"diving into quality of service qos and its implications permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Diving into Quality of Service (QoS) and Its Implications</h3>\n<p>As you delve deeper into the Kubernetes realm, Quality of Service (QoS) emerges as a pivotal concept closely intertwined with resource allocation. When you stipulate resource requests and limits for your containers, Kubernetes assigns a QoS tier based on your configuration. This tiered approach reflects the priority and potential behavior of your pods:</p>\n<ul>\n<li><strong>Guaranteed</strong>: Reserved for pods with identical resource requests and limits, the guaranteed QoS signifies high-priority tasks that are assured of the resources they need to operate optimally.</li>\n<li><strong>Burstable</strong>: Characterized by differing requests and limits, burstable pods enjoy minimal resource guarantees but have the potential to use additional resources if available. However, they might face termination if the node confronts resource scarcity.</li>\n<li><strong>Best-Effort</strong>: Pods falling under this QoS tier lack explicit resource requests and limits. As low-priority entities, best-effort pods might be terminated when incompressible resources become scarce.</li>\n</ul>\n<h3 id=\"navigating-qos-for-successful-pod-deployment\" style=\"position:relative;\"><a href=\"#navigating-qos-for-successful-pod-deployment\" aria-label=\"navigating qos for successful pod deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Navigating QoS for Successful Pod Deployment</h3>\n<p>To troubleshoot pod deployment challenges arising from QoS considerations, it's prudent to:</p>\n<ul>\n<li>Assess the priority of your pod's resource requests and limits.</li>\n<li>Evaluate if other pods' configurations can be adjusted to optimize resource allocation.</li>\n<li>Draw insights from the earlier-discussed throttling strategies to fine-tune your pod deployment approach.</li>\n</ul>\n<h2 id=\"Ô∏è-qos-vs-pods-priority-independent-forces-in-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-qos-vs-pods-priority-independent-forces-in-kubernetes\" aria-label=\"Ô∏è qos vs pods priority independent forces in kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ò∏Ô∏è QoS vs. Pods Priority: Independent Forces in Kubernetes</h2>\n<p>While QoS and Pods Priority might appear related, they operate separately in Kubernetes. QoS primarily helps Kubelet maintain node health by considering resource availability. In contrast, Pods Priority guides scheduler evictions, focusing solely on pods' priority classes. The scheduler evicts lower-priority pods to make room for higher-priority ones, ensuring efficient resource allocation.</p>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h2>\n<p>Navigating Kubernetes' intricate resource management involves understanding the nuances of Pod Priority, PriorityClass, and Preemption QoS. These mechanisms harmonize to optimize resource allocation, ensuring critical tasks take precedence while maintaining fairness. By comprehending the dynamic interplay between these elements, you're equipped to orchestrate a symphony of efficient container deployment, prioritization, and resource utilization within your Kubernetes ecosystem.</p>\n<h2 id=\"-references\" style=\"position:relative;\"><a href=\"#-references\" aria-label=\" references permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö References</h2>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Pod Priority and Preemption</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass\" target=\"_blank\" rel=\"noopener noreferrer\">Pod Priority Class</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#non-preempting-priority-class\" target=\"_blank\" rel=\"noopener noreferrer\">Pod Preemption Policy</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Quality of Service (QoS)</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/policy/limit-range/\" target=\"_blank\" rel=\"noopener noreferrer\">Resource Management with LimitRange and ResourceQuota</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\" target=\"_blank\" rel=\"noopener noreferrer\">Throttling in Kubernetes</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/\" target=\"_blank\" rel=\"noopener noreferrer\">Pod Priority and Preemption</a></li>\n</ul>\n<br>\n<p><strong>Thank you for Reading, see you in the next post. ‚úç</strong></p>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/chaos-engineering-kubernetes/","title":"Chaos Engineering on Kubernetes: A Beginner's Guide: Revel In Chaos","date":"2024-10-28 17:06:00"},"excerpt":"Easy Guide to Chaos Testing on Kubernetes üó∫ Overview üëÄ Making sure our systems work well is super important in today's fast world. This‚Ä¶","html":"<blockquote>\n<p><strong>Easy Guide to Chaos Testing on Kubernetes üó∫</strong></p>\n</blockquote>\n<h2 id=\"overview-\" style=\"position:relative;\"><a href=\"#overview-\" aria-label=\"overview  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Overview üëÄ</h2>\n<p>Making sure our systems work well is super important in today's fast world. This blog post is all about chaos testing on Kubernetes, which is a fancy way of checking if everything stays strong and doesn't break. We'll talk about why it's useful, how to do it, and what tools you can use.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Chaos_engineering\" target=\"_blank\" rel=\"noopener noreferrer\">Chaos testing</a> is like playing pretend chaos with your computer on purpose. It helps find weak spots, makes things stronger, and gets ready for problems. For Kubernetes, it means pretending things go wrong when making apps to find and fix problems early.</p>\n<p>In this article, we explain how you can use the practice of Chaos Engineering to proactively test the reliability of your Kubernetes clusters and ensure your workloads can run reliably, even during a failure.</p>\n<p>By using chaos testing, you make your Kubernetes setup stronger, so your apps keep working even if things don't go perfectly.</p>\n<h2 id=\"the-need-for-chaos-engineering-Ô∏è\" style=\"position:relative;\"><a href=\"#the-need-for-chaos-engineering-%EF%B8%8F\" aria-label=\"the need for chaos engineering Ô∏è permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>The need for Chaos Engineering üõ†Ô∏è</h2>\n<p>Testing these complex, distributed systems is challenging. Traditional testing tools like JMeter struggle to effectively test these productionized applications, highlighting the need for a different mechanism: chaos engineering.</p>\n<p>Chaos engineering isn't a new concept. It has been around for over 13 years, with Netflix's <a href=\"https://netflix.github.io/chaosmonkey/\" target=\"_blank\" rel=\"noopener noreferrer\">Chaos Monkey</a> being one of the earliest implementations. As systems have matured, so have the tools, leading to the emergence of cloud-native chaos engineering.</p>\n<p>Chaos engineering is the discipline of experimenting on a system to build confidence in its capability to withstand turbulent conditions in production. It involves injecting failures into the system to understand its behavior under different conditions and catch potential issues before they occur in production.</p>\n<h2 id=\"how-does-chaos-engineering-work-\" style=\"position:relative;\"><a href=\"#how-does-chaos-engineering-work-\" aria-label=\"how does chaos engineering work  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How does Chaos Engineering work? üîç</h2>\n<p>Chaos engineering operates on principles that start with defining a steady state representing how our application should behave. We then hypothesize that the steady state will continue, despite introducing real-world variables such as latency or node failures. By injecting these failures, we can observe how our application behaves and see if our hypothesis is disproven.</p>\n<p>Doing chaos engineering in production might seem counterintuitive. After all, turning off things isn't a good thing in production, right? This is where the concept of minimizing the blast radius comes into play. We carefully select which nodes or pods to run chaos experiments on to avoid negatively impacting customers.</p>\n<p>Communication is key when conducting chaos experiments. Everyone on the team should be aware of the what, why, and when of the chaos experiments. And as we keep adding new features to our applications, we need to redo these experiments to ensure the application's resilience.</p>\n<h2 id=\"define-your-kubernetes-chaos-engineering-tests-\" style=\"position:relative;\"><a href=\"#define-your-kubernetes-chaos-engineering-tests-\" aria-label=\"define your kubernetes chaos engineering tests  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Define your Kubernetes Chaos Engineering tests üß™</h2>\n<p>Deciding which tests to run can be difficult for engineers new to Chaos Engineering. Chaos Engineering tools come with many tests, each with several customizable parameters. How do you determine which test to run and where to run it?</p>\n<p>The best way to start is by separating potential issues into four categories:</p>\n<ul>\n<li><strong>Known Knowns:</strong> Things you are aware of and understand.</li>\n<li><strong>Known Unknowns:</strong> Things you are aware of but don't fully understand.</li>\n<li><strong>Unknown Knowns:</strong> Things you understand but are not aware of.</li>\n<li><strong>Unknown Unknowns:</strong> Things you are neither aware of nor fully understand.</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 66.47058823529413%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABkUlEQVR42pVSy07CQBTFiG4MvkiMURJZGeL/8AGs+Qi27HFPAjt+x0A1YsW2xF1bal/Tmc54bqWmMRRkkpO59865J/cxlcqfo5Q6CIKgEUVRMwzDu02gN+IQt1J28kfHcc6FEJ/wGeADAfwMZK9jjDjELeZuFDQM4xLkL5GmKo6Z5FwoKSUAnzGCBFcRh7ilgr1er4qrMR6PqRXHtJZqOntOFx+mTNNUolWp6ws5015Sw7BUFMUr8C7+VSEEvCRJ1GrlScwrqxAxFYZRFksSTr7nuu5uQSKhHU/9nKw9EiMUY8TZW5BazasjrO39BWF7lOf7vuRCqLzKEO0HmCUtaFvLmdNqteq4mpPJ5JYniW2YlnqaahyLEByrZoyJ17kusBQ+f9PxA2J361La7fYh3ZZl1aHgUUUxSyRj/LdlhkVlX0mku79NHrRt+wzJGvwl8A4YSDYJZK9jS/gacUsFiyMA4eRxOLzudrv3o9HoCkM7RazW7/cbnU7nYTAY3BAnH9d/D42AkqqF2DFQA47Ksr4Bo6osDGcNYcwAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"table\" title=\"\" src=\"/static/78c8c8d54cfab24113bedc1a26455bd1/c5bb3/table.png\" srcset=\"/static/78c8c8d54cfab24113bedc1a26455bd1/04472/table.png 170w,\n/static/78c8c8d54cfab24113bedc1a26455bd1/9f933/table.png 340w,\n/static/78c8c8d54cfab24113bedc1a26455bd1/c5bb3/table.png 680w,\n/static/78c8c8d54cfab24113bedc1a26455bd1/b12f7/table.png 1020w,\n/static/78c8c8d54cfab24113bedc1a26455bd1/c1b63/table.png 1200w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>While learning, focus on the \"Known Unknowns\" quadrant. These are characteristics of Kubernetes that you're aware of but don't fully understand. For example, you might know that Kubernetes uses several criteria to determine which node to schedule a Pod onto, but without knowing exactly what those criteria are.</p>\n<h2 id=\"-tools-for-cloud-native-chaos-engineering\" style=\"position:relative;\"><a href=\"#-tools-for-cloud-native-chaos-engineering\" aria-label=\" tools for cloud native chaos engineering permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèó Tools For Cloud-Native Chaos Engineering</h2>\n<p>There are some great tools out there to help you invoke chaos engineering without altering your production state. Below, we'll look at three open-source chaos engineering projects that you can use to quickly run experiments on your cloud-native architecture.</p>\n<h3 id=\"chaos-mesh\" style=\"position:relative;\"><a href=\"#chaos-mesh\" aria-label=\"chaos mesh permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Chaos Mesh</h3>\n<p>A chaos engineering platform for Kubernetes.</p>\n<p><a href=\"https://chaos-mesh.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Website</a> | <a href=\"https://github.com/chaos-mesh/chaos-mesh\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>Want to test the limits of your Kubernetes deployment? Look no further than Chaos Mesh to perform chaos engineering on your production Kubernetes clusters. Chaos Mesh is easily deployable as a CustomResourceDefinition (CRD), so you can get started quickly.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> <span class=\"token parameter variable\">-sSL</span> https://mirrors.chaos-mesh.org/v2.6.2/install.sh <span class=\"token operator\">|</span> <span class=\"token function\">bash</span></code></pre></div>\n<p>Using Chaos Mesh, operators could perform fault injection on the network, disk, file system, operating system, and other areas. Experiments can either be created in a user-friendly GUI or initiated using a YAML file.</p>\n<p>For example, you could use Chaos Mesh to simulate a stress test inside containers. This configuration below defines a sample StressChaos experiment to continually read and write, draining up to 256MB of memory. Fields could be easily changed to adjust the duration, pod, size, and other factors.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> chaos<span class=\"token punctuation\">-</span>mesh.org/v1alpha1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> StressChaos\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> memory<span class=\"token punctuation\">-</span>stress<span class=\"token punctuation\">-</span>example\n    <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> chaos<span class=\"token punctuation\">-</span>testing\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">mode</span><span class=\"token punctuation\">:</span> one\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">labelSelectors</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">'app'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'app1'</span>\n    <span class=\"token key atrule\">stressors</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">workers</span><span class=\"token punctuation\">:</span> <span class=\"token number\">4</span>\n            <span class=\"token key atrule\">size</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'256MB'</span></code></pre></div>\n<p>What's cool is that you can use Chaos Mesh to schedule cyclical testing behaviors. For example, this snippet in YAML from the documentation demonstrates how to configure Chaos Mesh to continually perform a NetworkChaos experiment five minutes after every hour. This particular experiment produces a network latency fault with a 12-second duration.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> chaos<span class=\"token punctuation\">-</span>mesh.org/v1alpha1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Schedule\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> schedule<span class=\"token punctuation\">-</span>delay<span class=\"token punctuation\">-</span>example\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">schedule</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'5 * * * *'</span>\n    <span class=\"token key atrule\">historyLimit</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n    <span class=\"token key atrule\">concurrencyPolicy</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'Allow'</span>\n    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'NetworkChaos'</span>\n    <span class=\"token key atrule\">networkChaos</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">action</span><span class=\"token punctuation\">:</span> delay\n        <span class=\"token key atrule\">mode</span><span class=\"token punctuation\">:</span> one\n        <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">namespaces</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> default\n            <span class=\"token key atrule\">labelSelectors</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">'app'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'web-show'</span>\n        <span class=\"token key atrule\">delay</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">latency</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'10ms'</span>\n        <span class=\"token key atrule\">duration</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'12s'</span></code></pre></div>\n<p>Using Chaos Mesh, there's no need to change your deployment logic to perform chaos experiments. You can observe the behavior in real-time and, if it's really going haywire, you can quickly roll back failures. The platform also supports RBAC as well as blacklisting and whitelisting to help protect the experimentation process itself from abuse. At the time of writing, Chaos Mesh is an open source incubating project with the CNCF.</p>\n<h2 id=\"litmus\" style=\"position:relative;\"><a href=\"#litmus\" aria-label=\"litmus permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Litmus</h2>\n<p>Helps SREs and developers practice chaos engineering in a cloud-native way.</p>\n<p><a href=\"https://litmuschaos.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Website</a> | <a href=\"https://github.com/litmuschaos/litmus\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>Litmus is an open-source chaos engineering project aimed at SREs who want to push their cloud-native architecture to the limits. Compared to Chaos Mesh, Litmus is a bit larger in scope, enabling developers to perform tests on many environments, including the Kubernetes platform, Kubernetes apps, cloud platforms, bare metal, legacy applications, and virtual machines.</p>\n<p>Litmus is easy to install using Helm:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token function\">install</span> litmuschaos/litmus</code></pre></div>\n<p>Once installed, engineers can choose a chaos scenario from a number of pre-defined Litmus Workflows. <a href=\"https://hub.litmuschaos.io/\" target=\"_blank\" rel=\"noopener noreferrer\">ChaosHub</a> is an open marketplace hosting many Litmus experiments to run chaos on various infrastructures. Litmus can structure chained sequences of experiments, so you can chain many experiments to wreak as much havoc as you like.</p>\n<p>For example, <a href=\"https://docs.litmuschaos.io/docs/getting-started/run-your-first-workflow/\" target=\"_blank\" rel=\"noopener noreferrer\">the documentation</a> showcases using the Litmus user interface to install an application, perform a chaos experiment on it, uninstall the application, and revert the chaos.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 42.94117647058824%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZUlEQVR42o2SaW6kMBSE+/7XiZRLRMrfidIdZRICNKuxAW8slbLdtCKNRgrSR8EzrreYkzYOgUGOEammiBoD8z02a/srTg+PH3h6btC2PfL8ivLaoCgrfGbpOS+STrNJm+bbZr7rI/bT8HzJuKGDHB064dAPCTF4dFSpFsYthAhdWOJYuUctJrzXXYyJG2qkoTYWznsIZdAIQzMb9aWoqBp/qHU/41K1yFtFE4HztY1rWaOY1EZaJpXBUAwqtjOoUFmqqukNPktB1fiqZUyQc3PVTqhoXlKv3YTXsonVB7PmMFSTjr0HrTlHbTz8umMlftmxbUmXZYux5bbm/AptHb/dIund4xTMtOZ8RoVe9jTYcFzLsnKWY5zNwLa0XWDcCmNXWJcwB2HtMAwYzlJJSXN9N5yZ6PxWICsE29VszfyXppvTKYdbmGGoJlz7vt8NQ9a/GX+jakDNmR0H8A884ZoJp8niG58AsxErjt57AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Litmus\" title=\"\" src=\"/static/6c162f8b57b44671cb1d4ecda2ff0866/c5bb3/Litmus.png\" srcset=\"/static/6c162f8b57b44671cb1d4ecda2ff0866/04472/Litmus.png 170w,\n/static/6c162f8b57b44671cb1d4ecda2ff0866/9f933/Litmus.png 340w,\n/static/6c162f8b57b44671cb1d4ecda2ff0866/c5bb3/Litmus.png 680w,\n/static/6c162f8b57b44671cb1d4ecda2ff0866/b12f7/Litmus.png 1020w,\n/static/6c162f8b57b44671cb1d4ecda2ff0866/2bef9/Litmus.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Using Litmus, engineers can also create custom workflows and schedule workflows to occur on a regular basis. For an open-source free tool, Litmus is surprisingly comprehensive, offering a feature-rich platform with a SaaS-like console.</p>\n<h2 id=\"chaosblade\" style=\"position:relative;\"><a href=\"#chaosblade\" aria-label=\"chaosblade permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>ChaosBlade</h2>\n<p>A powerful chaos engineering experiment toolkit.</p>\n<p><a href=\"https://chaosblade.io/en/\" target=\"_blank\" rel=\"noopener noreferrer\">Website</a> | <a href=\"https://github.com/chaosblade-io/chaosblade\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>ChaosBlade is another toolkit that can help DevOps engineers and SREs perform chaos on their cloud-native systems. Originally produced at Alibaba, ChaosBlade was open sourced in 2021 and is currently a sandbox project hosted by the CNCF. The package includes two main components: The chaos engineering experimental tool, ChaosBlade, and a chaos engineering platform, ChaosBlade-Box.</p>\n<p>Using ChaosBlade, engineers can perform experiments through a unified interface. The platform brings an assortment of features to help experiment with resource fluctuations pertaining to the CPU, memory, network, disk, process, kernel or files. Like the tools above, ChaosBlade also supports automating chaos engineering regularly.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.88235294117647%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe7ZoA//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAEBAAIDAAAAAAAAAAAAAAABABAxESFR/9oACAEBAAE/IeDwuiGdxrH/2gAMAwEAAgADAAAAEODP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAAICAwAAAAAAAAAAAAAAAQARQXEhkbH/2gAIAQEAAT8QcfQSworRASeM5nUALrM//9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ChaosBlade\" title=\"\" src=\"/static/d9a78d00687bf530d91c7bf4aff4ced3/7bf67/ChaosBlade.jpg\" srcset=\"/static/d9a78d00687bf530d91c7bf4aff4ced3/651be/ChaosBlade.jpg 170w,\n/static/d9a78d00687bf530d91c7bf4aff4ced3/d30a3/ChaosBlade.jpg 340w,\n/static/d9a78d00687bf530d91c7bf4aff4ced3/7bf67/ChaosBlade.jpg 680w,\n/static/d9a78d00687bf530d91c7bf4aff4ced3/990cb/ChaosBlade.jpg 1020w,\n/static/d9a78d00687bf530d91c7bf4aff4ced3/72e01/ChaosBlade.jpg 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"chaoskube\" style=\"position:relative;\"><a href=\"#chaoskube\" aria-label=\"chaoskube permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>chaoskube</h2>\n<p>chaoskube periodically kills random pods in your Kubernetes cluster.</p>\n<p><a href=\"https://github.com/linki/chaoskube\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>chaoskube periodically kills random pods in your Kubernetes cluster.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADnElEQVR42o2U3U/bZRTH+Qf0xnjlLky88EZjjBfGK4XEt2RRGYjKq8xZx0BWwMlgjEEYGeoctAxhpbzMyEBmKR1ZoQUCDDdooRR5KdsoiIyVUiisG9ABbT/++gITB2wnOcn5Pc85n995vs+TE8QO5vEIHojdbnCuuVgV3OX2bMvZyYJ2A3qL24w28mpvklI+hFg2RPZvJpQ3LL4f7GaPAZedLjr657moniY0Q0f4CT2ROf0+j8jq4+P0XoovT3BtcIEVp+vJwIdrbpq7rXyZaaDwopmR8ftkScbIko754qJfJ4hJN9DUaWV9w703cFMXy5yTb74bxPz3su+7pWMOjeBeM08tcyjVuLX3fy13BA4bF4iO03O12errWCI1Izlv9sVq9SzRsXr6dQt7A90u/45BPcqPIhVJh8eICjOQEG+kuHKC82WTJB408kWogYNxtygRNzKoGd1Wuw3oCuihLvmT95/NISlCxTHREJHvGck+Kmj4rYkDwcMcFw0yoqxCV5DA1dLrfuB/tHwM2F6l452gHD566Swpn1VzKrGL6BAdov1DXDnXyEpbGvQdoC8vBm1FX6DDPYBaeS+hQYXEvVLNsYga3tqXSu7RBpbaC6E7DLc2Ao8mnva0I2gq9E8Gtlb08nlQGeLXlORGqQjZl0/moTLsik9AE4u9RoSlLJXLkeloK58C2HKhB/FzCiThPeSHt3Aq6gql2U3Y6w8zK0tkJO84hu9PI383H23VUwC1lTeIFTqUBg9QEtdDwuu1JAfLMf2QjiHjJJ2in1CF/kLGi/m0Vut3vxRP4EE55pepy2sh8YVSMl5WUbC/i8wP6uk4cgZNpJTiNyWceENC3elW7ttXttXu+LA9gYkyZZqhKLaWmGekfPV8BfK3pWS/epbir+uZHLbsOnH8QI93TPlnlhfoCmji8bgxaEc5GSIj78NyBtpv+/Pw53s78+Z7R9zmvNsCbh7XOmnfOsZmB2vOdVYfPMQ2tcjGumtrbdpk3bnDe7YHXFf8haa8B3lKI6Pdk7QKD7y/2cS1ugF6VcPomkYoFyt9saF5jK4aA6WJfzDYdstXN3Nz7hFwqHMcmfB4lT93UJHWSHV6EwWfViETNwiuFHT7nQvJCs7F1HApt4Wi+EvIkhsoS1II6w2cCatkYuDOI+CGMIHv3rYxf2eJRasD2z+L2C0O7He9fo9FIZ6bsrM462BhZgnL+Dzz00u+mlWHE5sQb17mv7G8XpWEPexBAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"chaoskube\" title=\"\" src=\"/static/23496bc41cdc906ad3dc91ab20d0bbba/0a47e/chaoskube.png\" srcset=\"/static/23496bc41cdc906ad3dc91ab20d0bbba/04472/chaoskube.png 170w,\n/static/23496bc41cdc906ad3dc91ab20d0bbba/9f933/chaoskube.png 340w,\n/static/23496bc41cdc906ad3dc91ab20d0bbba/0a47e/chaoskube.png 600w\" sizes=\"(max-width: 600px) 100vw, 600px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>By default chaoskube will be friendly and not kill anything. When you validated your target cluster you may disable dry-run mode by passing the flag <code class=\"language-text\">--no-dry-run</code>. You can also specify a more aggressive interval and other supported flags for your deployment.\nIf you're running in a Kubernetes cluster and want to target the same cluster then this is all you need to do.\nIf you want to target a different cluster or want to run it locally specify your cluster via the <code class=\"language-text\">--master</code> flag or provide a valid kubeconfig via the <code class=\"language-text\">--kubeconfig</code> flag. By default, it uses your standard kubeconfig path in your home. That means, whatever is the current context in there will be targeted.\nIf you want to increase or decrease the amount of chaos change the interval between killings with the --interval flag. Alternatively, you can increase the number of replicas of your chaoskube deployment.</p>\n<p>Remember that chaoskube by default kills any pod in all your namespaces, including system pods and itself.\nchaoskube provides a simple HTTP endpoint that can be used to check that it is running. This can be used for Kubernetes liveness and readiness probes. By default, this listens on port 8080. To disable, pass --metrics-address=\"\" to chaoskube.</p>\n<h2 id=\"pumba\" style=\"position:relative;\"><a href=\"#pumba\" aria-label=\"pumba permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Pumba</h2>\n<p>Chaos testing, network emulation, and stress testing tool for containers</p>\n<p><a href=\"https://github.com/alexei-led/pumba\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>Pumba is a chaos testing command line tool for Docker containers. Pumba disturbs your containers by crashing containerized application, emulating network failures and stress-testing container resources (cpu, memory, fs, io, and others).</p>\n<h3 id=\"demo\" style=\"position:relative;\"><a href=\"#demo\" aria-label=\"demo permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Demo</h3>\n<p><a href=\"https://asciinema.org/a/82428\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://asciinema.org/a/82428.svg\" alt=\"asciicast\" loading=\"lazy\">\n          </a></p>\n<h2 id=\"kubeinvaders-\" style=\"position:relative;\"><a href=\"#kubeinvaders-\" aria-label=\"kubeinvaders  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>kubeinvaders üéÆ</h2>\n<p><strong>Gamified Chaos Engineering Tool for K8s</strong></p>\n<p><a href=\"https://github.com/lucky-sideburn/kubeinvaders\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a></p>\n<p>Backed by the teams at <a href=\"https://platformengineering.it/\" target=\"_blank\" rel=\"noopener noreferrer\">platformengineering.it</a> and <a href=\"https://devopstribe.it/\" target=\"_blank\" rel=\"noopener noreferrer\">devopstribe.it</a>, which provide enterprise-grade features and certified resilience services for your Kubernetes infrastructure.</p>\n<p><img src=\"/4aff514e317c6594ab2e5efea0567edb/kubeinvaders.gif\" alt=\"kubeinvaders\" loading=\"lazy\">\n          </p>\n<h2 id=\"summary-\" style=\"position:relative;\"><a href=\"#summary-\" aria-label=\"summary  permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Summary üìù</h2>\n<p>Unexpected, turbulent conditions are bound to arise from time to time. With that in mind, it's best to be prepared. Chaos engineering brings many benefits to modern cloud-native operators, helping expose bugs or bottlenecks in a system.</p>\n<p>By testing your architecture early on, your team can also practice how you respond to unforeseen problems.</p>\n<p><a href=\"https://principlesofchaos.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn more about Chaos Engineering</a></p>\n<p><strong>Thank You üñ§</strong></p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}