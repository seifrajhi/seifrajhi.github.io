{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/improve-k8s-image-caching/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>A Guide to Tools and Strategies of image cachingüö¶</strong></p>\n</blockquote>\n<h2 id=\"-intro\" style=\"position:relative;\"><a href=\"#-intro\" aria-label=\" intro permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöõ Intro</h2>\n<p>When deploying a containerized application to a Kubernetes cluster, delays can occur due to the time it takes to pull necessary container images from the registry.</p>\n<p>This delay can be especially problematic in scenarios where the application needs to scale out horizontally or process high-speed real-time data. Fortunately, there are several tools and strategies available to improve container image availability and caching in Kubernetes.</p>\n<p>In this blog post, we will explore a comprehensive guide to these tools and strategies, including <a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a>, <a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, Kubernetes built-in image caching features, local caches, and monitoring and cleaning up unused images.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 535px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 214.11764705882354%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAArCAYAAAB4pah1AAAACXBIWXMAAAsTAAALEwEAmpwYAAADNUlEQVR42p3X124sIRAE0Pn/z/Kjnxwk55xzzomrM1KNZies9xoJwQ5QVHd1A1uVTvn5+anbh4eHsrm5WQ4PD8vV1dXE2LRSjQFeXFyUpaWlsrW1VQ4ODv4O2C7Y7ezslK+vrzJrGQQMk+Pj47K+vl73v7+//wYYsLe3t3J6elpOTk7K4+Pj30zOgtfX19pvHx8fNbP9/f1apFlAqy7Yy8tL2d7erhmmfH5+1r6cBbTqMqNqwNoLge7u7v4KWnXBtN0F6QPd29srT09Po6BVwLpmjjEQQtTnmqF5VXYNMy1VLVAJo9pMG9Czs7MJAg1ge7eknNhbXV2tN1peXi4LCwvl/Py8CZ+YL6y68Vnxh8ld6ibb7PLyss6Y9/f3CRNvb28bgXo+vLu7K0dHR82CIf+1vzssAE5V+ebmpgFtA7SrgvH19fV0lTNgYhu0qzgwdWocDp2DfJbfafn0N7BBQKZTmJ/aAlD+/v5+dsCcMLLFAuoDTxWDUm+m06bNxEmd8vz8XEdAO5R+YznBELuhBeljnetgJpPbia8IZqmocodMWVxcrDPLb+NaWTNoMhWZmXMx6cYVTFdtqhUNNk+mBbTqhkyuzPjTAt8B62ttakNzsDQvB8UEoMmJNcq6T/zWp7D4tNimmLq3JYM5yfVRQOIwxVEVn8VEv7GMH83L0dYDTNgAYip2WGBloeMsrHwzbpPk+4TKdnNwjhWO/+0V0Us9fmIuttTkeCz0Bfbc3NyE8uZGca7oAcpZLLDlpyhqQbuvTYyaqxKxB7iystL4BzAWFDaZf7XxrXFMc4341gMUDs5EdwXnMwk4EJUwFusbT0tx4D1Ag+LLIuyishYYVlo1lsSXLKja923Sb2Njo/ZJMkQFOD8/X7NPPCZ74l+xWOWYspMJ3jD8aFIU1udH12l8NfQkaeIQGHRx5iO2qXzZ7hvPnZx3Y/siq3IQ2BkLJhFGgDsfLdT6JkbX1tYaloNPkTDMs4P5Yiv5mthr57P+2EO+yl2CEQZUzMmCjQtLH0NjGFM45+Yg4Ng9zHzmzvoc7uWymscPszB0ukTRrgD/9WjPHx911vf1VEAMmcx/Y/4aAvwHjN8Qs+BOyI0AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"lifecycle\" title=\"\" src=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png\" srcset=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/04472/lifecycle.png 170w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/9f933/lifecycle.png 340w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png 535w\" sizes=\"(max-width: 535px) 100vw, 535px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"Ô∏è-preamble\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-preamble\" aria-label=\"Ô∏è preamble permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ö°Ô∏è Preamble</h2>\n<p>When you deploy a workload to Kubernetes, the containers in a certain Pod are naturally based on OCI container Images. These Images can be pulled from private/public repositories of many kinds. Kubernetes caches the images locally on every node that has pulled them, so that other Pods might use the same image. The settings for how and when Kubernetes pulls images can be found in the <a href=\"https://kubernetes.io/docs/concepts/containers/images/\" target=\"_blank\" rel=\"noopener noreferrer\">documentation</a>.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 77.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACOElEQVR42q2U2W7TUBCG86QIiUfgnifgikVcgtTCHWIRBcRFUaVEIoLSliiFhLRZmjhOvKSpt+Oz+GfmOI7aCxIJYWlizznj/3yzxLWiAISkn/901Q5aGR6+XaLvSnILaFOgKEozxqyfN61VZgWffopwb2eBz8cJuRq51JBSQikFScZ3JelZrnxd7Wu7plc+G4vWvKVGvZ0gEwq+58Gdzm0gC/meD2fiIs+VFQiDAJPxlHxpY5aXS4wvHHo3t2sMWWNMpcoTzvwrjL0JEQkizXHuxxjMRtAyg6YDhmGMs9kQKqf9PIezzPDbHUOKGIIEWbHGqjmdZrTC/bqDuy+bSBNK3yg8ac5xZ7eBNI3pWIOdIw+3XzQw865seV6fLnDreRN9ygqFgWFBJmQ6Ftz72sOz/VOklIIh6v2TAR5/+IEkE1aw3h7h0fsThFFqD/zem+LBuyO4i9hmUFSCVQPmzgizi/N1kf3ZBO6wZ9PjZoSeC2fQhcgyW9PLwMOk30GWJramxXVC7pYQEmmW22D2udBRkpYdJcuINEmF3av2mV6qstN2bK4PJQcxLV+rsbJ0SUpN0QZRHK1jeZ+JmOzGYG8TDCON0VxgHGg4QXbj5VJQ/l2Q/wVMVKWkqTHfeikOfmnsHSs0OjlCGhWOqwY730RYUVpSMqMlvnRTvDo0+Ngq8OZQIE5IUJeCatXZjYLrdFb3KKPu+wsEQYhgKbZ/HLYFMEWn20Or/RNRFK1r98+CVW2rr8w2wj8fHdY5jW02ZAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cr\" title=\"\" src=\"/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png\" srcset=\"/static/13fb62f279e7732af7156210ef5c9ba1/04472/cr.png 170w,\n/static/13fb62f279e7732af7156210ef5c9ba1/9f933/cr.png 340w,\n/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png 680w,\n/static/13fb62f279e7732af7156210ef5c9ba1/3c051/cr.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>In most use cases, it's not enough. Most Cloud Kubernetes clusters today require auto-scaling and are dynamically allocated Nodes based on the customer's usage. What if multiple nodes have to pull the same image multiple times? And if this image is heavy, that can take minutes. In the applicative autoscaling world, that is a relatively long time.</p>\n<h2 id=\"-the-solution\" style=\"position:relative;\"><a href=\"#-the-solution\" aria-label=\" the solution permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí• The Solution</h2>\n<p>The expected solution needs to have a cache layer on top of Kubernetes, so that Kubernetes has a centralized image cache and all nodes \"pull\" from it. But because the cache needs to be very fast, the caching solutions need to sit inside Kubernetes, and all nodes should have the fastest latency towards it.</p>\n<h2 id=\"-existing-solutions\" style=\"position:relative;\"><a href=\"#-existing-solutions\" aria-label=\" existing solutions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÆ Existing Solutions</h2>\n<p>The widely-used approach to tackle the problem of delays in pulling container images from the registry is to have a registry mirror running inside the cluster. Two widely used solutions are the in-cluster self-hosted registry and pull-through cache.</p>\n<ul>\n<li><strong>In-cluster self-hosted registry:</strong> A local registry is run within the Kubernetes cluster and is configured as a mirror registry in the container runtime. Any image pull request is directed to the in-cluster registry.</li>\n<li><strong>Pull-through cache:</strong> A cache of container images is built and managed directly on the worker nodes.</li>\n</ul>\n<p>Other existing solutions include using a reliable caching solution like <a href=\"https://github.com/kuikproject/kuik\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, enabling image caching in Kubernetes, using a local cache, optimizing container image builds, and monitoring and cleaning up unused images.</p>\n<h2 id=\"-harbor\" style=\"position:relative;\"><a href=\"#-harbor\" aria-label=\" harbor permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö¢ Harbor</h2>\n<p><a href=\"https://goharbor.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Harbor</a> is a CNCF Graduated project that functions as a container registry, but most importantly as a Pull Through Proxy Cache.</p>\n<p>A pull-through proxy cache is a caching mechanism designed to optimize the distribution and retrieval of container images within a container registry environment. It acts as an intermediary between clients (such as container runtimes or build systems) and the upstream container registry.</p>\n<p>When a client requests a container image, the pull-through proxy cache checks if it already has a local copy of the requested image. If the image is present, the proxy cache serves it directly to the client, eliminating the need to download it from the upstream registry. This reduces network latency and conserves bandwidth.</p>\n<p>If the requested image is not present in the local cache, the proxy cache acts as a regular proxy and forwards the request to the upstream registry. The proxy cache then retrieves the image from the registry and serves it to the client. Additionally, the proxy cache stores a copy of the image in its local cache for future requests.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 125.88235294117646%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAADlklEQVR42q1Vz4scRRTef0oET55V8ODJsyD+AYKRoAfJRUQEIadEggkSSCJkiZcNJioTze7i7CS7mx13fmzv9OzMdO/2TE//7uqq6s9X1d2707O7QsCCoutVdX316nvfe7UCagnLEcQS/0db4QK48yTE96se3IAM5JAyR56/fteAjOe4/sjHVz+5GB6nkCJDyhg458iyYswW7CRl+ru4rr9pqkFXFOrMFzDtlBYYjicWXNejMUeapLDGE3heiIwTWJJgMhojCCKyOaIogk3rYRDXAdVAnRgzjsedIwxtA7nI4cUMj/ePYDkDbU/DFE+6Q9jOEBAStp/gac+EMxvRLfgZoOJMEGCQZHj/1jau/97WXFpegndubuPWX/uaH8OJ8N4PL3F3s6ftthXg3ZsvsNoy6HdBZyx5GEYxnrf20B1YxQFBiD+3CjuXEq7n4VnzFYzRCdkCM3eOBtmmNdV0nbuylByHhgl3NtMeqGBYoyE8dwYVRMXT5MhE4M31ukfcWSMTSeQToKhfmUiB7Urcf8awtpVSNLOavk7lUViwZoykFuBeg5RBk0IsAAoNKNHqZ/ji9hxf33OJ6JCiGqF/cADHcTAYDHBAY8M4JDyGjT0XH303xac3ZggTqSmQixxK4kjpaW3DxovOFJXnak6tKQ8ULapXbW1d/VtcX8msdmXVgzCE0WtjYHTBhahduQJVXR2idMhSn2ym17mocSg16b4/x9bODrrmMbkt9XwFFIZRDbToxT4NyJcAVev1/8EvrUdodoaUJYk+VQiST5zCPDlBxiK9sbp65ek5DwsOc4psjK3dDdjWuLhmeXpnyvDxzy2s7/fL+fw08gr0nIeFDkVJblbKCKcVZDQNce3hS7QOZ5cDiksAGf2g+Cl4LTayJIZtdpHGQQl0xu2Zh/xiwCprVGdl6UooQ+Z+qA8rbJrP+KlULvGQ1wqlroGUn6Ikf7EmqhQVPFugJa/rUGdLKQVJig9TKk1OgKlP3mRSzymJCFFU9AfPJ/j8zi5MJ9U2L/fqir38JkT0vrTHEo19id/2BDqTQlKiDNRm38ebVzfx1pdNfHKjXWb2wpuyDJiSrJ6+4njY5PixwQlA1AB/3XXxxpV1vP3ZH/jwm79rYBcCqmbPM6p9IexpBDesgnW2/u1qDx9ca6DZdWoyOgdYBWTuutimFOz1evodWW5jsw+js4Mo8Kud/+1hVWWqGrfcuK48F6/9C56mf4tpydg9AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"harbor\" title=\"\" src=\"/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png\" srcset=\"/static/f87b305ff11726efc93f028b32db411e/04472/harbor.png 170w,\n/static/f87b305ff11726efc93f028b32db411e/9f933/harbor.png 340w,\n/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png 680w,\n/static/f87b305ff11726efc93f028b32db411e/3c051/harbor.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<hr>\n<h2 id=\"-kube-fledged\" style=\"position:relative;\"><a href=\"#-kube-fledged\" aria-label=\" kube fledged permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê¶ kube-fledged</h2>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.176470588235293%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABBklEQVR42o2QQUvDQBCF+7s9iEXxINVz7x6tFkEPepBeom2h2ELBmNrYUDTZJNoQkyZp0v1cg9qDVRx4MDvz9uMxFdZVUcBwCJYFhgGaBlnGf6qyHriEmzYymIEQyNGoHEu5snz08i/gj+W9DmGAnE7Jk3Tt518TpgtJ5yHGD3OeXhe09BjPtPGGJrH7wuF1QGcUESYFt5M53XGMYadY/gInyGndvZEVcgXM1aN67LB35lJT2lfaaAgOLme0xwn1C4/NI5vtpsNOUyifYOvEoaq0eyqonbsl4xu4VAfRn9MypWbE9CcJPTNmoNJEaYEpUvrWnN7jvEzY6AbUr3zljRh8zr9O8g7kE8LJCduHhQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"fledged\" title=\"\" src=\"/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png\" srcset=\"/static/8243b2682f856240dd3fae34b0836534/04472/fledged.png 170w,\n/static/8243b2682f856240dd3fae34b0836534/9f933/fledged.png 340w,\n/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png 680w,\n/static/8243b2682f856240dd3fae34b0836534/8ae3e/fledged.png 756w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a> is a Kubernetes add-on or operator for creating and managing a cache of container images directly on the worker nodes of a Kubernetes cluster. It allows a user to define a list of images and onto which worker nodes those images should be cached (i.e. pulled). As a result, application pods start almost instantly, since the images need not be pulled from the registry. kube-fledged provides CRUD APIs to manage the lifecycle of the image cache, and supports several configurable parameters in order to customize the functioning as per one's needs.</p>\n<p>kube-fledged is designed and built as a general-purpose solution for managing an image cache in Kubernetes. Though the primary use case is to enable rapid Pod start-up and scaling, the solution supports a wide variety of use cases as mentioned below.</p>\n<h3 id=\"how-kube-fledged-works\" style=\"position:relative;\"><a href=\"#how-kube-fledged-works\" aria-label=\"how kube fledged works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How kube-fledged works</h3>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADRklEQVR42n1TbUxTVxg+4CyVigSn1g+WuOhm7FBmCH78EUOEuIZp/VGXaU2VZNO44DcDxHgNWqXUIm3px+39aO+9pd9XoJWyVlcRqJ3KhPBbXbaQ7I8//aW99/W2SEJi4ps8Oed9z3Pe85z3PQehRabVYrLjx5hW/Zn+tQ063RpAqLicptvKOO9rBcf8I6eo3wtEDCtGn1rRojkUHJ2OXXNcxxInT7LVCOZjqz3M9fUB/7N1/oFnq9zui/lY069YqV6PybHFiT/yP2vhcHjJPBcVtRxoKVEipUJyl0mQV1ZWLtPr9XKQEjXVNJViCMn0dXXyBalF8zfBvrBYLCVajFi542akb7dxiK/tHorVGPhT+fVe5+PLLPuU5waeB93EpGlhXzXm3bkd467t7GS/RoslH719v6LVSJbVd/iVu8yJf/fZH8Je1zjU9sQCeQrjm4o+Ss9BMvkKIpHZl0illeXj3xkGr1QZ41B1a1CNGhu1K1Uq1XKkRIrm5tYy7RlsucQpKT90bqNMc26r7OfWb8qPtldIsVKtHlvb0olv6ugbVrZLhy/UrcoQq9tmiDq337r3rdRalQwzBKsG7zzUhI2pgzHT2I+kcURjNkfVdkus0e1I1jvwR/vNtvhhc39cbXKM1NvIsT39vsxeO5ep/djQonypCiXA8amlVnx8Wsel4CdvXNB4hsVL5AOBI7Oih3wi0u4M+OisyIf+Fnh+RhiJzkIsMCOOhGYh5J8S77rG9uRVXjKZFIWEM+yMoo+cmKvwcVDJh4SlAS+oCR4c1hScv8tDZ98QXJDGLst9cOIT8Bv5B/xCj4onqITQy/4FDJFVb7jKtn1/Izy5pZNJIQyPleLE5H8Gehw68AdCG54Ck/NP6LGNQrP1Hpy1DcMJGw/ttjj0OtPwFcvCihAjFgdo8RQ9CvaepG6rNT29j3kBqq7IG0nh/4p+Kju32R+B1cEBYUWQgwYiCrRrArrtSTBaE9AtJbe60kBRWeilJqCHeixKEDy+aXAzU01fXuUOVncFNdtuD1SgsDa8xOoayxxhEvCDd+hdg2cwdxpP5LzEkxxFZHKkO5Oj8pB8QoKHyEpr2fcMmRUIIvP2jiNdM/+GofBrPgAU7aW+ygepZQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"working\" title=\"\" src=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png\" srcset=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/04472/working.png 170w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/9f933/working.png 340w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png 680w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/b12f7/working.png 1020w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/525d3/working.png 1090w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Kubernetes allows developers to extend the Kubernetes API via Custom Resources. kube-fledged defines a custom resource of kind \"ImageCache\" and implements a custom controller (named kubefledged-controller). kubefledged-controller does the heavy-lifting for managing image cache. Users can use kubectl commands for creation and deletion of ImageCache resources.</p>\n<h2 id=\"-kubernetes-image-puller\" style=\"position:relative;\"><a href=\"#-kubernetes-image-puller\" aria-label=\" kubernetes image puller permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê≥ kubernetes-image-puller</h2>\n<p>To cache images, <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Image Puller</a> creates a Daemonset on the desired cluster, which in turn creates a pod on each node in the cluster consisting of a list of containers with command <code class=\"language-text\">sleep 720h</code>. This ensures that all nodes in the cluster have those images cached. The sleep binary being used is <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main/sleep\" target=\"_blank\" rel=\"noopener noreferrer\">golang-based</a> (please see <a href=\"https://github.com/che-incubator/kubernetes-image-puller#scratch-images\" target=\"_blank\" rel=\"noopener noreferrer\">Scratch Images</a>). We also periodically check the health of the daemonset and re-create it if necessary.</p>\n<p>The application can be deployed via Helm or by processing and applying OpenShift Templates. Also, there is a community-supported operator available on the <a href=\"https://operatorhub.io/operator/kubernetes-imagepuller-operator\" target=\"_blank\" rel=\"noopener noreferrer\">OperatorHub</a>.</p>\n<p>üí° Kubernetes Image Puller deploys a huge number of containers (one container per image and per node / uses a daemonset for the caching mechanism), to fulfill the caching feature.</p>\n<p>Let's take this example: With 5 nodes &#x26; 10 images in cache, we already have 50 containers within the cluster dedicated for the caching feature.</p>\n<h2 id=\"Ô∏è-tugger\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-tugger\" aria-label=\"Ô∏è tugger permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Tugger</h2>\n<p><a href=\"https://github.com/jainishshah17/tugger\" target=\"_blank\" rel=\"noopener noreferrer\">Tugger</a> uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<p>üí° Tugger uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<h2 id=\"Ô∏è-kube-image-keeper-kuik\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kube-image-keeper-kuik\" aria-label=\"Ô∏è kube image keeper kuik permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è kube-image-keeper (kuik)</h2>\n<p><a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kube-image-keeper</a> (a.k.a. kuik, which is pronounced /kw…™k/, like \"quick\") is a container image caching system for Kubernetes. It saves the container images used by your pods in its own local registry so that these images remain available if the original becomes unavailable.</p>\n<h3 id=\"how-it-works\" style=\"position:relative;\"><a href=\"#how-it-works\" aria-label=\"how it works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How it works</h3>\n<p>When a pod is created, kuik's mutating webhook rewrites its images on the fly, adding a <code class=\"language-text\">localhost:{port}/</code> prefix (the port is 7439 by default, and is configurable).</p>\n<p>On <code class=\"language-text\">localhost:{port}</code>, there is an image proxy that serves images from kuik's caching registry (when the images have been cached) or directly from the original registry (when the images haven't been cached yet).</p>\n<p>One controller watches pods, and when it notices new images, it creates CachedImage custom resources for these images.</p>\n<p>Another controller watches these CachedImage custom resources, and copies images from source registries to kuik's caching registry accordingly.</p>\n<h3 id=\"architecture-and-components\" style=\"position:relative;\"><a href=\"#architecture-and-components\" aria-label=\"architecture and components permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Architecture and components</h3>\n<p>In kuik's namespace, you will find:</p>\n<ul>\n<li>A Deployment to run kuik's controllers.</li>\n<li>A DaemonSet to run kuik's image proxy.</li>\n<li>A StatefulSet to run kuik's image cache (a Deployment is used instead when this component runs in HA mode).</li>\n</ul>\n<p>The image cache will obviously require a bit of disk space to run (see <a href=\"https://github.com/enix/kube-image-keeper#garbage-collection-and-limitations\" target=\"_blank\" rel=\"noopener noreferrer\">Garbage collection and limitations</a>). Otherwise, kuik's components are fairly lightweight in terms of compute resources. This shows CPU and RAM usage with the default setup, featuring two controllers in HA mode:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl <span class=\"token function\">top</span> pods\nNAME                                             CPU<span class=\"token punctuation\">(</span>cores<span class=\"token punctuation\">)</span>   MEMORY<span class=\"token punctuation\">(</span>bytes<span class=\"token punctuation\">)</span>\nkube-image-keeper-0                              1m           86Mi\nkube-image-keeper-controllers-5b5cc9fcc6-bv6cp   1m           16Mi\nkube-image-keeper-controllers-5b5cc9fcc6-tjl7t   3m           24Mi\nkube-image-keeper-proxy-54lzk                    1m           19Mi</code></pre></div>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 54.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5qiQf/xAAVEAEBAAAAAAAAAAAAAAAAAAAQAf/aAAgBAQABBQJr/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERQRAxUXH/2gAIAQEAAT8hcqyHQp7gtFv0/9oADAMBAAIAAwAAABAwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQADAQADAAAAAAAAAAAAAAEAESExUXGB/9oACAEBAAE/ELFYruxRFZUGtd8QC7OHqCmr9T//2Q=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuik\" title=\"\" src=\"/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg\" srcset=\"/static/13e29d44f62c2c264156b503f96d7501/651be/kuik.jpg 170w,\n/static/13e29d44f62c2c264156b503f96d7501/d30a3/kuik.jpg 340w,\n/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg 680w,\n/static/13e29d44f62c2c264156b503f96d7501/990cb/kuik.jpg 1020w,\n/static/13e29d44f62c2c264156b503f96d7501/151cf/kuik.jpg 1181w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-warm-image\" style=\"position:relative;\"><a href=\"#-warm-image\" aria-label=\" warm image permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• warm-image</h2>\n<p>The WarmImage CRD takes an image reference (with optional secrets) and prefetches it onto every node in your cluster. To install this custom resource onto your cluster, you may simply run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\n<span class=\"token function\">curl</span> https://raw.githubusercontent.com/mattmoor/warm-image/master/release.yaml <span class=\"token punctuation\">\\</span>\n    <span class=\"token operator\">|</span> kubectl create <span class=\"token parameter variable\">-f</span> -</code></pre></div>\n<p>Alternately you may git clone this repository and run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\nkubectl create <span class=\"token parameter variable\">-f</span> release.yaml</code></pre></div>\n<hr>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèÅ Conclusion</h2>\n<p>In this post, we showed you how to speed up Pod startup by caching images on nodes. By prefetching container images on worker nodes in your Kubernetes Cluster, you can significantly reduce Pod startup times, even for large images, down to a few seconds. This technique can greatly benefit customers running workloads such as machine learning, simulation, data analytics, and code builds, improving container startup performance and overall workload efficiency.</p>\n<p>By eliminating the need for additional management of infrastructure or Kubernetes resources, this approach offers a cost-efficient solution for addressing the slow container startup problem in Kubernetes-based environments.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **A Guide to Tools and Strategies of image cachingüö¶**\n\n## üöõ Intro\n\nWhen deploying a containerized application to a Kubernetes cluster, delays can occur due to the time it takes to pull necessary container images from the registry.\n\nThis delay can be especially problematic in scenarios where the application needs to scale out horizontally or process high-speed real-time data. Fortunately, there are several tools and strategies available to improve container image availability and caching in Kubernetes.\n\nIn this blog post, we will explore a comprehensive guide to these tools and strategies, including [kube-fledged](https://github.com/senthilrch/kube-fledged), [kuik](https://github.com/enix/kube-image-keeper), Kubernetes built-in image caching features, local caches, and monitoring and cleaning up unused images.\n\n![lifecycle](./lifecycle.png)\n\n## ‚ö°Ô∏è Preamble\n\nWhen you deploy a workload to Kubernetes, the containers in a certain Pod are naturally based on OCI container Images. These Images can be pulled from private/public repositories of many kinds. Kubernetes caches the images locally on every node that has pulled them, so that other Pods might use the same image. The settings for how and when Kubernetes pulls images can be found in the [documentation](https://kubernetes.io/docs/concepts/containers/images/).\n\n![cr](./cr.png)\n\nIn most use cases, it's not enough. Most Cloud Kubernetes clusters today require auto-scaling and are dynamically allocated Nodes based on the customer's usage. What if multiple nodes have to pull the same image multiple times? And if this image is heavy, that can take minutes. In the applicative autoscaling world, that is a relatively long time.\n\n## üí• The Solution\n\nThe expected solution needs to have a cache layer on top of Kubernetes, so that Kubernetes has a centralized image cache and all nodes \"pull\" from it. But because the cache needs to be very fast, the caching solutions need to sit inside Kubernetes, and all nodes should have the fastest latency towards it.\n\n## üîÆ Existing Solutions\n\nThe widely-used approach to tackle the problem of delays in pulling container images from the registry is to have a registry mirror running inside the cluster. Two widely used solutions are the in-cluster self-hosted registry and pull-through cache.\n\n- **In-cluster self-hosted registry:** A local registry is run within the Kubernetes cluster and is configured as a mirror registry in the container runtime. Any image pull request is directed to the in-cluster registry.\n- **Pull-through cache:** A cache of container images is built and managed directly on the worker nodes.\n\nOther existing solutions include using a reliable caching solution like [kuik](https://github.com/kuikproject/kuik), enabling image caching in Kubernetes, using a local cache, optimizing container image builds, and monitoring and cleaning up unused images.\n\n## üö¢ Harbor\n\n[Harbor](https://goharbor.io/) is a CNCF Graduated project that functions as a container registry, but most importantly as a Pull Through Proxy Cache.\n\nA pull-through proxy cache is a caching mechanism designed to optimize the distribution and retrieval of container images within a container registry environment. It acts as an intermediary between clients (such as container runtimes or build systems) and the upstream container registry.\n\nWhen a client requests a container image, the pull-through proxy cache checks if it already has a local copy of the requested image. If the image is present, the proxy cache serves it directly to the client, eliminating the need to download it from the upstream registry. This reduces network latency and conserves bandwidth.\n\nIf the requested image is not present in the local cache, the proxy cache acts as a regular proxy and forwards the request to the upstream registry. The proxy cache then retrieves the image from the registry and serves it to the client. Additionally, the proxy cache stores a copy of the image in its local cache for future requests.\n\n![harbor](./harbor.png)\n\n---\n\n## üê¶ kube-fledged\n\n![fledged](./fledged.png)\n\n[kube-fledged](https://github.com/senthilrch/kube-fledged) is a Kubernetes add-on or operator for creating and managing a cache of container images directly on the worker nodes of a Kubernetes cluster. It allows a user to define a list of images and onto which worker nodes those images should be cached (i.e. pulled). As a result, application pods start almost instantly, since the images need not be pulled from the registry. kube-fledged provides CRUD APIs to manage the lifecycle of the image cache, and supports several configurable parameters in order to customize the functioning as per one's needs.\n\nkube-fledged is designed and built as a general-purpose solution for managing an image cache in Kubernetes. Though the primary use case is to enable rapid Pod start-up and scaling, the solution supports a wide variety of use cases as mentioned below.\n\n### How kube-fledged works\n\n![working](./working.png)\n\nKubernetes allows developers to extend the Kubernetes API via Custom Resources. kube-fledged defines a custom resource of kind \"ImageCache\" and implements a custom controller (named kubefledged-controller). kubefledged-controller does the heavy-lifting for managing image cache. Users can use kubectl commands for creation and deletion of ImageCache resources.\n\n## üê≥ kubernetes-image-puller\n\nTo cache images, [Kubernetes Image Puller](https://github.com/che-incubator/kubernetes-image-puller/tree/main) creates a Daemonset on the desired cluster, which in turn creates a pod on each node in the cluster consisting of a list of containers with command `sleep 720h`. This ensures that all nodes in the cluster have those images cached. The sleep binary being used is [golang-based](https://github.com/che-incubator/kubernetes-image-puller/tree/main/sleep) (please see [Scratch Images](https://github.com/che-incubator/kubernetes-image-puller#scratch-images)). We also periodically check the health of the daemonset and re-create it if necessary.\n\nThe application can be deployed via Helm or by processing and applying OpenShift Templates. Also, there is a community-supported operator available on the [OperatorHub](https://operatorhub.io/operator/kubernetes-imagepuller-operator).\n\nüí° Kubernetes Image Puller deploys a huge number of containers (one container per image and per node / uses a daemonset for the caching mechanism), to fulfill the caching feature.\n\nLet's take this example: With 5 nodes & 10 images in cache, we already have 50 containers within the cluster dedicated for the caching feature.\n\n## üõ†Ô∏è Tugger\n\n[Tugger](https://github.com/jainishshah17/tugger) uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.\n\nüí° Tugger uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.\n\n## ‚öôÔ∏è kube-image-keeper (kuik)\n\n[kube-image-keeper](https://github.com/enix/kube-image-keeper) (a.k.a. kuik, which is pronounced /kw…™k/, like \"quick\") is a container image caching system for Kubernetes. It saves the container images used by your pods in its own local registry so that these images remain available if the original becomes unavailable.\n\n### How it works\n\nWhen a pod is created, kuik's mutating webhook rewrites its images on the fly, adding a `localhost:{port}/` prefix (the port is 7439 by default, and is configurable).\n\nOn `localhost:{port}`, there is an image proxy that serves images from kuik's caching registry (when the images have been cached) or directly from the original registry (when the images haven't been cached yet).\n\nOne controller watches pods, and when it notices new images, it creates CachedImage custom resources for these images.\n\nAnother controller watches these CachedImage custom resources, and copies images from source registries to kuik's caching registry accordingly.\n\n### Architecture and components\n\nIn kuik's namespace, you will find:\n\n- A Deployment to run kuik's controllers.\n- A DaemonSet to run kuik's image proxy.\n- A StatefulSet to run kuik's image cache (a Deployment is used instead when this component runs in HA mode).\n\nThe image cache will obviously require a bit of disk space to run (see [Garbage collection and limitations](https://github.com/enix/kube-image-keeper#garbage-collection-and-limitations)). Otherwise, kuik's components are fairly lightweight in terms of compute resources. This shows CPU and RAM usage with the default setup, featuring two controllers in HA mode:\n\n```shell\n$ kubectl top pods\nNAME                                             CPU(cores)   MEMORY(bytes)\nkube-image-keeper-0                              1m           86Mi\nkube-image-keeper-controllers-5b5cc9fcc6-bv6cp   1m           16Mi\nkube-image-keeper-controllers-5b5cc9fcc6-tjl7t   3m           24Mi\nkube-image-keeper-proxy-54lzk                    1m           19Mi\n```\n\n![kuik](./kuik.jpg)\n\n## üî• warm-image\n\nThe WarmImage CRD takes an image reference (with optional secrets) and prefetches it onto every node in your cluster. To install this custom resource onto your cluster, you may simply run:\n\n```shell\n# Install the CRD and Controller.\ncurl https://raw.githubusercontent.com/mattmoor/warm-image/master/release.yaml \\\n    | kubectl create -f -\n```\n\nAlternately you may git clone this repository and run:\n\n```shell\n# Install the CRD and Controller.\nkubectl create -f release.yaml\n```\n\n---\n\n## üèÅ Conclusion\n\nIn this post, we showed you how to speed up Pod startup by caching images on nodes. By prefetching container images on worker nodes in your Kubernetes Cluster, you can significantly reduce Pod startup times, even for large images, down to a few seconds. This technique can greatly benefit customers running workloads such as machine learning, simulation, data analytics, and code builds, improving container startup performance and overall workload efficiency.\n\nBy eliminating the need for additional management of infrastructure or Kubernetes resources, this approach offers a cost-efficient solution for addressing the slow container startup problem in Kubernetes-based environments.\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1426},"frontmatter":{"id":"98857acdae58056f614a2415","path":"/blog/improve-k8s-image-caching/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Improve Container Image Availability and Speed with Caching in Kubernetes üï∏","keywords":["Kubernetes","Image Caching","Container Management","Platform engineering","DevOps"],"excerpt":"Discover effective tools and strategies to enhance container image availability and speed through caching in Kubernetes.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBP/EABUBAQEAAAAAAAAAAAAAAAAAAAME/9oADAMBAAIQAxAAAAHNZ1Gywokn/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDABESITL/2gAIAQEAAQUCPNaJslZI606eqgZPz//EABgRAQEAAwAAAAAAAAAAAAAAAAEAERQx/9oACAEDAQE/AUAtfPL/xAAYEQACAwAAAAAAAAAAAAAAAAAAEQITUf/aAAgBAgEBPwFstlp//8QAHBAAAQMFAAAAAAAAAAAAAAAAAAERIRASMUFR/9oACAEBAAY/AoxwW9W2PJA6U//EABsQAQADAQADAAAAAAAAAAAAAAEAETEhYXGR/9oACAEBAAE/IdZ18ROgtXqwQdRJCqnkYApTsT4ep//aAAwDAQACAAMAAAAQwy//xAAZEQEAAgMAAAAAAAAAAAAAAAABABEhQcH/2gAIAQMBAT8QoBvsQrWJ/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhEf/aAAgBAgEBPxAzY4ar/8QAHRABAAMAAwADAAAAAAAAAAAAAQARITFBUWFxgf/aAAgBAQABPxDMHVFda4K4gki6QFifH1M5xtPHsjABXrT9muFkLr7AZdN9J//Z"},"images":{"fallback":{"src":"/static/102e5a0b223094562fa33d0dd81207b6/d3119/caching-cover.jpg","srcSet":"/static/102e5a0b223094562fa33d0dd81207b6/d3119/caching-cover.jpg 500w","sizes":"100vw"},"sources":[{"srcSet":"/static/102e5a0b223094562fa33d0dd81207b6/cd07d/caching-cover.webp 500w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5619999999999999}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/managing-deprecated-k8s-apis/","title":"Managing Deprecated Kubernetes APIs: Best Practices and Tools¬†‚ò∏Ô∏è","date":"2024-10-25 20:22:00"},"excerpt":"Techniques for Handling Deprecated APIs¬†üê≥ üìå Introduction As new features and functionality are added, older APIs are deprecated and‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/scaling-kubernetes-istio-metrics-hpa/","title":"Scaling Kubernetes Workloads with Istio Metrics and the Horizontal Pod Autoscaler","date":"2024-10-25 18:34:00"},"excerpt":"Autoscaling for Istio-Powered Kubernetes Applications üìö Introduction The need for effective autoscaling is very important in Kubernetes‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}