{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/improve-k8s-image-caching/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>A Guide to Tools and Strategies of image cachingüö¶</strong></p>\n</blockquote>\n<h2 id=\"-intro\" style=\"position:relative;\"><a href=\"#-intro\" aria-label=\" intro permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöõ Intro</h2>\n<p>When deploying a containerized application to a Kubernetes cluster, delays can occur due to the time it takes to pull necessary container images from the registry.</p>\n<p>This delay can be especially problematic in scenarios where the application needs to scale out horizontally or process high-speed real-time data. Fortunately, there are several tools and strategies available to improve container image availability and caching in Kubernetes.</p>\n<p>In this blog post, we will explore a comprehensive guide to these tools and strategies, including <a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a>, <a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, Kubernetes built-in image caching features, local caches, and monitoring and cleaning up unused images.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 535px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 214.11764705882354%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAArCAYAAAB4pah1AAAACXBIWXMAAAsTAAALEwEAmpwYAAADI0lEQVR42p2W147CMBBF/f9/xSsvIEC0pXdYeu9eHUs3MmlEa2nkxPFc36mOsaHxfr/dvN/v7c/Pjx0Oh/b39/fjW9owSYCLxcKWSiXbarXsYDD4P6A/AOp0Ovb5fNqsIxZQTMbjsW00Gu759Xr9D1Bg1+vVTqdTB3o4HP5nshQul4sz936/O2b9ft8FKQuoiQNrt9uOocbj8XC+zAJqwmBEVWC+IqDdbver+cYHI+/iwPQMaK/Xs8fjMRHUxDFLY0AKjUYjezqdYvcZHM+pgAocs1A4n8/2drs54TD2CnQ2mwU6H4CkBYoaOJ7cq9Vq7qByuWyLxaKdz+eB/2Q+aRXOT4M/2BymPplMXA5Sx9QzLH0TN5tNEPWID7fbrVOWQpz//PXlcukAU6O8Xq+ds8MAvghstVqlRzkJNK4DAZiah2Gl3W7nyk7vmglAlr4YAYQlEUbZX6vX6+6wzIB+gqNA9AGSkIOUXqZu46cCftIguckADcz+xvKDIeziFPQMa10HmUwOFz7lRhUhKslCoeCY8401RCUZMRlzVfSAqNxwBaYzcyjPVAmHq9IoxQggm5RnKLMRBdbVMPzGQbAoSYioU30Aslm5xmbqmXeeiTA1reSGabPZdFXDHtV6IiDBkSnyoUyEITNrALFPfkwEpOdhKqbDAlawo52JFWuwVyAjUeY0GmfSgO23v4hI6aGAuWKHwILI4tNcLhd8Y017FawIIN0aUKImX2lG8KfW8CHv7EXoqRHAarUa+AdgWBBh2OFDZhjBmu8w1TXCWgSQdKAnclfgfEwCHBCEiKLMM98143/ATbi985H8Qgl2nMoaM2CwYkawBJE/scD4961aPID4BEdLAMzn8848+VO+lJCLRm0KIBTxF34UCKdrnetUvkr6zzFiBTrCBvwHYyT8zHfdyfpv9F0WXKOcDAsCQmBIcPojisys4T+uArFMZYjZREq55ueefKR65nvS7Wf0t0rUYAcL2NBZYIPwzE8nM8z9vhkLmHQPcwDmZv0d/ug2Ev38YBbsKpVKEFE/AJkuqXD3Jh+RrP/XqYAwxGdIkr/iAP8A1zUQIALQissAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"lifecycle\" title=\"\" src=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png\" srcset=\"/static/3955bd7aec4ab77c01a64fe07bef3f19/04472/lifecycle.png 170w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/9f933/lifecycle.png 340w,\n/static/3955bd7aec4ab77c01a64fe07bef3f19/b5245/lifecycle.png 535w\" sizes=\"(max-width: 535px) 100vw, 535px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"Ô∏è-preamble\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-preamble\" aria-label=\"Ô∏è preamble permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ö°Ô∏è Preamble</h2>\n<p>When you deploy a workload to Kubernetes, the containers in a certain Pod are naturally based on OCI container Images. These Images can be pulled from private/public repositories of many kinds. Kubernetes caches the images locally on every node that has pulled them, so that other Pods might use the same image. The settings for how and when Kubernetes pulls images can be found in the <a href=\"https://kubernetes.io/docs/concepts/containers/images/\" target=\"_blank\" rel=\"noopener noreferrer\">documentation</a>.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 77.64705882352942%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACNUlEQVR42q2UXW/TMBSG+0vhgp/APb+AKz7E5aRpu0N8CFUgJBgSnWDaNLZO21ToutE2TZc0ydokthPbfTnHaaoVaaqEsHSSvLbznOPXThp2DsiCLv+pNb60JZ6+naI3LknOYSjDfF6FtdZFreu+2/p2OODGxxSPNhPsHOUkDYpSo6TQevXuQrM29Fwu7hrGVJqDoY1wavD1JIdUJSZhCN8PliDWI2+MoiidjqMIw6FPunCA6c0NhoMRvVtQIQykCrlMrasMF9EM3sSDLhVNKNCLMvwOBjClhCFgP8nRC/rQhXLQ0Uyiez1EqTIoSsrEBlNVoWGNxuOWh4cv9yByWr7VePE9wIOtXQiRUVqLzR8h7m/v4jqcOXtenyW4t7WHSz8kGPnNQK6Qq2Ng8+ACG5/PIGgJlqr+dHyF5+/byKVywNZpH8+ax4hT4RIedsd40jyCn2RuBfMa6IynCEZ9jAc9l4A9Ccce/KuuW542FnEwxujyJ5QUblOSSQiv14EUufN9CaxMN85cQdXwMwf7kuZioTUkjeVCup3lvoKsYs0wtylYbErdqol65aCqQpKHwo1lWbrsZ++5Il7NysG+C7g4p4gzg0GgMIwM/EiuvLwWyF8B+8XgCl5ivyuwc67x7lCj1VGIp8LNqw8223UnsK7SwSisKfGtI/DqwOJD2+LNvkKWS+q3S1/rT+5O4N8tkxphlCCOI8Qzuf7nsG4CV9H51cXJ6TnSNFt698/A2tv6L7Ouwj9wPdX8jT7mLQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"cr\" title=\"\" src=\"/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png\" srcset=\"/static/13fb62f279e7732af7156210ef5c9ba1/04472/cr.png 170w,\n/static/13fb62f279e7732af7156210ef5c9ba1/9f933/cr.png 340w,\n/static/13fb62f279e7732af7156210ef5c9ba1/c5bb3/cr.png 680w,\n/static/13fb62f279e7732af7156210ef5c9ba1/3c051/cr.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>In most use cases, it's not enough. Most Cloud Kubernetes clusters today require auto-scaling and are dynamically allocated Nodes based on the customer's usage. What if multiple nodes have to pull the same image multiple times? And if this image is heavy, that can take minutes. In the applicative autoscaling world, that is a relatively long time.</p>\n<h2 id=\"-the-solution\" style=\"position:relative;\"><a href=\"#-the-solution\" aria-label=\" the solution permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üí• The Solution</h2>\n<p>The expected solution needs to have a cache layer on top of Kubernetes, so that Kubernetes has a centralized image cache and all nodes \"pull\" from it. But because the cache needs to be very fast, the caching solutions need to sit inside Kubernetes, and all nodes should have the fastest latency towards it.</p>\n<h2 id=\"-existing-solutions\" style=\"position:relative;\"><a href=\"#-existing-solutions\" aria-label=\" existing solutions permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîÆ Existing Solutions</h2>\n<p>The widely-used approach to tackle the problem of delays in pulling container images from the registry is to have a registry mirror running inside the cluster. Two widely used solutions are the in-cluster self-hosted registry and pull-through cache.</p>\n<ul>\n<li><strong>In-cluster self-hosted registry:</strong> A local registry is run within the Kubernetes cluster and is configured as a mirror registry in the container runtime. Any image pull request is directed to the in-cluster registry.</li>\n<li><strong>Pull-through cache:</strong> A cache of container images is built and managed directly on the worker nodes.</li>\n</ul>\n<p>Other existing solutions include using a reliable caching solution like <a href=\"https://github.com/kuikproject/kuik\" target=\"_blank\" rel=\"noopener noreferrer\">kuik</a>, enabling image caching in Kubernetes, using a local cache, optimizing container image builds, and monitoring and cleaning up unused images.</p>\n<h2 id=\"-harbor\" style=\"position:relative;\"><a href=\"#-harbor\" aria-label=\" harbor permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üö¢ Harbor</h2>\n<p><a href=\"https://goharbor.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Harbor</a> is a CNCF Graduated project that functions as a container registry, but most importantly as a Pull Through Proxy Cache.</p>\n<p>A pull-through proxy cache is a caching mechanism designed to optimize the distribution and retrieval of container images within a container registry environment. It acts as an intermediary between clients (such as container runtimes or build systems) and the upstream container registry.</p>\n<p>When a client requests a container image, the pull-through proxy cache checks if it already has a local copy of the requested image. If the image is present, the proxy cache serves it directly to the client, eliminating the need to download it from the upstream registry. This reduces network latency and conserves bandwidth.</p>\n<p>If the requested image is not present in the local cache, the proxy cache acts as a regular proxy and forwards the request to the upstream registry. The proxy cache then retrieves the image from the registry and serves it to the client. Additionally, the proxy cache stores a copy of the image in its local cache for future requests.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 125.88235294117646%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAAAsTAAALEwEAmpwYAAADp0lEQVR42q1Vz28bRRTOn4VQb9yL1Ct3DvAHIBCol9zoCSH1REAVFFVUQAuVEFQtlFCRJnGSxo1T27HjTWKvHa+9v3dnZ2Y/3rzddW2nBQl1pNHumx/ffO+9782sgFoicgRxjtfRVpQGvnkQ4vO7HtxQ0VAOnefI/0dnQCFzXL/nY/XbKU5GKbTKkIoMUkpkWUbsBUQm2BbGTgV/KzulebMupXEDumJQHV+hZ6c0ITCyh3CnHjLakKYp7MEAnhcyQJIkGJz1EQQRz0dRhOHARhjGvHYGaH7MhlhI3G+eEtNj5BQLLxb4jWx7bJGdwwlTPGidYjg+AWh+5Cd42LYwnvQhaO8MUOscigCDJMOVr/Zx/Y8Gx9L2Elxee4obj5scn+44wttre7i12Wa7YQe4/OUuftrt0nJFZywxDKMYj3caaPbs4oAgxHqtgZZlm1Mx9Tz8uX2A7tk5mQrO1GXbGjgcrgsuay3R6fTgOA4zEBTw/qlFMXVgkpimCdu+5/K868dsx6FPgGrRZQoKRq7G7b8Efq0llN2M3a7aTB6FheFEkNQCfLdOyqBBpeYAFQNq7HUyXP3axbXbU4ynEWU1IsZdZmxZFrrdLo6Pe4Qn8OTAxbufOfjgiwnCRCOnEOj5GGqKkSBWv/w9xM7zCSrmRmNmzjAwYTG9ary26fF/li1l2XSThE7rEN1OG1KphZIqQCUDm0NYp4lPyRM8L+dd1rlmw/c91Op1tE5GRFszSMUuDKPZf9Wl0igrjpgvJEXz4FHnOe7t/oxa84QzKnkjySdOYZ2fIxMRb6xcr5heYFjEMKc6jbFd38DQ7hdulqc3HYH3ftjFk2anHM9nmTegFxgWOlRlcLNSRpjdIGeTEKs/7mGnN3k1oHoFYHWzmDBUgGkSw+61kEZBCfQitv/JsKoavp5IRsXVlFJVBCwNUz3mCkt5Ts4OfQlDuXBRmo2S2Koy+Aa0ugNNiSqZcdUU7uecnBkgV0spBVP0ESl/6ASY+MQk0zymVOGi2fz9ho2Pbz6DNU65wmS5l2/s5TchpvflsK+x3tT4vaHQsgtJqTJRW50Ab36yiUtXt/H+2mFZ2XNvyjJgSrJ6eCBxpyZxY11is6MWAO/Xp3jjow289eEjvHNtC8tP28rLXi57mmHiRrCdCNOwStaL+U/vtHFl9RG2muMFGV0ArBLiui6e7tfRPjriillYY3TZO0L7cB9R4M+N/gtDozGT2UoKyy0jiZleFcC8y/8AOpCAHF2eNjUAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"harbor\" title=\"\" src=\"/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png\" srcset=\"/static/f87b305ff11726efc93f028b32db411e/04472/harbor.png 170w,\n/static/f87b305ff11726efc93f028b32db411e/9f933/harbor.png 340w,\n/static/f87b305ff11726efc93f028b32db411e/c5bb3/harbor.png 680w,\n/static/f87b305ff11726efc93f028b32db411e/3c051/harbor.png 760w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<hr>\n<h2 id=\"-kube-fledged\" style=\"position:relative;\"><a href=\"#-kube-fledged\" aria-label=\" kube fledged permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê¶ kube-fledged</h2>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 31.176470588235293%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABEUlEQVR42o2QzUsCURTF/bfbRBkEfa2jdcusCN3UItxMmZEpCKnjRybpjJpN6XNeM+P7dYeUhFp44Lx73uFyONwE/yGKoFyGdhtqNYxlYbRmFSQWwsQ08ymB5jaHeffAcTC2/ePP5jvCmTwLHfNPIMvJMapP8OlhOh1CX2Ng9YY6MOTrirdxyOsoIFsR3ewyKDdQ/SHHlsddfcLYjyg+T7lvKOye5mUY4H6EZJ/GBJH5DQzlkzx12Mm47KVdDjJ91lIyr0bkGj6HlwPWT3okzx22hLvpPhtnDpvC7QuHfdkPlwPje1S6WloobmxFse1TaClK0kbpiKb7Jd6Uh9aUR/FSeY+j6yFWdUJJ/IL4i5N8A7+1wrOBb4LNAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"fledged\" title=\"\" src=\"/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png\" srcset=\"/static/8243b2682f856240dd3fae34b0836534/04472/fledged.png 170w,\n/static/8243b2682f856240dd3fae34b0836534/9f933/fledged.png 340w,\n/static/8243b2682f856240dd3fae34b0836534/c5bb3/fledged.png 680w,\n/static/8243b2682f856240dd3fae34b0836534/8ae3e/fledged.png 756w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p><a href=\"https://github.com/senthilrch/kube-fledged\" target=\"_blank\" rel=\"noopener noreferrer\">kube-fledged</a> is a Kubernetes add-on or operator for creating and managing a cache of container images directly on the worker nodes of a Kubernetes cluster. It allows a user to define a list of images and onto which worker nodes those images should be cached (i.e. pulled). As a result, application pods start almost instantly, since the images need not be pulled from the registry. kube-fledged provides CRUD APIs to manage the lifecycle of the image cache, and supports several configurable parameters in order to customize the functioning as per one's needs.</p>\n<p>kube-fledged is designed and built as a general-purpose solution for managing an image cache in Kubernetes. Though the primary use case is to enable rapid Pod start-up and scaling, the solution supports a wide variety of use cases as mentioned below.</p>\n<h3 id=\"how-kube-fledged-works\" style=\"position:relative;\"><a href=\"#how-kube-fledged-works\" aria-label=\"how kube fledged works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How kube-fledged works</h3>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADOklEQVR42oVTSUwTYRT+UWSxgloFXPCgQaJIqoSIhkuNihpUgiY1JkBQa6IHITYViSxOlEW7pHaB0unMtNNObUthqFJk2CRUbKHGICpHEyUx8eDVk7bznKlBTTz4kpf3v/d/b/9/hP4ihSKQVlfjalI2EnlHLyrzAKFV62lHcxbj+igROIOibieBGLYK/Uspf50hqdTWunPr6xhcqfTKEARWi7Zcmsa2+ryxLR4mthknVEmcWidRqVSZvwOD4A+Qgv5HGIalipkBUEq9vD4jJ6donaBnipyfn5/Z0NCQDkKgClmFpB6hDBGzUmrKSgCTyZSuwAhpSeeA8bAmyB58ODRc2jlwXbw34rNqt3ueZR7H/AQV0a74HWhnDsruMu1lrfadf5oWMimwgFSjIbOOdnnzDhm45SPWaZDbX0KZfsQnYjy+N4MzM19gcvITsOzSB3SqIV207+sKthRrRqC4O1iJTpxQSAsKyrJRDlp3pakpq0pgAZOx/vzNXWkKdVFaHVaYrcSkCOVJLl7BtjW2OXa3GLitGDEmXZlbcdeQXNYx2CvDhgqF1RalYV3+Yr9+qtqr56pY/fMqXP+sWqdjT1sehU5azdxxS99MhdYSOmfsCVXqbaPHTO5wuckVlZvpcFmybWHG4qiSy8Xx12ss9heLtZ4JuOAaSVTTw6AmJ3kPFQOamgMHEQWPcx4G+xf4IfYtz7FLEPK/47mB9xDwLYC+93m5WKVap5Mkgy+6FyVGcvbzRo8H8tn+xBo/DZUEC1bzBKgesdBqfJKU900j0IfPwg1yDK46OP4SNZowuOfBRcxVbm9zN+/vDET2tromEIYPr8WJyHIHFYZmfDJ+yzbOa61TvNbM8Uozyzean/KXBXnHEuINfdP8DhfDb+hnEqk+Z+IaxUGPZqxmj3FqQe54BUX3+r+icaHCHiryucAbgBzf40S2j4EKYhActll42DMOGvMoPLBwYLJNA0VFwUC+AC0Z5jVkOOH0LICdip3Z1OI+W9LNVu3HnBtQQBFYbbbNRBT0KJxyBr8fdwbj1+zP4rQ9GieJSJy0v4xTIhPROCHoTkHSxNwPFzmXEPRveut06a83DMlf8xMHkalxHxoy5QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"working\" title=\"\" src=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png\" srcset=\"/static/ec0f9e3b44c21ef9e43a0e15855782a7/04472/working.png 170w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/9f933/working.png 340w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/c5bb3/working.png 680w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/b12f7/working.png 1020w,\n/static/ec0f9e3b44c21ef9e43a0e15855782a7/525d3/working.png 1090w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>Kubernetes allows developers to extend the Kubernetes API via Custom Resources. kube-fledged defines a custom resource of kind \"ImageCache\" and implements a custom controller (named kubefledged-controller). kubefledged-controller does the heavy-lifting for managing image cache. Users can use kubectl commands for creation and deletion of ImageCache resources.</p>\n<h2 id=\"-kubernetes-image-puller\" style=\"position:relative;\"><a href=\"#-kubernetes-image-puller\" aria-label=\" kubernetes image puller permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê≥ kubernetes-image-puller</h2>\n<p>To cache images, <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Image Puller</a> creates a Daemonset on the desired cluster, which in turn creates a pod on each node in the cluster consisting of a list of containers with command <code class=\"language-text\">sleep 720h</code>. This ensures that all nodes in the cluster have those images cached. The sleep binary being used is <a href=\"https://github.com/che-incubator/kubernetes-image-puller/tree/main/sleep\" target=\"_blank\" rel=\"noopener noreferrer\">golang-based</a> (please see <a href=\"https://github.com/che-incubator/kubernetes-image-puller#scratch-images\" target=\"_blank\" rel=\"noopener noreferrer\">Scratch Images</a>). We also periodically check the health of the daemonset and re-create it if necessary.</p>\n<p>The application can be deployed via Helm or by processing and applying OpenShift Templates. Also, there is a community-supported operator available on the <a href=\"https://operatorhub.io/operator/kubernetes-imagepuller-operator\" target=\"_blank\" rel=\"noopener noreferrer\">OperatorHub</a>.</p>\n<p>üí° Kubernetes Image Puller deploys a huge number of containers (one container per image and per node / uses a daemonset for the caching mechanism), to fulfill the caching feature.</p>\n<p>Let's take this example: With 5 nodes &#x26; 10 images in cache, we already have 50 containers within the cluster dedicated for the caching feature.</p>\n<h2 id=\"Ô∏è-tugger\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-tugger\" aria-label=\"Ô∏è tugger permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Tugger</h2>\n<p><a href=\"https://github.com/jainishshah17/tugger\" target=\"_blank\" rel=\"noopener noreferrer\">Tugger</a> uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<p>üí° Tugger uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.</p>\n<h2 id=\"Ô∏è-kube-image-keeper-kuik\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-kube-image-keeper-kuik\" aria-label=\"Ô∏è kube image keeper kuik permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è kube-image-keeper (kuik)</h2>\n<p><a href=\"https://github.com/enix/kube-image-keeper\" target=\"_blank\" rel=\"noopener noreferrer\">kube-image-keeper</a> (a.k.a. kuik, which is pronounced /kw…™k/, like \"quick\") is a container image caching system for Kubernetes. It saves the container images used by your pods in its own local registry so that these images remain available if the original becomes unavailable.</p>\n<h3 id=\"how-it-works\" style=\"position:relative;\"><a href=\"#how-it-works\" aria-label=\"how it works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How it works</h3>\n<p>When a pod is created, kuik's mutating webhook rewrites its images on the fly, adding a <code class=\"language-text\">localhost:{port}/</code> prefix (the port is 7439 by default, and is configurable).</p>\n<p>On <code class=\"language-text\">localhost:{port}</code>, there is an image proxy that serves images from kuik's caching registry (when the images have been cached) or directly from the original registry (when the images haven't been cached yet).</p>\n<p>One controller watches pods, and when it notices new images, it creates CachedImage custom resources for these images.</p>\n<p>Another controller watches these CachedImage custom resources, and copies images from source registries to kuik's caching registry accordingly.</p>\n<h3 id=\"architecture-and-components\" style=\"position:relative;\"><a href=\"#architecture-and-components\" aria-label=\"architecture and components permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Architecture and components</h3>\n<p>In kuik's namespace, you will find:</p>\n<ul>\n<li>A Deployment to run kuik's controllers.</li>\n<li>A DaemonSet to run kuik's image proxy.</li>\n<li>A StatefulSet to run kuik's image cache (a Deployment is used instead when this component runs in HA mode).</li>\n</ul>\n<p>The image cache will obviously require a bit of disk space to run (see <a href=\"https://github.com/enix/kube-image-keeper#garbage-collection-and-limitations\" target=\"_blank\" rel=\"noopener noreferrer\">Garbage collection and limitations</a>). Otherwise, kuik's components are fairly lightweight in terms of compute resources. This shows CPU and RAM usage with the default setup, featuring two controllers in HA mode:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ kubectl <span class=\"token function\">top</span> pods\nNAME                                             CPU<span class=\"token punctuation\">(</span>cores<span class=\"token punctuation\">)</span>   MEMORY<span class=\"token punctuation\">(</span>bytes<span class=\"token punctuation\">)</span>\nkube-image-keeper-0                              1m           86Mi\nkube-image-keeper-controllers-5b5cc9fcc6-bv6cp   1m           16Mi\nkube-image-keeper-controllers-5b5cc9fcc6-tjl7t   3m           24Mi\nkube-image-keeper-proxy-54lzk                    1m           19Mi</code></pre></div>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 54.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5qiQf/xAAVEAEBAAAAAAAAAAAAAAAAAAAQAf/aAAgBAQABBQJr/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERQRAxUXH/2gAIAQEAAT8hcqyHQp7gtFv0/9oADAMBAAIAAwAAABAwD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExcYH/2gAIAQEAAT8QsViu7FEVlQa12AXZw8gpq/U//9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuik\" title=\"\" src=\"/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg\" srcset=\"/static/13e29d44f62c2c264156b503f96d7501/651be/kuik.jpg 170w,\n/static/13e29d44f62c2c264156b503f96d7501/d30a3/kuik.jpg 340w,\n/static/13e29d44f62c2c264156b503f96d7501/7bf67/kuik.jpg 680w,\n/static/13e29d44f62c2c264156b503f96d7501/990cb/kuik.jpg 1020w,\n/static/13e29d44f62c2c264156b503f96d7501/151cf/kuik.jpg 1181w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-warm-image\" style=\"position:relative;\"><a href=\"#-warm-image\" aria-label=\" warm image permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• warm-image</h2>\n<p>The WarmImage CRD takes an image reference (with optional secrets) and prefetches it onto every node in your cluster. To install this custom resource onto your cluster, you may simply run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\n<span class=\"token function\">curl</span> https://raw.githubusercontent.com/mattmoor/warm-image/master/release.yaml <span class=\"token punctuation\">\\</span>\n    <span class=\"token operator\">|</span> kubectl create <span class=\"token parameter variable\">-f</span> -</code></pre></div>\n<p>Alternately you may git clone this repository and run:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># Install the CRD and Controller.</span>\nkubectl create <span class=\"token parameter variable\">-f</span> release.yaml</code></pre></div>\n<hr>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üèÅ Conclusion</h2>\n<p>In this post, we showed you how to speed up Pod startup by caching images on nodes. By prefetching container images on worker nodes in your Kubernetes Cluster, you can significantly reduce Pod startup times, even for large images, down to a few seconds. This technique can greatly benefit customers running workloads such as machine learning, simulation, data analytics, and code builds, improving container startup performance and overall workload efficiency.</p>\n<p>By eliminating the need for additional management of infrastructure or Kubernetes resources, this approach offers a cost-efficient solution for addressing the slow container startup problem in Kubernetes-based environments.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **A Guide to Tools and Strategies of image cachingüö¶**\n\n## üöõ Intro\n\nWhen deploying a containerized application to a Kubernetes cluster, delays can occur due to the time it takes to pull necessary container images from the registry.\n\nThis delay can be especially problematic in scenarios where the application needs to scale out horizontally or process high-speed real-time data. Fortunately, there are several tools and strategies available to improve container image availability and caching in Kubernetes.\n\nIn this blog post, we will explore a comprehensive guide to these tools and strategies, including [kube-fledged](https://github.com/senthilrch/kube-fledged), [kuik](https://github.com/enix/kube-image-keeper), Kubernetes built-in image caching features, local caches, and monitoring and cleaning up unused images.\n\n![lifecycle](./lifecycle.png)\n\n## ‚ö°Ô∏è Preamble\n\nWhen you deploy a workload to Kubernetes, the containers in a certain Pod are naturally based on OCI container Images. These Images can be pulled from private/public repositories of many kinds. Kubernetes caches the images locally on every node that has pulled them, so that other Pods might use the same image. The settings for how and when Kubernetes pulls images can be found in the [documentation](https://kubernetes.io/docs/concepts/containers/images/).\n\n![cr](./cr.png)\n\nIn most use cases, it's not enough. Most Cloud Kubernetes clusters today require auto-scaling and are dynamically allocated Nodes based on the customer's usage. What if multiple nodes have to pull the same image multiple times? And if this image is heavy, that can take minutes. In the applicative autoscaling world, that is a relatively long time.\n\n## üí• The Solution\n\nThe expected solution needs to have a cache layer on top of Kubernetes, so that Kubernetes has a centralized image cache and all nodes \"pull\" from it. But because the cache needs to be very fast, the caching solutions need to sit inside Kubernetes, and all nodes should have the fastest latency towards it.\n\n## üîÆ Existing Solutions\n\nThe widely-used approach to tackle the problem of delays in pulling container images from the registry is to have a registry mirror running inside the cluster. Two widely used solutions are the in-cluster self-hosted registry and pull-through cache.\n\n- **In-cluster self-hosted registry:** A local registry is run within the Kubernetes cluster and is configured as a mirror registry in the container runtime. Any image pull request is directed to the in-cluster registry.\n- **Pull-through cache:** A cache of container images is built and managed directly on the worker nodes.\n\nOther existing solutions include using a reliable caching solution like [kuik](https://github.com/kuikproject/kuik), enabling image caching in Kubernetes, using a local cache, optimizing container image builds, and monitoring and cleaning up unused images.\n\n## üö¢ Harbor\n\n[Harbor](https://goharbor.io/) is a CNCF Graduated project that functions as a container registry, but most importantly as a Pull Through Proxy Cache.\n\nA pull-through proxy cache is a caching mechanism designed to optimize the distribution and retrieval of container images within a container registry environment. It acts as an intermediary between clients (such as container runtimes or build systems) and the upstream container registry.\n\nWhen a client requests a container image, the pull-through proxy cache checks if it already has a local copy of the requested image. If the image is present, the proxy cache serves it directly to the client, eliminating the need to download it from the upstream registry. This reduces network latency and conserves bandwidth.\n\nIf the requested image is not present in the local cache, the proxy cache acts as a regular proxy and forwards the request to the upstream registry. The proxy cache then retrieves the image from the registry and serves it to the client. Additionally, the proxy cache stores a copy of the image in its local cache for future requests.\n\n![harbor](./harbor.png)\n\n---\n\n## üê¶ kube-fledged\n\n![fledged](./fledged.png)\n\n[kube-fledged](https://github.com/senthilrch/kube-fledged) is a Kubernetes add-on or operator for creating and managing a cache of container images directly on the worker nodes of a Kubernetes cluster. It allows a user to define a list of images and onto which worker nodes those images should be cached (i.e. pulled). As a result, application pods start almost instantly, since the images need not be pulled from the registry. kube-fledged provides CRUD APIs to manage the lifecycle of the image cache, and supports several configurable parameters in order to customize the functioning as per one's needs.\n\nkube-fledged is designed and built as a general-purpose solution for managing an image cache in Kubernetes. Though the primary use case is to enable rapid Pod start-up and scaling, the solution supports a wide variety of use cases as mentioned below.\n\n### How kube-fledged works\n\n![working](./working.png)\n\nKubernetes allows developers to extend the Kubernetes API via Custom Resources. kube-fledged defines a custom resource of kind \"ImageCache\" and implements a custom controller (named kubefledged-controller). kubefledged-controller does the heavy-lifting for managing image cache. Users can use kubectl commands for creation and deletion of ImageCache resources.\n\n## üê≥ kubernetes-image-puller\n\nTo cache images, [Kubernetes Image Puller](https://github.com/che-incubator/kubernetes-image-puller/tree/main) creates a Daemonset on the desired cluster, which in turn creates a pod on each node in the cluster consisting of a list of containers with command `sleep 720h`. This ensures that all nodes in the cluster have those images cached. The sleep binary being used is [golang-based](https://github.com/che-incubator/kubernetes-image-puller/tree/main/sleep) (please see [Scratch Images](https://github.com/che-incubator/kubernetes-image-puller#scratch-images)). We also periodically check the health of the daemonset and re-create it if necessary.\n\nThe application can be deployed via Helm or by processing and applying OpenShift Templates. Also, there is a community-supported operator available on the [OperatorHub](https://operatorhub.io/operator/kubernetes-imagepuller-operator).\n\nüí° Kubernetes Image Puller deploys a huge number of containers (one container per image and per node / uses a daemonset for the caching mechanism), to fulfill the caching feature.\n\nLet's take this example: With 5 nodes & 10 images in cache, we already have 50 containers within the cluster dedicated for the caching feature.\n\n## üõ†Ô∏è Tugger\n\n[Tugger](https://github.com/jainishshah17/tugger) uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.\n\nüí° Tugger uses a single configuration file, defined through its Helm file values. It does not allow us to segregate \"system\" configurations (e.g., exclude specific images from the caching system) and \"users\" configurations.\n\n## ‚öôÔ∏è kube-image-keeper (kuik)\n\n[kube-image-keeper](https://github.com/enix/kube-image-keeper) (a.k.a. kuik, which is pronounced /kw…™k/, like \"quick\") is a container image caching system for Kubernetes. It saves the container images used by your pods in its own local registry so that these images remain available if the original becomes unavailable.\n\n### How it works\n\nWhen a pod is created, kuik's mutating webhook rewrites its images on the fly, adding a `localhost:{port}/` prefix (the port is 7439 by default, and is configurable).\n\nOn `localhost:{port}`, there is an image proxy that serves images from kuik's caching registry (when the images have been cached) or directly from the original registry (when the images haven't been cached yet).\n\nOne controller watches pods, and when it notices new images, it creates CachedImage custom resources for these images.\n\nAnother controller watches these CachedImage custom resources, and copies images from source registries to kuik's caching registry accordingly.\n\n### Architecture and components\n\nIn kuik's namespace, you will find:\n\n- A Deployment to run kuik's controllers.\n- A DaemonSet to run kuik's image proxy.\n- A StatefulSet to run kuik's image cache (a Deployment is used instead when this component runs in HA mode).\n\nThe image cache will obviously require a bit of disk space to run (see [Garbage collection and limitations](https://github.com/enix/kube-image-keeper#garbage-collection-and-limitations)). Otherwise, kuik's components are fairly lightweight in terms of compute resources. This shows CPU and RAM usage with the default setup, featuring two controllers in HA mode:\n\n```shell\n$ kubectl top pods\nNAME                                             CPU(cores)   MEMORY(bytes)\nkube-image-keeper-0                              1m           86Mi\nkube-image-keeper-controllers-5b5cc9fcc6-bv6cp   1m           16Mi\nkube-image-keeper-controllers-5b5cc9fcc6-tjl7t   3m           24Mi\nkube-image-keeper-proxy-54lzk                    1m           19Mi\n```\n\n![kuik](./kuik.jpg)\n\n## üî• warm-image\n\nThe WarmImage CRD takes an image reference (with optional secrets) and prefetches it onto every node in your cluster. To install this custom resource onto your cluster, you may simply run:\n\n```shell\n# Install the CRD and Controller.\ncurl https://raw.githubusercontent.com/mattmoor/warm-image/master/release.yaml \\\n    | kubectl create -f -\n```\n\nAlternately you may git clone this repository and run:\n\n```shell\n# Install the CRD and Controller.\nkubectl create -f release.yaml\n```\n\n---\n\n## üèÅ Conclusion\n\nIn this post, we showed you how to speed up Pod startup by caching images on nodes. By prefetching container images on worker nodes in your Kubernetes Cluster, you can significantly reduce Pod startup times, even for large images, down to a few seconds. This technique can greatly benefit customers running workloads such as machine learning, simulation, data analytics, and code builds, improving container startup performance and overall workload efficiency.\n\nBy eliminating the need for additional management of infrastructure or Kubernetes resources, this approach offers a cost-efficient solution for addressing the slow container startup problem in Kubernetes-based environments.\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1426},"frontmatter":{"id":"98857acdae58056f614a2415","path":"/blog/improve-k8s-image-caching/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Improve Container Image Availability and Speed with Caching in Kubernetes üï∏","keywords":["Kubernetes","Image Caching","Container Management","Platform engineering","DevOps"],"excerpt":"Discover effective tools and strategies to enhance container image availability and speed through caching in Kubernetes.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBP/EABUBAQEAAAAAAAAAAAAAAAAAAAME/9oADAMBAAIQAxAAAAHNZ1Gywokn/8QAGxAAAwACAwAAAAAAAAAAAAAAAQIDABESITL/2gAIAQEAAQUCPNaJslZI606eqgZPz//EABgRAQEAAwAAAAAAAAAAAAAAAAEAERQx/9oACAEDAQE/AUAtfPL/xAAYEQACAwAAAAAAAAAAAAAAAAAAEQITUf/aAAgBAgEBPwFstlp//8QAHBAAAQMFAAAAAAAAAAAAAAAAAAERIRASMUFR/9oACAEBAAY/AoxwW9W2PJA6U//EABsQAQACAgMAAAAAAAAAAAAAAAEAMREhYXGR/9oACAEBAAE/IbZt4idBWzawQbRJCTDqMAYU3E8Op//aAAwDAQACAAMAAAAQ0C//xAAZEQEAAgMAAAAAAAAAAAAAAAABABEhQcH/2gAIAQMBAT8QoBvsQrWJ/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhQf/aAAgBAgEBPxAzY6K//8QAHhABAAMAAgIDAAAAAAAAAAAAAQARIUFRMWFxgbH/2gAIAQEAAT8QzB1RXWvBUEkXSAsT18TOcbTp5IQAr1p9zXCyF17gMum+H5P/2Q=="},"images":{"fallback":{"src":"/static/102e5a0b223094562fa33d0dd81207b6/d3119/caching-cover.jpg","srcSet":"/static/102e5a0b223094562fa33d0dd81207b6/d3119/caching-cover.jpg 500w","sizes":"100vw"},"sources":[{"srcSet":"/static/102e5a0b223094562fa33d0dd81207b6/cd07d/caching-cover.webp 500w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5619999999999999}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/managing-deprecated-k8s-apis/","title":"Managing Deprecated Kubernetes APIs: Best Practices and Tools¬†‚ò∏Ô∏è","date":"2024-10-25 20:22:00"},"excerpt":"Techniques for Handling Deprecated APIs¬†üê≥ üìå Introduction As new features and functionality are added, older APIs are deprecated and‚Ä¶","html":"<blockquote>\n<p><strong>Techniques for Handling Deprecated APIs¬†üê≥</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìå Introduction</h2>\n<p>As new features and functionality are added, older APIs are deprecated and eventually removed. While this is a necessary part of the evolution of Kubernetes, it can create challenges for organizations that rely on the platform to run their applications. Kubernetes API serves as the interface to interact with a K8 cluster. If deprecated APIs are still active in the cluster, disruptions may occur.</p>\n<p>In this blog post, we'll explore what deprecated Kubernetes APIs are, why they matter, and how to manage them effectively. We'll also introduce some of the available tools for handling obsolete APIs in Kubernetes and provide best practices for managing deprecated APIs. By the end of this post, you'll have a better understanding of how to approach Kubernetes cluster upgrades and gain confidence in your infrastructure.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABcBAAMBAAAAAAAAAAAAAAAAAAABAgT/2gAMAwEAAhADEAAAAdmbKzoYz//EABYQAAMAAAAAAAAAAAAAAAAAAAAgIf/aAAgBAQABBQIq/wD/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAACAgMAAAAAAAAAAAAAAAABEQAxIFGh/9oACAEBAAE/IUXfIUO4Kw//2gAMAwEAAgADAAAAECsP/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Qp//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/EIf/xAAaEAEBAAMBAQAAAAAAAAAAAAABEQAQMVEh/9oACAEBAAE/EGpDPMP5YJ7MqL3UHoa//9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"groups\" title=\"\" src=\"/static/81a36cd7d9ead89424bd3f916109d6c7/7bf67/groups.jpg\" srcset=\"/static/81a36cd7d9ead89424bd3f916109d6c7/651be/groups.jpg 170w,\n/static/81a36cd7d9ead89424bd3f916109d6c7/d30a3/groups.jpg 340w,\n/static/81a36cd7d9ead89424bd3f916109d6c7/7bf67/groups.jpg 680w,\n/static/81a36cd7d9ead89424bd3f916109d6c7/990cb/groups.jpg 1020w,\n/static/81a36cd7d9ead89424bd3f916109d6c7/72e01/groups.jpg 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<hr>\n<h3 id=\"-api-lifecycles\" style=\"position:relative;\"><a href=\"#-api-lifecycles\" aria-label=\" api lifecycles permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üåÄ API Lifecycles</h3>\n<p>Kubernetes follows a maturity progression of alpha ‚Üí beta ‚Üí stable with some additional versioning, so that a resource can iterate without needing to progress to the next level of maturity. An alpha resource can start at <code class=\"language-text\">v1alpha1</code> and iterate with <code class=\"language-text\">v1alpha2</code> or, if there are breaking changes, maybe <code class=\"language-text\">v2alpha1</code>. A beta API may be the same spec as an alpha API, but the maturity and contract with the user will be different.</p>\n<ul>\n<li><strong>Alpha APIs</strong> are experimental. They can have bugs and backward-incompatible changes. They are not enabled by default, and you should use them sparingly.</li>\n<li><strong>Beta APIs</strong> are well-tested and enabled by default. They can be relied upon for future functionality, but their implementation may change based on user feedback or constraints such as scalability.</li>\n<li><strong>Stable APIs</strong> don't have \"beta\" or \"alpha\" names. They are represented with a version (e.g., <code class=\"language-text\">v1</code>) and their implementation should not have breaking changes without changing the version number.</li>\n</ul>\n<hr>\n<p>The lifecycle I mentioned runs as follows:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 454px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 41.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZklEQVR42mNgAIL///8zhob+Z4ZhoAgTAxigitfXYxeH6KEMMOKVNS9+aGyY/7BaK/t+kWH+oxrzooc2IHGToieyJkWPq3RyHxYZFj6qsSh5FAgSt69/yWNc+DhPN+9BmVbOwwqTgodpQG8iLDEpfZXv3vbnf0jXxx/u7X//mxQ/aQeJq0dsDrFr+PHfp+3DTzeguGnpq00gcSm7JkPritefPVs//Q/o+vbfqurtXYbQVQhvGwEN9Gj//T+o6wPQwD//TUuetYHENWP2BDg0fgeKv//p1QUUL30JNlDeZ76GbfWrT4Fdn/6H9X37b1P19g4i3MFefmZsWPikRivrUbFBwaNa86KnYC8b592VMy56Uq2T8wQsblHyFOxlcf1wBa24/S0acUc7VaP2T9CM2NTEwM8vSG6EcAKxAhDzATEvEPMDsQQQi8JV1NfXM9nX72eBYQZY8gAGNLJ4KFI4gZIaMoYZBgD+w7hdph5NkAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"lifecycle\" title=\"\" src=\"/static/0e6c74a4423d60c0741d527c631aa8f1/b3c1d/lifecycle.png\" srcset=\"/static/0e6c74a4423d60c0741d527c631aa8f1/04472/lifecycle.png 170w,\n/static/0e6c74a4423d60c0741d527c631aa8f1/9f933/lifecycle.png 340w,\n/static/0e6c74a4423d60c0741d527c631aa8f1/b3c1d/lifecycle.png 454w\" sizes=\"(max-width: 454px) 100vw, 454px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>A deprecation means a version of the API has been removed, and you need to verify in your manifests and resources that you're using the correct version of the API. In some cases, you may need to change your resource fields to update the new scheme for the resource. If an API is available in multiple versions at the same time, the Kubernetes API can silently upgrade some of them for you. However, you should still make sure you have the correct resource scheme‚Ää‚Äî‚Ääespecially since as alpha APIs mature, the scheme may change between versions.</p>\n<p>You can check the <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/\" target=\"_blank\" rel=\"noopener noreferrer\">K8s API Overview here</a>, for instance, deployments belong to the app group and have a <code class=\"language-text\">v1</code> version. You can list them with the endpoint:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">/apis/apps/v1/namespaces/<span class=\"token punctuation\">{</span>namespace<span class=\"token punctuation\">}</span>/deployments</code></pre></div>\n<h3 id=\"Ô∏è-deprecating-and-removing-kubernetes-apis\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-deprecating-and-removing-kubernetes-apis\" aria-label=\"Ô∏è deprecating and removing kubernetes apis permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ôªÔ∏è Deprecating and Removing Kubernetes APIs</h3>\n<p>If you're running an outdated version of the Kubernetes API, you're putting your application at risk of substantial downtime. Even if an upgrade doesn't result in an outage, subtle differences in Kubernetes APIs can cause frustration and wasted effort investigating underlying problems.</p>\n<p>In this context, deprecation implies identifying an API component for eventual removal. While it functions currently, it is scheduled to be eliminated in an upcoming version. Kubernetes follows a well-defined deprecation policy that informs users about APIs that are slated for removal or modification.</p>\n<p>The Kubernetes API serves as the interface to interact with a Kubernetes cluster, allowing users to query and manipulate various Kubernetes objects like pods, namespaces, and deployments. These APIs can be accessed through tools such as <code class=\"language-text\">kubectl</code>, via the REST API directly, or by using client libraries. As Kubernetes evolves, older APIs are marked as deprecated and eventually phased out. This underscores the importance for users or maintainers to be aware of deprecated Kubernetes APIs.</p>\n<h3 id=\"the-concerns-of-deprecated-kubernetes-apis\" style=\"position:relative;\"><a href=\"#the-concerns-of-deprecated-kubernetes-apis\" aria-label=\"the concerns of deprecated kubernetes apis permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>The Concerns of Deprecated Kubernetes APIs</h3>\n<p>When configuring applications in Kubernetes, users specify the API version of the employed Kubernetes object, a critical aspect indicated by the <code class=\"language-text\">apiVersion</code> field in YAML manifests or Helm charts. This emphasizes the need for users and maintainers to stay informed about deprecated Kubernetes API versions and their scheduled removal in upcoming releases.</p>\n<p>During a Kubernetes cluster upgrade, encountering deprecated APIs becomes a potential issue, especially if the upgraded version no longer supports them. For instance, if resources in your cluster use an outdated API version, your application relying on that resource may cease to function due to the elimination of the deprecated API in the new cluster version. This scenario can lead to significant downtime, exemplified by Reddit's sitewide outage.</p>\n<p>An illustrative case is the removal of <code class=\"language-text\">APIVersion extensions/v1beta1</code> of the Ingress Resource in Kubernetes version <code class=\"language-text\">v1.22</code>. Attempting to utilize this removed API version in your configuration would result in an error message:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">Error: UPGRADE FAILED: current release manifest contains removed kubernetes api<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> this kubernetes version and it is therefore unable to build the kubernetes objects <span class=\"token keyword\">for</span> performing the diff. error from kubernetes: unable to recognize <span class=\"token string\">\"\"</span><span class=\"token builtin class-name\">:</span> no matches <span class=\"token keyword\">for</span> kind <span class=\"token string\">\"Ingress\"</span> <span class=\"token keyword\">in</span> version <span class=\"token string\">\"extensions/v1beta1\"</span></code></pre></div>\n<h3 id=\"how-k8s-apis-are-used\" style=\"position:relative;\"><a href=\"#how-k8s-apis-are-used\" aria-label=\"how k8s apis are used permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How K8s APIs are Used</h3>\n<p>To specify a particular <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide\" target=\"_blank\" rel=\"noopener noreferrer\">API version</a> in your configuration, refer to the sample below, sourced from <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes documentation</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1     <span class=\"token comment\"># API Version of the Kubernetes object</span>\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx</code></pre></div>\n<p>You can review all supported API groups and their versions through the official documentation or by using the <code class=\"language-text\">kubectl</code> command-line tool's <code class=\"language-text\">api-versions</code> command:</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">kubectl api-versions\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1</code></pre></div>\n<h3 id=\"challenges-in-identifying-deprecated-apis\" style=\"position:relative;\"><a href=\"#challenges-in-identifying-deprecated-apis\" aria-label=\"challenges in identifying deprecated apis permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Challenges in Identifying Deprecated APIs</h3>\n<p>Identifying the resources in your cluster that utilize deprecated APIs can be quite challenging. Additionally, Kubernetes follows a stringent API versioning protocol, resulting in multiple deprecations of <code class=\"language-text\">v1beta1</code> and <code class=\"language-text\">v2beta1</code> APIs across several releases. Their policy states that Beta API versions are mandated to receive support for a minimum of 9 months or 3 releases (whichever is longer) after deprecation, after which they may be subject to removal.</p>\n<p>In cases where APIs that have been deprecated are still actively employed by workloads, tools, or other components interfacing with the cluster, disruptions may occur. Hence, users and administrators must conduct a thorough assessment of their cluster to identify any APIs in use slated for removal and subsequently migrate the affected components to leverage the appropriate new API version.</p>\n<h3 id=\"-tools-for-managing-deprecated-kubernetes-apis\" style=\"position:relative;\"><a href=\"#-tools-for-managing-deprecated-kubernetes-apis\" aria-label=\" tools for managing deprecated kubernetes apis permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üóÉ Tools for Managing Deprecated Kubernetes APIs</h3>\n<p>To tackle the issue of handling obsolete Kubernetes APIs, several tools can be employed:</p>\n<h4 id=\"tool-1-pluto-by-fairwindsopsautomated-detection-and-github-integration\" style=\"position:relative;\"><a href=\"#tool-1-pluto-by-fairwindsopsautomated-detection-and-github-integration\" aria-label=\"tool 1 pluto by fairwindsopsautomated detection and github integration permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Tool 1: Pluto by FairwindsOps‚Ää‚Äî‚ÄäAutomated Detection and GitHub Integration</h4>\n<p>FairwindsOps presents <a href=\"https://github.com/FairwindsOps/pluto\" target=\"_blank\" rel=\"noopener noreferrer\">Pluto</a>, an automated solution for detecting deprecated Kubernetes APIs within code repositories and Helm releases. With seamless GitHub workflow integration, Pluto ensures continuous monitoring, enabling timely identification of deprecated APIs and proactive management.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 41.76470588235294%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB9UlEQVR42m1STWsTQRjeRhvr5xKTnZkNSZadmbXtWqqyhuzM7AcNRgVRbKSBgKgUzEWoNw8iqPgDejAiFMRAD9qzFEWrolepVVTwFwie/AUSZ2Yb6MEX3vl+n4+XMYwsxtQQGMG4mjFiDz3Ekuzqbs7YEbgQmKQcV7Pdwq5RrYycXqdpurtSCfdO1WLb9/28uqGIP6M2P61Jgp4mofTsHjW7iF+mSKyMCGrmuYIx0XZ8f0HXGgSxpwSygcx1DPmjbYXPCeDMLbNFD4n2NkkfQzZDEN+Q+WcS8rlSKYmnYfCqVaXrlUIwwLgF5EOxiW0xr4ts8Y5YjMvCFQUowe4QO+plxGLDsVIkFV4iUDxWdl0Uvf2y6G4Nl0/+fnCx9d00xX2l5r1jsykNCPmAlJMLFIV9UuYMa0B+PQPkbyiKLEXuIb4sm7FvthZ9/Pvi9tfh2vzw5dKZT6aZ9g1pdUvmKgXsprS8aVnpAc+OVyUww1ajTSD/gFF4VTr5JZUDF7G6VPjjqJNMW2b9Xic5//NJN/pcr574BotzTcX8Wtq5gmF4ywWNWa1G9Q80oO4n5Nck4Q0XRKcmi/ygPgOse8QSUS8IxvdPHOvk8mxJ9TP7BoCv1UqxveP7jBn/j9E5zBt5H5usSQ6FHb/ImjOHGy2vwLqOyY//A4JRbr2nISMuAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"pluto\" title=\"\" src=\"/static/4bc7f03c9d5b0a542de3f3414a9618aa/c5bb3/pluto.png\" srcset=\"/static/4bc7f03c9d5b0a542de3f3414a9618aa/04472/pluto.png 170w,\n/static/4bc7f03c9d5b0a542de3f3414a9618aa/9f933/pluto.png 340w,\n/static/4bc7f03c9d5b0a542de3f3414a9618aa/c5bb3/pluto.png 680w,\n/static/4bc7f03c9d5b0a542de3f3414a9618aa/076ca/pluto.png 914w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h4 id=\"tool-2-kube-no-trouble-kubent-by-doitintlcomprehensive-cluster-wide-checks\" style=\"position:relative;\"><a href=\"#tool-2-kube-no-trouble-kubent-by-doitintlcomprehensive-cluster-wide-checks\" aria-label=\"tool 2 kube no trouble kubent by doitintlcomprehensive cluster wide checks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Tool 2: Kube No Trouble (kubent) by doitintl‚Ää‚Äî‚ÄäComprehensive Cluster-Wide Checks</h4>\n<p>Developed by <a href=\"https://www.doit.com/\" target=\"_blank\" rel=\"noopener noreferrer\">doitintl</a>, <a href=\"https://github.com/doitintl/kube-no-trouble\" target=\"_blank\" rel=\"noopener noreferrer\">Kube No Trouble (kubent)</a> specializes in thorough cluster-wide checks for deprecated APIs, focusing on deployments for detection. This tool requires the storage of original manifests, offering a comprehensive solution to identify and address deprecated APIs across Kubernetes clusters.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 600px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABQUlEQVR42mNgQAL/GRgYYZiBUgAz5Lm4qvkHGS0hLCoYEfR/RkKGMUMM0+P+LK2a91FaxQImDnft//+M9fX/mUA0Q309U2joKmYI/s8MFkMHVxi02J5Lq5q/k1L0fSOp4vZZTFEcJmec9pTLIPeZKAPQAPWk67z6+fcFoFLYXfpWStnqrZRSxBtJRdf3UmrOQANdX0sqBzyX0gwDupBJL/dtiWH2s1bj7KdGhnmvMwxyXhcbZjwONsx5FmSQ8zTSIOtxlV7Oc0VkAy0/SCmHgw2SVnb8IKXoAjLwlbR6KNBANu38N0FAw3L1c56UGuQ8qTfIeV5hmPO0Wz/7WTXQogyD7KeVetmv1HB4WQXDyyCgVX+FzaLwPycozFRyt7GDaHD4AQEoLO3t/7OQFimoMUtUTBNINihJhhGTjcNQShM2AM41kAxoTZ4xAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kubent\" title=\"\" src=\"/static/40efcf9efca5ec4946f4469751e6d8c5/0a47e/kubent.png\" srcset=\"/static/40efcf9efca5ec4946f4469751e6d8c5/04472/kubent.png 170w,\n/static/40efcf9efca5ec4946f4469751e6d8c5/9f933/kubent.png 340w,\n/static/40efcf9efca5ec4946f4469751e6d8c5/0a47e/kubent.png 600w\" sizes=\"(max-width: 600px) 100vw, 600px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h4 id=\"tool-3-helm-mapkubeapis-pluginchart-centric-api-identification\" style=\"position:relative;\"><a href=\"#tool-3-helm-mapkubeapis-pluginchart-centric-api-identification\" aria-label=\"tool 3 helm mapkubeapis pluginchart centric api identification permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Tool 3: Helm MapkubeAPIs Plugin‚Ää‚Äî‚ÄäChart-Centric API Identification</h4>\n<p>The <a href=\"https://github.com/helm/helm-mapkubeapis\" target=\"_blank\" rel=\"noopener noreferrer\">Helm MapkubeAPIs Plugin</a> is a valuable asset for identifying deprecated APIs within Helm charts installed on your cluster. This plugin provides a targeted approach to managing API deprecation, ensuring compatibility and smooth transitions during upgrades.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABkklEQVR42qVR207CQBDl/z/A2x+okJgQE99MFAQlKJbiHcFLBITubru9d9vj7KIBgvHFJmfnsjNnz0xL+MdXFOt+SR+elPA8CSEEhOtiNBrjYzTCdDpDGEZUXPxKmOcF7h9msOwJlMoXhB9EcN3toWvfGP+03kD18Aj1RhOvb+/fCoolNXP/ecCxV+mhcnCL/oAtCB8e+zhvtQ3B8UkN7asO2VM0L1qonTVxUjtDEARrhMOhwO6+jd1yDy+vYkGoVWh19cY5LjsWLFLbsbqGWKu9ub2Dv0S4OrIDu/e5OnKcJKQgpH3FCGhnLu1T+qHx9Z2UARgXmDnM2DTL/v4pmmA6Y3AYBxeuaXIIrusRmW9ynHyPfEFW40eRHn+ZuKTomDIBJrz5GARJauMkNXFIltODcZqavIbuV8SSERRFUarApc6ruUL9hIpCqDgiRirJUqjQR6EyE8eei5xyeZog9eX6LhXV0IqMwu1ngR3CZp9h49HBVp9jZ+Bi84kZaF/XmLqBMPc/8RyceiXK4ze0JlV8AdbP9IBSf2aTAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MapkubeAPIs\" title=\"\" src=\"/static/c56b695606ed60227fd376365585e16e/c5bb3/MapkubeAPIs.png\" srcset=\"/static/c56b695606ed60227fd376365585e16e/04472/MapkubeAPIs.png 170w,\n/static/c56b695606ed60227fd376365585e16e/9f933/MapkubeAPIs.png 340w,\n/static/c56b695606ed60227fd376365585e16e/c5bb3/MapkubeAPIs.png 680w,\n/static/c56b695606ed60227fd376365585e16e/b12f7/MapkubeAPIs.png 1020w,\n/static/c56b695606ed60227fd376365585e16e/c1b63/MapkubeAPIs.png 1200w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h4 id=\"tool-4-plural-cdversatile-api-management\" style=\"position:relative;\"><a href=\"#tool-4-plural-cdversatile-api-management\" aria-label=\"tool 4 plural cdversatile api management permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Tool 4: Plural CD‚Ää‚Äî‚ÄäVersatile API Management</h4>\n<p><a href=\"https://github.com/pluralsh/plural\" target=\"_blank\" rel=\"noopener noreferrer\">Plural CD</a> stands out as a versatile tool for the comprehensive management of deprecated Kubernetes APIs. Its multifaceted capabilities contribute to a smoother transition during Kubernetes upgrades, making it an essential component for identifying and handling deprecated APIs effectively.</p>\n<p>These tools collectively help users to proactively identify and address deprecated APIs, minimizing potential issues during Kubernetes upgrades. By seamlessly incorporating these tools into your workflow, you can ensure a smooth transition to newer API versions, enhancing the overall stability and reliability of your Kubernetes infrastructure.</p>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Conclusion</h2>\n<p>The Kubernetes API is designed to be flexible and change often, which is one of its core strengths. Users must be aware of which groups and versions their resources are using to ensure compatibility with the current Kubernetes API. Resources can often be modified and stored as newer resources without user action, allowing for gradual scheme changes and more confidence in API upgrades.</p>\n<p>It is important to safely migrate resources from one version to another by verifying resources statically with a tool or automatically converting resources with a conversion webhook. Adding tests early will help give confidence in using Kubernetes long term.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">LinkedIn</a></strong></p>\n<p><strong>‚ôªÔ∏è <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">X/Twitter</a></strong></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"},"nextThought":{"frontmatter":{"path":"/blog/scaling-kubernetes-istio-metrics-hpa/","title":"Scaling Kubernetes Workloads with Istio Metrics and the Horizontal Pod Autoscaler","date":"2024-10-25 18:34:00"},"excerpt":"Autoscaling for Istio-Powered Kubernetes Applications üìö Introduction The need for effective autoscaling is very important in Kubernetes‚Ä¶","html":"<blockquote>\n<p><strong>Autoscaling for Istio-Powered Kubernetes Applications</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>The need for effective autoscaling is very important in Kubernetes. The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a great tool that allows you to automatically scale your application deployments based on various metrics.</p>\n<p>But what if you're using <a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a>, the popular service mesh for Kubernetes? Can you leverage Istio's rich set of metrics to drive your autoscaling decisions?</p>\n<p>The answer is yes üéâ, and in this blog post, we'll explore how to configure the HPA to scale your workloads based on Istio metrics.</p>\n<h2 id=\"-istio-and-service-mesh\" style=\"position:relative;\"><a href=\"#-istio-and-service-mesh\" aria-label=\" istio and service mesh permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Istio and Service Mesh</h2>\n<p><a href=\"https://istio.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Istio</a> is a popular open-source service mesh framework that provides a comprehensive solution for managing, securing, and observing microservices-based applications running on Kubernetes. At its core, Istio acts as a transparent layer that intercepts all network traffic between services, allowing it to apply various policies and perform advanced traffic management tasks.</p>\n<p>One of Istio's key capabilities is its rich telemetry and observability features. Istio automatically collects a wide range of metrics, logs, and traces for all the traffic flowing through the service mesh.</p>\n<p>This telemetry data can be used for various purposes, including monitoring the health and performance of individual services, identifying performance bottlenecks, and troubleshooting issues.</p>\n<h2 id=\"-horizontal-pod-autoscaler-hpa\" style=\"position:relative;\"><a href=\"#-horizontal-pod-autoscaler-hpa\" aria-label=\" horizontal pod autoscaler hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìà Horizontal Pod Autoscaler (HPA)</h2>\n<p>The <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes Horizontal Pod Autoscaler (HPA)</a> is a built-in feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. The HPA periodically checks the target metric, and if the value exceeds or falls below the specified thresholds, it will scale the number of pods up or down accordingly.</p>\n<p>When running Istio-powered applications on Kubernetes, the HPA can be configured to scale based on the rich metrics provided by Istio. For example, the HPA can monitor the incoming HTTP traffic to a service and scale the number of pods to handle the load.</p>\n<p>This allows for dynamic and efficient scaling of Istio-based microservices, ensuring that the application can handle fluctuations in traffic without over-provisioning resources.</p>\n<h2 id=\"-istio-metrics-and-the-hpa\" style=\"position:relative;\"><a href=\"#-istio-metrics-and-the-hpa\" aria-label=\" istio metrics and the hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìä Istio Metrics and the HPA</h2>\n<p>To integrate autoscaling into an Istio-powered Kubernetes application, you need to configure the Kubernetes Horizontal Pod Autoscaler (HPA) to monitor the metrics provided by Istio.</p>\n<p>Istio generates detailed metrics about the application's traffic, which can then be used as the basis for scaling decisions. One solution for using Istio metrics with the HPA is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, a general-purpose metrics adapter developed by Zalando.</p>\n<p>The Kube Metrics Adapter can collect and serve custom and external metrics, including Prometheus metrics generated by Istio, for the HPA to use. The Kube Metrics Adapter works by discovering HPA resources in the cluster and then collecting the requested metrics, storing them in memory. It supports configuring the metric collection via annotations on the HPA object.</p>\n<p>This allows you to specify the Prometheus queries to use for retrieving the relevant Istio metrics, such as requests per second. Istio's built-in monitoring capabilities are a key advantage of using a service mesh. Istio automatically collects valuable metrics like HTTP request rates, response status codes, and request durations directly from the Envoy sidecar proxies, without requiring any changes to your application code.</p>\n<p>This rich telemetry data provides deep visibility into your microservices' performance and behavior. Beyond monitoring, these Istio-generated metrics can be leveraged to drive advanced operational capabilities, such as autoscaling and canary deployments, without additional instrumentation.</p>\n<p>I found a very clear diagram in a <a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\" target=\"_blank\" rel=\"noopener noreferrer\">Medium article by Stefan Prodan</a>:</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 51.17647058823529%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhUlEQVR42oWSy07zMBCF+/4bnoM3YMfiZwMSVEVQQaGF5uI0SRPnYse5ODn/2DQhXQAjWZGOZr45OfYCtgbcb0rsfAHmOUjSFF2nSe/x4kg8bgU89wOMBWjbzuobV+Jhk6MsUiRJiqZpLGkxAlc0tPUKMN9BHB8JaAY1DQo87Uq4zgd830dtBzXePIHlhiM6MARBgLqu50DDHND3/Qk0L6PrSR+ob6x5/6hPDq+XGe7XIdz9Fow2tqfGy90rLlZ3cN/e8bReoxTC6kuK6N8qQxx65JxRFO25Q3ZsEKcKUpRQStntpnayxDrnyCmnlHNora0eZw3N1GjqClJWk7748mfst9CdQlVVdtsIHIocmqcQpJucRv2nssCOAI7LcHXLEYRHuk0fSkrbUCzvcLy9gRtFcPb7KXwT2SzOc6BxxLMcz581hDQu5Szsnn6nOz2j80v50aF5Q4LCHvrGOjBHKfNV4HkFFhU4HAI64fTefgWaQL8h36dtaoRJhT0rkfEEURz/CfwPrUAFezQgX+8AAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"istio hpa\" title=\"\" src=\"/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png\" srcset=\"/static/2688b10269743da00b07b1f0e9204486/04472/istio-hpa.png 170w,\n/static/2688b10269743da00b07b1f0e9204486/9f933/istio-hpa.png 340w,\n/static/2688b10269743da00b07b1f0e9204486/c5bb3/istio-hpa.png 680w,\n/static/2688b10269743da00b07b1f0e9204486/b12f7/istio-hpa.png 1020w,\n/static/2688b10269743da00b07b1f0e9204486/b5a09/istio-hpa.png 1360w,\n/static/2688b10269743da00b07b1f0e9204486/29007/istio-hpa.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/google-cloud/kubernetes-autoscaling-with-istio-metrics-76442253a45a\">source</a></div>\n<h2 id=\"Ô∏è-configuring-hpa-with-istio-metrics\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-configuring-hpa-with-istio-metrics\" aria-label=\"Ô∏è configuring hpa with istio metrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚öôÔ∏è Configuring HPA with Istio Metrics</h2>\n<p>To configure the Kubernetes Horizontal Pod Autoscaler (HPA) to use metrics provided by Istio, follow these steps:</p>\n<ol>\n<li><strong>Enable Telemetry and Prometheus</strong>: Ensure that when installing Istio, the telemetry service and Prometheus are both enabled.</li>\n<li><strong>Install Metrics Adapter</strong>: You'll need a metrics adapter that can query Prometheus and make the Istio metrics available to the HPA. One such adapter is the <a href=\"https://github.com/zalando-incubator/kube-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Kube Metrics Adapter</a>, developed by Zalando.</li>\n<li><strong>Adapter Functionality</strong>: The Kube Metrics Adapter scans the HPA objects in your cluster, executes the specified Prometheus queries (configured via annotations), and stores the metrics in memory. This allows the HPA to use the rich telemetry data provided by Istio, such as HTTP request rates, as the basis for scaling your application's pods up or down as needed.</li>\n</ol>\n<h3 id=\"Ô∏è-installing-the-custom-metrics-adapter\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-installing-the-custom-metrics-adapter\" aria-label=\"Ô∏è installing the custom metrics adapter permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Installing the Custom Metrics Adapter</h3>\n<p>For our solution, we will need <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/values.yaml#L167\" target=\"_blank\" rel=\"noopener noreferrer\">telemetry to be enabled in Istio</a> and Prometheus to scrape metrics from Istio:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo <span class=\"token function\">add</span> stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo update\nhelm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus prometheus-community/prometheus</code></pre></div>\n<p>It is the default Helm chart for the Prometheus installation. We use this one because Istio has a default configuration to expose metrics for it, i.e., the pod has <a href=\"https://github.com/istio/istio/blob/master/manifests/charts/istio-control/istio-discovery/files/waypoint.yaml#L56\" target=\"_blank\" rel=\"noopener noreferrer\">the following annotations</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus.io/path</span><span class=\"token punctuation\">:</span> /stats/prometheus\n<span class=\"token key atrule\">prometheus.io/port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">15020</span>\n<span class=\"token key atrule\">prometheus.io/scrape</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span></code></pre></div>\n<p>Simply having Prometheus installed in your Kubernetes cluster does not automatically make its metrics available for use with the Horizontal Pod Autoscaler (HPA). To leverage Istio's metrics for autoscaling, you'll need to set up the Prometheus Adapter. The Prometheus Adapter is a component that translates Prometheus metrics into a format that the HPA can understand and use. You'll need to provide a custom configuration file, <code class=\"language-text\">prometheus-adapter.yaml</code>, with the following settings:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">prometheus</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> http<span class=\"token punctuation\">:</span>//prometheus<span class=\"token punctuation\">-</span>server.monitoring.svc.cluster.local\n  <span class=\"token key atrule\">port</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span>\n<span class=\"token key atrule\">rules</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">custom</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">seriesQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'istio_requests_total{kubernetes_namespace!=\"\",kubernetes_pod_name!=\"\"}'</span>\n    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">overrides</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">kubernetes_namespace</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"namespace\"</span><span class=\"token punctuation\">}</span>\n        <span class=\"token key atrule\">kubernetes_pod_name</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"pod\"</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">matches</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"^(.*)_total\"</span>\n      <span class=\"token key atrule\">as</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"${1}_per_second\"</span>\n    <span class=\"token key atrule\">metricsQuery</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'sum(rate(&lt;&lt;.Series>>{&lt;&lt;.LabelMatchers>>}[2m]))'</span></code></pre></div>\n<p>This configuration tells the Prometheus Adapter to:</p>\n<ul>\n<li>Connect to the Prometheus server running in the monitoring namespace.</li>\n<li>Query the <code class=\"language-text\">istio_requests_total</code> metric, which provides the total number of requests.</li>\n<li>Convert the metric to a rate (requests per second) using a 2-minute window.</li>\n<li>Expose the metric with a name like <code class=\"language-text\">istio_requests_per_second</code>.</li>\n</ul>\n<p>With the Prometheus Adapter configured and deployed, the HPA will now be able to use the Istio-generated metrics to scale your application's pods as needed. Here, we can see our Prometheus instance URL, port, and one custom rule. Let's focus on this rule:</p>\n<ul>\n<li><strong>seriesQuery</strong>: Needed for metric discovery.</li>\n<li><strong>resources/overrides</strong>: Mapping fields from the metric (<code class=\"language-text\">kubernetes_namespace</code>, <code class=\"language-text\">kubernetes_pod_name</code>) to the names required by Kubernetes (<code class=\"language-text\">namespace</code>, <code class=\"language-text\">pod</code>).</li>\n<li><strong>name/matches, name/as</strong>: Needed to change the metric name. We are transforming this metric, so it is good to change the name <code class=\"language-text\">istio_requests_total</code> to <code class=\"language-text\">istio_requests_per_second</code>.</li>\n<li><strong>metricsQuery</strong>: The actual query (which is actually a query template) and it will be run by the adapter while scraping the metric from Prometheus. <code class=\"language-text\">rate</code> and <code class=\"language-text\">[2m]</code> \"calculates the per-second average rate of increase of the time series in the range vector\" (from Prometheus documentation), here it is the per-second rate of HTTP requests as measured over the last 2 minutes, per time series in the range vector (also, almost from the Prometheus documentation).</li>\n</ul>\n<p>Now, as we have the adapter configuration, we can deploy it using:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token parameter variable\">-n</span> monitoring <span class=\"token function\">install</span> prometheus-adapter prometheus-community/prometheus-adapter <span class=\"token parameter variable\">-f</span> prometheus-adapter.yaml</code></pre></div>\n<h3 id=\"-installing-the-test-app\" style=\"position:relative;\"><a href=\"#-installing-the-test-app\" aria-label=\" installing the test app permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üß™ Installing the Test App</h3>\n<p>You will use <a href=\"https://github.com/stefanprodan/podinfo\" target=\"_blank\" rel=\"noopener noreferrer\">podinfo</a> to test the HPA.</p>\n<p>First, create a test namespace with Istio sidecar injection enabled:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/test.yaml</code></pre></div>\n<p>Create the deployment and ClusterIP service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/deployment.yaml,./demo-app/service.yaml</code></pre></div>\n<p>In order to trigger the autoscaling, you'll need a tool to generate traffic. Deploy the load test service in the test namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./loadtest/</code></pre></div>\n<p>Verify the install by calling the app API. Exec into the load tester pod and use <code class=\"language-text\">hey</code> to generate load for a couple of seconds:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">loadtester</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get pod <span class=\"token parameter variable\">-l</span> <span class=\"token string\">\"app=loadtester\"</span> <span class=\"token parameter variable\">-o</span> <span class=\"token assign-left variable\">jsonpath</span><span class=\"token operator\">=</span><span class=\"token string\">'{.items[0].metadata.name}'</span><span class=\"token variable\">)</span></span>\nkubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n\n$ hey <span class=\"token parameter variable\">-z</span> 5s <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898\n\nSummary:\n  Total:\t<span class=\"token number\">5.0138</span> secs\n  Requests/sec:\t<span class=\"token number\">19.9451</span>\n\nStatus code distribution:\n  <span class=\"token punctuation\">[</span><span class=\"token number\">200</span><span class=\"token punctuation\">]</span>\t<span class=\"token number\">100</span> responses\n$ <span class=\"token builtin class-name\">exit</span></code></pre></div>\n<p>The app ClusterIP service exposes port 9898 under the <code class=\"language-text\">http</code> name. When using the <code class=\"language-text\">http</code> prefix, the Envoy sidecar will switch to L7 routing and the telemetry service will collect HTTP metrics.</p>\n<h3 id=\"querying-the-istiometrics\" style=\"position:relative;\"><a href=\"#querying-the-istiometrics\" aria-label=\"querying the istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Querying the Istio¬†metrics</h3>\n<p>The Istio telemetry service collects metrics from the mesh and stores them in Prometheus. One such metric is istio_requests_total, with it you can determine the rate of requests per second a workload receives.\nThis is how you can query Prometheus for the req/sec rate received by podinfo in the last minute, excluding 404s:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"<span class=\"token punctuation\">,</span>\n        reporter=\"destination\"<span class=\"token punctuation\">,</span>\n        response_code<span class=\"token tag\">!=</span>\"404\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  )</code></pre></div>\n<p>The HPA needs to know the req/sec that each pod receives. You can use the container memory usage metric from kubelet to count the number of pods and calculate the Istio request rate per pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">sum(\n    rate(\n      istio_requests_total<span class=\"token punctuation\">{</span>\n        destination_workload=\"podinfo\"<span class=\"token punctuation\">,</span>\n        destination_workload_namespace=\"test\"\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">[</span>1m<span class=\"token punctuation\">]</span>\n    )\n  ) /\n  count(\n    count(\n      container_memory_usage_bytes<span class=\"token punctuation\">{</span>\n        namespace=\"test\"<span class=\"token punctuation\">,</span>\n        pod=~\"podinfo.<span class=\"token important\">*\"</span>\n      <span class=\"token punctuation\">}</span>\n    ) by (pod)\n  )</code></pre></div>\n<h3 id=\"configuring-the-hpa-with-istiometrics\" style=\"position:relative;\"><a href=\"#configuring-the-hpa-with-istiometrics\" aria-label=\"configuring the hpa with istiometrics permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Configuring the HPA with Istio¬†metrics</h3>\n<p>Using the req/sec query you can define a HPA that will scale the podinfo workload based on the number of requests per second that each instance receives:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> autoscaling/v2\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> HorizontalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">namespace</span><span class=\"token punctuation\">:</span> test\n  <span class=\"token key atrule\">annotations</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/per-replica</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"true\"</span>\n    <span class=\"token key atrule\">metric-config.object.istio-requests-total.prometheus/query</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n      sum(\n        rate(\n          istio_requests_total{\n            destination_workload=\"podinfo\",\n            destination_workload_namespace=\"test\"\n          }[1m]\n        )\n      ) /\n      count(\n        count(\n          container_memory_usage_bytes{\n            namespace=\"test\",\n            pod=~\"podinfo.*\"\n          }\n        ) by (pod)\n      )</span>\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">maxReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n  <span class=\"token key atrule\">minReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n  <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n    <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n  <span class=\"token key atrule\">metrics</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Object\n      <span class=\"token key atrule\">object</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metricName</span><span class=\"token punctuation\">:</span> istio<span class=\"token punctuation\">-</span>requests<span class=\"token punctuation\">-</span>total\n        <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n          <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n          <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> podinfo\n        <span class=\"token key atrule\">targetValue</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<p>The above configuration will instruct the Horizontal Pod Autoscaler to scale up the deployment when the average traffic load goes over 10 req/sec per replica.</p>\n<p>Create the HPA with:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> ./demo-app/hpa.yaml</code></pre></div>\n<p>Start a load test and verify that the adapter computes the metric:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> kube-system logs deployment/kube-metrics-adapter <span class=\"token parameter variable\">-f</span>\nCollected <span class=\"token number\">1</span> new metric<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\nCollected new custom metric <span class=\"token string\">'istio-requests-total'</span> <span class=\"token punctuation\">(</span>44m<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> Pod test/podinfo</code></pre></div>\n<p>List the custom metrics resources:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get <span class=\"token parameter variable\">--raw</span> <span class=\"token string\">\"/apis/custom.metrics.k8s.io/v1beta1\"</span> <span class=\"token operator\">|</span> jq <span class=\"token builtin class-name\">.</span>\nThe Kubernetes API should <span class=\"token builtin class-name\">return</span> a resource list containing the Istio metric:\n<span class=\"token punctuation\">{</span>\n  <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"APIResourceList\"</span>,\n  <span class=\"token string\">\"apiVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"v1\"</span>,\n  <span class=\"token string\">\"groupVersion\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"custom.metrics.k8s.io/v1beta1\"</span>,\n  <span class=\"token string\">\"resources\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token string\">\"name\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"pods/istio-requests-total\"</span>,\n      <span class=\"token string\">\"singularName\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"\"</span>,\n      <span class=\"token string\">\"namespaced\"</span><span class=\"token builtin class-name\">:</span> true,\n      <span class=\"token string\">\"kind\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"MetricValueList\"</span>,\n      <span class=\"token string\">\"verbs\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token string\">\"get\"</span>\n      <span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>After a couple of seconds the HPA will fetch the metric from the adapter:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   44m/10    <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">1</span></code></pre></div>\n<h3 id=\"autoscaling-based-on-http-traffic\" style=\"position:relative;\"><a href=\"#autoscaling-based-on-http-traffic\" aria-label=\"autoscaling based on http traffic permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Autoscaling Based on HTTP Traffic</h3>\n<p>To test the HPA, you can use the load tester to trigger a scale-up event. You can use other tools besides <code class=\"language-text\">hey</code> (e.g., <code class=\"language-text\">siege</code>). It is important that the tool supports HTTP/1.1, so <code class=\"language-text\">ab</code> (Apache Benchmark) is not the right solution.</p>\n<p>Exec into the tester pod and use <code class=\"language-text\">hey</code> to generate load for 5 minutes:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-it</span> <span class=\"token variable\">${loadtester}</span> -- <span class=\"token function\">sh</span>\n$ hey <span class=\"token parameter variable\">-z</span> 5m <span class=\"token parameter variable\">-c</span> <span class=\"token number\">10</span> <span class=\"token parameter variable\">-q</span> <span class=\"token number\">2</span> http://podinfo.test:9898</code></pre></div>\n<p>Press <code class=\"language-text\">Ctrl+C</code> then <code class=\"language-text\">exit</code> to get out of the load test terminal if you want to stop prematurely. After a minute, the HPA will start to scale up the workload until the requests per second per pod drop under the target value:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">watch</span> kubectl <span class=\"token parameter variable\">-n</span> <span class=\"token builtin class-name\">test</span> get hpa/podinfo\nNAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS\npodinfo   Deployment/podinfo   25272m/10   <span class=\"token number\">1</span>         <span class=\"token number\">10</span>        <span class=\"token number\">3</span></code></pre></div>\n<p>When the load test finishes, the number of requests per second will drop to zero, and the HPA will start to scale down the workload. Note that the HPA has a back-off mechanism that prevents rapid scale-up/down events; the number of replicas will go back to one after a couple of minutes.</p>\n<p>By default, the metrics sync happens once every 30 seconds, and scaling up/down can only happen if there was no rescaling within the last 3‚Äì5 minutes.</p>\n<p>In this way, the HPA prevents rapid execution of conflicting decisions and gives time for the <a href=\"https://github.com/kubernetes/autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Cluster Autoscaler</a> to kick in.</p>\n<h2 id=\"-summary\" style=\"position:relative;\"><a href=\"#-summary\" aria-label=\" summary permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìã Summary</h2>\n<p>In this article, we explored the Kubernetes Horizontal Pod Autoscaler (HPA) using Custom Metrics. We set up the Prometheus Adapter to expose Istio metrics to Kubernetes' Custom Metrics API. However, the workload we tested did not respond effectively to the configured HPA parameters. A less erratic workload should exhibit more predictable behavior with the HPA settings.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}