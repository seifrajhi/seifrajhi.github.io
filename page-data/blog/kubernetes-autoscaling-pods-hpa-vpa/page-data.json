{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/kubernetes-autoscaling-pods-hpa-vpa/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>How to Scale Pods Vertically and Horizontally in Kubernetes üåê</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üê≥ Introduction</h2>\n<p><a href=\"https://kubernetes.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Kubernetes</a> embodies resilience and scalability by deploying diverse pods with varying resource allocations, ensuring application redundancy. While manual adjustments suffice, Kubernetes elevates scaling with Horizontal Pod Autoscaling (HPA). This self-regulating loop dynamically expands or contracts resources (app Pods) based on real-time demands. Simply deploy a <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\" target=\"_blank\" rel=\"noopener noreferrer\">HorizontalPodAutoscaler (HPA)</a> resource for auto-scaling, leaving the automation to it.</p>\n<p>Moreover, in addition to HPA, the <a href=\"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">Vertical Pod Autoscaler (VPA)</a> offers a distinct approach. Unlike HPA's horizontal scaling, VPA modifies a Pod's resources.requests, prompting the Kubernetes Scheduler to shift Pods across WorkerNodes as needed. VPA incessantly monitors container resource usage, automatically tweaking requests to prevent waste and ensure sufficient CPU and memory allocation. This synergy of HPA and VPA empowers Kubernetes users to effortlessly achieve efficient, tailored scaling.</p>\n<p>This blog explores the universe of Kubernetes autoscaling, casting a spotlight on these two powerful tools: HPA and VPA. We'll uncover how they work, their distinctions, and how they can collaborate to enhance resource utilization and application performance.</p>\n<p>Join us on a quest to become proficient in the art of scaling within Kubernetes.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 75.29411764705883%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAQX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAAB5VUzGVGA/wD/xAAZEAACAwEAAAAAAAAAAAAAAAAAAQIREjH/2gAIAQEAAQUCXWyxSibVaR//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAZEAACAwEAAAAAAAAAAAAAAAAAEQEQIjH/2gAIAQEABj8CMwoR2k6//8QAHBAAAwACAwEAAAAAAAAAAAAAAAERMVFhkaHw/9oACAEBAAE/IawFNNs5npe3ZRmDkZItJnzWf//aAAwDAQACAAMAAAAQ6C//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQMBAT8QhX//xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAgEBPxCn/8QAGxABAAMBAQEBAAAAAAAAAAAAAQARITFBUXH/2gAIAQEAAT8Qvp23WkL1zilfHEFu036jLVRyhZWmwDg/ciftw//Z'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"hpa-vpa\" title=\"\" src=\"/static/063a135be697a2c366aa6efcd27d818a/7bf67/hpa-vpa.jpg\" srcset=\"/static/063a135be697a2c366aa6efcd27d818a/651be/hpa-vpa.jpg 170w,\n/static/063a135be697a2c366aa6efcd27d818a/d30a3/hpa-vpa.jpg 340w,\n/static/063a135be697a2c366aa6efcd27d818a/7bf67/hpa-vpa.jpg 680w,\n/static/063a135be697a2c366aa6efcd27d818a/990cb/hpa-vpa.jpg 1020w,\n/static/063a135be697a2c366aa6efcd27d818a/e5166/hpa-vpa.jpg 1200w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-horizontalpodautoscaler-hpa-scaling-horizontally\" style=\"position:relative;\"><a href=\"#-horizontalpodautoscaler-hpa-scaling-horizontally\" aria-label=\" horizontalpodautoscaler hpa scaling horizontally permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìà HorizontalPodAutoscaler (HPA): Scaling Horizontally</h2>\n<p>Under the hood, HPA is powered by a dedicated Kubernetes controller. You create an HPA YAML targeting your app's Deployment and use <code class=\"language-text\">kubectl</code> to apply it.</p>\n<h2 id=\"-vertical-pod-autoscaler-vpa-scaling-vertically\" style=\"position:relative;\"><a href=\"#-vertical-pod-autoscaler-vpa-scaling-vertically\" aria-label=\" vertical pod autoscaler vpa scaling vertically permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìâ Vertical Pod Autoscaler (VPA): Scaling Vertically</h2>\n<p>VPA involves three pods:</p>\n<ul>\n<li><strong>Recommender</strong>: <a href=\"https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/recommender/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">Analyzes</a> Pod resource usage and recommends CPU/memory requests.</li>\n<li><strong>Updater</strong>: <a href=\"https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/updater/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">Monitors</a> and aligns Pod requests with recommendations, recreating if needed.</li>\n<li><strong>Admission-plugin</strong>: <a href=\"https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/admission-controller/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">Sets</a> request values for new/updated Pods.</li>\n</ul>\n<h3 id=\"Ô∏è-vpa-limitations\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-vpa-limitations\" aria-label=\"Ô∏è vpa limitations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ö†Ô∏è VPA Limitations</h3>\n<ul>\n<li>VPA doesn't oversee Pod recreation post-eviction, needing tools like Cluster Autoscaler.</li>\n<li>It can't be used with HPA for CPU/memory scaling, but custom metrics are compatible.</li>\n<li>VPA's Pod recreation might cause service downtime without fault-tolerant solutions.</li>\n</ul>\n<p>HPA and VPA require a <a href=\"https://github.com/kubernetes-sigs/metrics-server\" target=\"_blank\" rel=\"noopener noreferrer\">metrics server</a>, like Kubernetes Metrics Server, to gather CPU/memory metrics for scaling decisions.</p>\n<h3 id=\"-metrics-api-types\" style=\"position:relative;\"><a href=\"#-metrics-api-types\" aria-label=\" metrics api types permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìä Metrics API Types</h3>\n<ul>\n<li><strong>metrics.k8s.io</strong>: Default metrics, provided by the <a href=\"https://github.com/kubernetes-sigs/metrics-server\" target=\"_blank\" rel=\"noopener noreferrer\">metrics-server</a>.</li>\n<li><strong>custom.metrics.k8s.io</strong>: Metrics provided by adapters from inside a cluster, e.g., <a href=\"https://github.com/Azure/azure-k8s-metrics-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Microsoft Azure Adapter</a>, <a href=\"https://github.com/GoogleCloudPlatform/k8s-stackdriver\" target=\"_blank\" rel=\"noopener noreferrer\">Google Stackdriver</a>, <a href=\"https://github.com/kubernetes-sigs/prometheus-adapter\" target=\"_blank\" rel=\"noopener noreferrer\">Prometheus Adapter</a>.</li>\n<li><strong>external.metrics.k8s.io</strong>: Similar to the Custom Metrics API, but metrics are provided by an external system, such as <a href=\"https://aws.amazon.com/cloudwatch/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS CloudWatch</a>.</li>\n</ul>\n<p>In a nutshell, <code class=\"language-text\">metrics.k8s.io</code> encompasses default metrics from the metrics-server, while <code class=\"language-text\">custom.metrics.k8s.io</code> involves internal cluster adapters like Microsoft Azure or Google Stackdriver, and <code class=\"language-text\">external.metrics.k8s.io</code> pertains to external systems like AWS CloudWatch, providing adaptable metrics.</p>\n<h2 id=\"Ô∏è-deploy-metrics-server-using-helm\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-deploy-metrics-server-using-helm\" aria-label=\"Ô∏è deploy metrics server using helm permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚òÅÔ∏è Deploy Metrics Server Using Helm</h2>\n<p>To initiate the process, integrate the metrics-server repository into your helm package collection. Employ the <code class=\"language-text\">helm repo add</code> command as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> metrics-server https://kubernetes-sigs.github.io/metrics-server</code></pre></div>\n<p>Then, employ <code class=\"language-text\">helm repo update</code> to refresh the pool of accessible packages:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ helm repo update metrics-server\n\nHang tight <span class=\"token keyword\">while</span> we grab the latest from your chart repositories<span class=\"token punctuation\">..</span>.\n<span class=\"token punctuation\">..</span>.Successfully got an update from the <span class=\"token string\">\"metrics-server\"</span> chart repository\nUpdate Complete. ‚éàHappy Helming<span class=\"token operator\">!</span>‚éà\n\n$ helm search repo metrics-server</code></pre></div>\n<h3 id=\"Ô∏è-repository-integration-complete-metrics-server-deployment\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-repository-integration-complete-metrics-server-deployment\" aria-label=\"Ô∏è repository integration complete metrics server deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>‚ò∏Ô∏è Repository Integration Complete: Metrics Server Deployment</h3>\n<p>With the repository successfully added to Helm, you're poised to include metrics-server in your Kubernetes deployments. Here's how you can create your deployment configuration:</p>\n<p>Clone the Kubernetes Starter Kit Git repository:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/K8s-Workload-Scaling-Strategies.git</code></pre></div>\n<p>Locate the metrics-server configuration in the following path:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">cd</span> K8s-Workload-Scaling-Strategies\n<span class=\"token function\">vim</span> assets/manifests/metrics-server-values-v3.11.0.yaml</code></pre></div>\n<p>By following these steps, you'll seamlessly integrate metrics-server into your Kubernetes environment and have the flexibility to tailor your deployment configuration to your needs.</p>\n<p>It contains a few stock parameters. Note that replicas is a fixed value, 2.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\">## Ref: https://github.com/kubernetes-sigs/metrics-server/blob/metrics-server-helm-chart-3.8.2/charts/metrics-server</span>\n<span class=\"token comment\"># Number of metrics-server replicas to run</span>\nreplicas: <span class=\"token number\">2</span>\n\napiService:\n    <span class=\"token comment\"># Specifies if the v1beta1.metrics.k8s.io API service should be created.</span>\n    create: <span class=\"token boolean\">true</span>\n\nhostNetwork:\n    <span class=\"token comment\"># Specifies if metrics-server should be started in hostNetwork mode.</span>\n    enabled: <span class=\"token boolean\">false</span></code></pre></div>\n<p>You can check the Metrics Server chart page for explanations of the available values for metrics-server.</p>\n<p>Then, you can install the Kubernetes Metrics Server using Helm (a dedicated metrics-server namespace will be created as well):</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token function\">install</span> metrics-server metrics-server/metrics-server <span class=\"token parameter variable\">--version</span> <span class=\"token number\">3.11</span>.0<span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--namespace</span> metrics-server <span class=\"token punctuation\">\\</span>\n    --create-namespace <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">-f</span> <span class=\"token string\">\"assets/manifests/metrics-server-v3.11.0.yaml\"</span></code></pre></div>\n<p>This will deploy metrics-server to your configured Kubernetes cluster.</p>\n<p>After deploying, you can use <code class=\"language-text\">helm ls</code> to verify that metrics-server has been added to your deployment:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm <span class=\"token function\">ls</span> <span class=\"token parameter variable\">-n</span> metrics-server</code></pre></div>\n<p>Next, you can check the status of all of the Kubernetes resources deployed to the metrics-server namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get all <span class=\"token parameter variable\">-n</span> metrics-server</code></pre></div>\n<p>Finally, check if the <code class=\"language-text\">kubectl top</code> command works (similar to Linux top command - prints current resource usage, such as CPU and memory). Below command displays current resource usage for all Pods in the kube-system namespace:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token function\">top</span> pods <span class=\"token parameter variable\">-n</span> kube-system</code></pre></div>\n<p>You have now deployed metrics-server into your Kubernetes cluster. In the next step, you'll review some of the parameters of a HorizontalPodAutoscaler Custom Resource Definition.</p>\n<h2 id=\"introducing-hpas-the-key-to-dynamic-scaling\" style=\"position:relative;\"><a href=\"#introducing-hpas-the-key-to-dynamic-scaling\" aria-label=\"introducing hpas the key to dynamic scaling permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Introducing HPAs: The Key to Dynamic Scaling</h2>\n<p>So far, you've been manually configuring the number of pods in your Kubernetes deployments. This is fine for simple cases, but it can be a pain to keep up with as your application grows and traffic fluctuates.</p>\n<p>That's where <strong>HorizontalPodAutoscalers (HPAs)</strong> come in. HPAs are a Kubernetes feature that automatically scales your deployments up or down based on metrics like CPU usage and memory utilization. This means that you can focus on building great applications, and let HPAs take care of the scaling for you.</p>\n<h3 id=\"how-an-hpa-works\" style=\"position:relative;\"><a href=\"#how-an-hpa-works\" aria-label=\"how an hpa works permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How an HPA Works</h3>\n<ol>\n<li><strong>Metric Monitoring</strong>: The HPA watches a metric, such as CPU usage.</li>\n<li><strong>Threshold Exceeded</strong>: When the metric exceeds a threshold, the HPA triggers a scale operation.</li>\n<li><strong>Scaling</strong>: The scale operation either increases or decreases the number of pods in the deployment.</li>\n</ol>\n<h3 id=\"hpa-crd-custom-resource-definition\" style=\"position:relative;\"><a href=\"#hpa-crd-custom-resource-definition\" aria-label=\"hpa crd custom resource definition permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>HPA CRD (Custom Resource Definition)</h3>\n<p>The HPA CRD is a YAML file that specifies the configuration of the HPA. The CRD includes the following fields:</p>\n<ul>\n<li><strong>Target</strong>: The name of the Kubernetes object that the HPA is monitoring.</li>\n<li><strong>Metrics</strong>: The metrics that the HPA is watching.</li>\n<li><strong>Scaling Policies</strong>: The rules for how the HPA scales the deployment.</li>\n</ul>\n<p>Here's an example of an HPA CRD:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> autoscaling/v2beta2\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> HorizontalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> demo<span class=\"token punctuation\">-</span>hpa\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n        <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> my<span class=\"token punctuation\">-</span>deployment\n    <span class=\"token key atrule\">minReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">maxReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n    <span class=\"token key atrule\">metrics</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Resource\n            <span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cpu\n                <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Utilization\n                    <span class=\"token key atrule\">averageUtilization</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span></code></pre></div>\n<h3 id=\"explanation-of-the-configuration\" style=\"position:relative;\"><a href=\"#explanation-of-the-configuration\" aria-label=\"explanation of the configuration permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Explanation of the Configuration</h3>\n<ul>\n<li><strong>spec.scaleTargetRef</strong>: Specifies the Kubernetes object that the HPA is monitoring. In this case, it is the <code class=\"language-text\">my-deployment</code> deployment.</li>\n<li><strong>spec.minReplicas</strong>: Specifies the lower limit for the number of replicas in the deployment. The HPA will never scale the deployment down below 1 pod.</li>\n<li><strong>spec.maxReplicas</strong>: Specifies the upper limit for the number of replicas in the deployment. The HPA will never scale the deployment up above 3 pods.</li>\n<li><strong>spec.metrics.type</strong>: Specifies the type of metric that the HPA is using to calculate the desired replica count. Here, it is using the <code class=\"language-text\">Resource</code> type, which means it is scaling the deployment based on the average CPU utilization.</li>\n<li><strong>spec.metrics.resource.name</strong>: Specifies the name of the resource that the HPA is monitoring. In this case, it is the <code class=\"language-text\">cpu</code> resource.</li>\n<li><strong>spec.metrics.resource.averageUtilization</strong>: Specifies the threshold value for the metric. The HPA will scale the deployment up if the average CPU utilization exceeds 80%.</li>\n</ul>\n<h3 id=\"creating-an-hpa\" style=\"position:relative;\"><a href=\"#creating-an-hpa\" aria-label=\"creating an hpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Creating an HPA</h3>\n<p>There are two ways to create an HPA for your application deployment. You can use the <code class=\"language-text\">kubectl autoscale</code> command on an existing deployment, or you can create an HPA YAML manifest.</p>\n<h4 id=\"using-code-classlanguage-textkubectl-autoscalecode-command\" style=\"position:relative;\"><a href=\"#using-code-classlanguage-textkubectl-autoscalecode-command\" aria-label=\"using code classlanguage textkubectl autoscalecode command permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Using <code class=\"language-text\">kubectl autoscale</code> Command</h4>\n<p>To create an HPA using the <code class=\"language-text\">kubectl autoscale</code> command, you need to specify the name of the deployment and the target CPU utilization. For example, the following command would create an HPA for the <code class=\"language-text\">myapp-test</code> deployment with a target CPU utilization of 80%:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> myapp<span class=\"token punctuation\">-</span>test\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> myapp<span class=\"token punctuation\">-</span>test\n    <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> myapp<span class=\"token punctuation\">-</span>test\n        <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> busybox\n                    <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> busybox\n                    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n                            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 50m\n                        <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n                            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 20m\n                    <span class=\"token key atrule\">command</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"sh\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"-c\"</span><span class=\"token punctuation\">]</span>\n                    <span class=\"token key atrule\">args</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> while <span class=\"token punctuation\">[</span> <span class=\"token number\">1</span> <span class=\"token punctuation\">]</span>; do\n                            echo \"Test\";\n                            sleep 0.01;\n                            done</code></pre></div>\n<p>Note the last few lines of this file. They contain some shell syntax to repeatedly print \"Test\" a hundred times a second, to simulate load. Once you are done reviewing the file, you can deploy it into your cluster using <code class=\"language-text\">kubectl</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> assets/manifests/hpa/metrics-server/myapp-test.yaml</code></pre></div>\n<p>Finally, create a HorizontalPodAutoscaler targeting the <code class=\"language-text\">myapp-test</code> deployment:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl autoscale deployment hpa-test --cpu-percent<span class=\"token operator\">=</span><span class=\"token number\">80</span> <span class=\"token parameter variable\">--min</span><span class=\"token operator\">=</span><span class=\"token number\">1</span> <span class=\"token parameter variable\">--max</span><span class=\"token operator\">=</span><span class=\"token number\">3</span></code></pre></div>\n<p>You can check if the HPA resource was created by running:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get hpa</code></pre></div>\n<p>The <code class=\"language-text\">TARGETS</code> column of the output will eventually show a figure of current usage%/target usage%.</p>\n<p>You can also observe the logged events that an HPA generates by using:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl describe hpa hpa-test</code></pre></div>\n<p>In a real-world scenario, you will want to use a dedicated YAML manifest to define each HPA. This way, you can track the changes by having the manifest committed in a Git repository, as well as come back to it later and perform changes.</p>\n<h3 id=\"scaling-applications-automatically-with-metrics-server\" style=\"position:relative;\"><a href=\"#scaling-applications-automatically-with-metrics-server\" aria-label=\"scaling applications automatically with metrics server permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Scaling Applications Automatically with Metrics Server</h3>\n<p>In this last step, you will experiment with two different ways of generating server load and scaling your applications automatically via a YAML manifest.</p>\n<h4 id=\"application-deployment\" style=\"position:relative;\"><a href=\"#application-deployment\" aria-label=\"application deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Application Deployment</h4>\n<p>One way to generate server load is to create an application deployment that performs some CPU-intensive computations. This will cause the pods in the deployment to use more CPU resources, which will trigger the HorizontalPodAutoscaler (HPA) to scale the deployment up.</p>\n<h4 id=\"shell-script\" style=\"position:relative;\"><a href=\"#shell-script\" aria-label=\"shell script permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Shell Script</h4>\n<p>Another way to generate server load is to use a shell script. This script can be configured to perform fast successive HTTP calls to a web application. This will cause the web application to use more CPU resources, which will also trigger the HPA to scale the deployment up.</p>\n<h4 id=\"constant-load-test\" style=\"position:relative;\"><a href=\"#constant-load-test\" aria-label=\"constant load test permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Constant Load Test</h4>\n<p>In this scenario, you will create a sample application that performs some CPU-intensive computations. This application is implemented in Python and is included in one of the example manifests from the starter kit. You can open the manifest, called <code class=\"language-text\">constant-load.yaml</code>, using nano or your favorite text editor.</p>\n<p>The manifest defines a deployment that creates a pod that runs the Python application. The application will continuously perform CPU-intensive computations, which will cause the pod to use more CPU resources. This will trigger the HorizontalPodAutoscaler (HPA) to scale the deployment up.</p>\n<p>Once you have opened the manifest, you can edit it to change the number of replicas in the deployment. The default number of replicas is 1, but you can increase this number to create more pods. The more pods you create, the more CPU resources will be used, and the more the HPA will scale the deployment up.</p>\n<p>Once you have edited the manifest, you can save it and apply it to your cluster using the <code class=\"language-text\">kubectl apply</code> command:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> assets/manifests/hpa/metrics-server/constant-load.yaml</code></pre></div>\n<p>Once the manifest has been applied, the HPA will start monitoring the CPU usage of the deployment. If the CPU usage exceeds the target utilization, the HPA will scale the deployment up to add more pods.</p>\n<p>Verify that the deployment was created successfully, and that it's up and running:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get deployments</code></pre></div>\n<p>Next, you will need to deploy another HorizontalPodAutoscaler (HPA) to this cluster. There is an example HPA that is matched to this scenario in the file <code class=\"language-text\">constant-load-hpa.yaml</code>.</p>\n<p>The <code class=\"language-text\">constant-load-hpa.yaml</code> file defines an HPA that monitors the CPU usage of the constant-load deployment. The HPA will scale the deployment up if the CPU usage exceeds the target utilization, which is set to 80% by default.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">cat</span> assets/manifests/hpa/metrics-server/constant-load-hpa.yaml</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> autoscaling/v2beta2\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> HorizontalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> constant<span class=\"token punctuation\">-</span>load\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">scaleTargetRef</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n        <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> constant<span class=\"token punctuation\">-</span>load<span class=\"token punctuation\">-</span>deployment\n    <span class=\"token key atrule\">minReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n    <span class=\"token key atrule\">maxReplicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span>\n    <span class=\"token key atrule\">metrics</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Resource\n            <span class=\"token key atrule\">resource</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> cpu\n                <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> Utilization\n                    <span class=\"token key atrule\">averageUtilization</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> constant-load-hpa.yaml</code></pre></div>\n<p>Once the HPA has been applied, it will start monitoring the CPU usage of the constant-load deployment. If the CPU usage exceeds the target utilization, the HPA will scale the deployment up to add more pods.</p>\n<h2 id=\"running-vertical-pod-autoscaler\" style=\"position:relative;\"><a href=\"#running-vertical-pod-autoscaler\" aria-label=\"running vertical pod autoscaler permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Running Vertical Pod Autoscaler</h2>\n<p>For the VPA to work, it relies on the Kubernetes Metrics Server to get a Pod's CPU/Memory values. However, it can also use Prometheus. See <a href=\"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#using-prometheus\" target=\"_blank\" rel=\"noopener noreferrer\">How can I use Prometheus for the VPA recommender</a>.</p>\n<h3 id=\"Ô∏è-installing-vertical-pod-autoscaler\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-installing-vertical-pod-autoscaler\" aria-label=\"Ô∏è installing vertical pod autoscaler permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Installing Vertical Pod Autoscaler</h3>\n<p>Let's use the Helm chart <code class=\"language-text\">cowboysysop/vertical-pod-autoscaler</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">helm repo <span class=\"token function\">add</span> cowboysysop https://cowboysysop.github.io/charts/\nhelm <span class=\"token parameter variable\">-n</span> kube-system upgrade <span class=\"token parameter variable\">--install</span> vertical-pod-autoscaler cowboysysop/vertical-pod-autoscaler</code></pre></div>\n<p>Then, you can check VPA's Pods:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token parameter variable\">-n</span> kube-system get pod <span class=\"token parameter variable\">-l</span> app.kubernetes.io/name<span class=\"token operator\">=</span>vertical-pod-autoscaler</code></pre></div>\n<p>And its CustomResourceDefinitions:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get crds\nNAME                                                   CREATED AT\nverticalpodautoscalercheckpoints.autoscaling.k8s.io  <span class=\"token number\">2023</span>-08-26T10:45:46Z\nverticalpodautoscalers.autoscaling.k8s.io            <span class=\"token number\">2023</span>-08-26T10:45:46Z</code></pre></div>\n<p>Now everything is ready to start using it.</p>\n<h3 id=\"-examples-of-work-with-vertical-pod-autoscaler\" style=\"position:relative;\"><a href=\"#-examples-of-work-with-vertical-pod-autoscaler\" aria-label=\" examples of work with vertical pod autoscaler permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìÑ Examples of Work with Vertical Pod Autoscaler</h3>\n<p>In the <a href=\"https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">VPA repository</a>, there is a directory named <code class=\"language-text\">vpa</code>, which contains examples of manifests. In the <code class=\"language-text\">hamster.yaml</code> file, there is an example of a configured VPA and a test Deployment. But let's create our manifests and deploy resources separately.</p>\n<h4 id=\"-first-describe-a-deployment\" style=\"position:relative;\"><a href=\"#-first-describe-a-deployment\" aria-label=\" first describe a deployment permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üêπ First, Describe a Deployment</h4>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> apps/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> hamster\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> hamster\n    <span class=\"token key atrule\">replicas</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n    <span class=\"token key atrule\">template</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> hamster\n        <span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n            <span class=\"token key atrule\">securityContext</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">runAsNonRoot</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n                <span class=\"token key atrule\">runAsUser</span><span class=\"token punctuation\">:</span> <span class=\"token number\">65534</span> <span class=\"token comment\"># nobody</span>\n            <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n                <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> hamster\n                    <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> registry.k8s.io/ubuntu<span class=\"token punctuation\">-</span>slim<span class=\"token punctuation\">:</span><span class=\"token number\">0.1</span>\n                    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n                            <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 100m\n                            <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 50Mi\n                    <span class=\"token key atrule\">command</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"/bin/sh\"</span><span class=\"token punctuation\">]</span>\n                    <span class=\"token key atrule\">args</span><span class=\"token punctuation\">:</span>\n                        <span class=\"token punctuation\">-</span> <span class=\"token string\">\"-c\"</span>\n                        <span class=\"token punctuation\">-</span> <span class=\"token string\">\"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"</span></code></pre></div>\n<p>Here we have to create two Pods with requests at 100 Milli CPU and 50 Megabyte memory. Deploy it:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> assets/manifests/vpa/hamster.yaml\ndeployment.apps/hamster created</code></pre></div>\n<p>A few minutes later, check the resources that are actually consumed by the Pods:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token function\">top</span> pod\nNAME                       CPU<span class=\"token punctuation\">(</span>cores<span class=\"token punctuation\">)</span>   MEMORY<span class=\"token punctuation\">(</span>bytes<span class=\"token punctuation\">)</span>\nhamster-65cd4dd797-fq9lq   498m         0Mi\nhamster-65cd4dd797-lnpks   499m         0Mi</code></pre></div>\n<h4 id=\"now-add-a-vpa\" style=\"position:relative;\"><a href=\"#now-add-a-vpa\" aria-label=\"now add a vpa permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Now, Add a VPA</h4>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"autoscaling.k8s.io/v1\"</span>\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> VerticalPodAutoscaler\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> hamster<span class=\"token punctuation\">-</span>vpa\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">targetRef</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"apps/v1\"</span>\n        <span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Deployment\n        <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> hamster\n    <span class=\"token key atrule\">resourcePolicy</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">containerPolicies</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">containerName</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'*'</span>\n                <span class=\"token key atrule\">minAllowed</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> 100m\n                    <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 50Mi\n                <span class=\"token key atrule\">maxAllowed</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n                    <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> 500Mi\n                <span class=\"token key atrule\">controlledResources</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"cpu\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"memory\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Deploy it:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl apply <span class=\"token parameter variable\">-f</span> assets/manifests/vpa/vpa.yaml\nverticalpodautoscaler.autoscaling.k8s.io/hamster-vpa created</code></pre></div>\n<p>Check the VPA object:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get vpa\nNAME         MODE CPU MEM PROVIDED AGE\nhamster-vpa   Auto                 14s</code></pre></div>\n<p>And in a minute or two, the Recommender starts working:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get vpa\nNAME          MODE  CPU   MEM     PROVIDED AGE\nhamster-vpa   Auto  587m  262144k  True    43s</code></pre></div>\n<p>And in another minute, check the Updater work‚Ää‚Äî‚Ääit kills old Pods to apply new recommended values for the requests:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get pod\nNAME                       READY   STATUS        RESTARTS   AGE\nhamster-65cd4dd797-fq9lq   <span class=\"token number\">1</span>/1     Terminating   <span class=\"token number\">0</span>          3m43s\nhamster-65cd4dd797-hc9cn   <span class=\"token number\">1</span>/1     Running       <span class=\"token number\">0</span>          13s\nhamster-65cd4dd797-lnpks   <span class=\"token number\">1</span>/1     Running       <span class=\"token number\">0</span>          3m43s</code></pre></div>\n<p>Check the value requests of the new Pod:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl get pod hamster-65cd4dd797-hc9cn <span class=\"token parameter variable\">-o</span> yaml <span class=\"token operator\">|</span> yq <span class=\"token string\">'.spec.containers[].resources'</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"requests\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"cpu\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"587m\"</span>,\n        <span class=\"token string\">\"memory\"</span><span class=\"token builtin class-name\">:</span> <span class=\"token string\">\"262144k\"</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>That's all for now. üéâ</p>\n<h2 id=\"-closing-notes\" style=\"position:relative;\"><a href=\"#-closing-notes\" aria-label=\" closing notes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìï Closing Notes</h2>\n<p>Kubernetes autoscaling is a pivotal strategy for achieving seamless scalability and operational efficiency in your containerized applications. By mastering the art of vertical and horizontal pod autoscaling, you unlock the ability to adapt to varying workloads while optimizing resource utilization. Whether it's boosting performance through vertical scaling or accommodating increased traffic with horizontal scaling, Kubernetes provides the tools to strike a balance between performance and cost-effectiveness. Harness the power of autoscaling to elevate your applications, ensuring they effortlessly meet user demands and resource constraints in today's dynamic computing landscape.</p>\n<br>\n<p><strong><em>Until next time, „Å§„Å•„Åè üéâ</em></strong></p>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìÖ Stay updated</strong></p>\n<p><strong>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</strong></p>","timeToRead":11,"rawMarkdownBody":"\n> **How to Scale Pods Vertically and Horizontally in Kubernetes üåê**\n\n## üê≥ Introduction\n\n[Kubernetes](https://kubernetes.io/) embodies resilience and scalability by deploying diverse pods with varying resource allocations, ensuring application redundancy. While manual adjustments suffice, Kubernetes elevates scaling with Horizontal Pod Autoscaling (HPA). This self-regulating loop dynamically expands or contracts resources (app Pods) based on real-time demands. Simply deploy a [HorizontalPodAutoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) resource for auto-scaling, leaving the automation to it.\n\nMoreover, in addition to HPA, the [Vertical Pod Autoscaler (VPA)](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) offers a distinct approach. Unlike HPA's horizontal scaling, VPA modifies a Pod's resources.requests, prompting the Kubernetes Scheduler to shift Pods across WorkerNodes as needed. VPA incessantly monitors container resource usage, automatically tweaking requests to prevent waste and ensure sufficient CPU and memory allocation. This synergy of HPA and VPA empowers Kubernetes users to effortlessly achieve efficient, tailored scaling.\n\nThis blog explores the universe of Kubernetes autoscaling, casting a spotlight on these two powerful tools: HPA and VPA. We'll uncover how they work, their distinctions, and how they can collaborate to enhance resource utilization and application performance.\n\nJoin us on a quest to become proficient in the art of scaling within Kubernetes.\n\n![hpa-vpa](./hpa-vpa.jpg)\n\n## üìà HorizontalPodAutoscaler (HPA): Scaling Horizontally\n\nUnder the hood, HPA is powered by a dedicated Kubernetes controller. You create an HPA YAML targeting your app's Deployment and use `kubectl` to apply it.\n\n## üìâ Vertical Pod Autoscaler (VPA): Scaling Vertically\n\nVPA involves three pods:\n\n- **Recommender**: [Analyzes](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/recommender/README.md) Pod resource usage and recommends CPU/memory requests.\n- **Updater**: [Monitors](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/updater/README.md) and aligns Pod requests with recommendations, recreating if needed.\n- **Admission-plugin**: [Sets](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/pkg/admission-controller/README.md) request values for new/updated Pods.\n\n### ‚ö†Ô∏è VPA Limitations\n\n- VPA doesn't oversee Pod recreation post-eviction, needing tools like Cluster Autoscaler.\n- It can't be used with HPA for CPU/memory scaling, but custom metrics are compatible.\n- VPA's Pod recreation might cause service downtime without fault-tolerant solutions.\n\nHPA and VPA require a [metrics server](https://github.com/kubernetes-sigs/metrics-server), like Kubernetes Metrics Server, to gather CPU/memory metrics for scaling decisions.\n\n### üìä Metrics API Types\n\n- **metrics.k8s.io**: Default metrics, provided by the [metrics-server](https://github.com/kubernetes-sigs/metrics-server).\n- **custom.metrics.k8s.io**: Metrics provided by adapters from inside a cluster, e.g., [Microsoft Azure Adapter](https://github.com/Azure/azure-k8s-metrics-adapter), [Google Stackdriver](https://github.com/GoogleCloudPlatform/k8s-stackdriver), [Prometheus Adapter](https://github.com/kubernetes-sigs/prometheus-adapter).\n- **external.metrics.k8s.io**: Similar to the Custom Metrics API, but metrics are provided by an external system, such as [AWS CloudWatch](https://aws.amazon.com/cloudwatch/).\n\nIn a nutshell, `metrics.k8s.io` encompasses default metrics from the metrics-server, while `custom.metrics.k8s.io` involves internal cluster adapters like Microsoft Azure or Google Stackdriver, and `external.metrics.k8s.io` pertains to external systems like AWS CloudWatch, providing adaptable metrics.\n\n## ‚òÅÔ∏è Deploy Metrics Server Using Helm\n\nTo initiate the process, integrate the metrics-server repository into your helm package collection. Employ the `helm repo add` command as follows:\n\n```shell\nhelm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server\n```\n\nThen, employ `helm repo update` to refresh the pool of accessible packages:\n\n```shell\n$ helm repo update metrics-server\n\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"metrics-server\" chart repository\nUpdate Complete. ‚éàHappy Helming!‚éà\n\n$ helm search repo metrics-server\n```\n\n### ‚ò∏Ô∏è Repository Integration Complete: Metrics Server Deployment\n\nWith the repository successfully added to Helm, you're poised to include metrics-server in your Kubernetes deployments. Here's how you can create your deployment configuration:\n\nClone the Kubernetes Starter Kit Git repository:\n\n```shell\ngit clone https://github.com/seifrajhi/K8s-Workload-Scaling-Strategies.git\n```\n\nLocate the metrics-server configuration in the following path:\n\n```shell\ncd K8s-Workload-Scaling-Strategies\nvim assets/manifests/metrics-server-values-v3.11.0.yaml\n```\n\nBy following these steps, you'll seamlessly integrate metrics-server into your Kubernetes environment and have the flexibility to tailor your deployment configuration to your needs.\n\nIt contains a few stock parameters. Note that replicas is a fixed value, 2.\n\n```shell\n## Ref: https://github.com/kubernetes-sigs/metrics-server/blob/metrics-server-helm-chart-3.8.2/charts/metrics-server\n# Number of metrics-server replicas to run\nreplicas: 2\n\napiService:\n    # Specifies if the v1beta1.metrics.k8s.io API service should be created.\n    create: true\n\nhostNetwork:\n    # Specifies if metrics-server should be started in hostNetwork mode.\n    enabled: false\n```\n\nYou can check the Metrics Server chart page for explanations of the available values for metrics-server.\n\nThen, you can install the Kubernetes Metrics Server using Helm (a dedicated metrics-server namespace will be created as well):\n\n```shell\nhelm install metrics-server metrics-server/metrics-server --version 3.11.0\\\n    --namespace metrics-server \\\n    --create-namespace \\\n    -f \"assets/manifests/metrics-server-v3.11.0.yaml\"\n```\n\nThis will deploy metrics-server to your configured Kubernetes cluster.\n\nAfter deploying, you can use `helm ls` to verify that metrics-server has been added to your deployment:\n\n```shell\nhelm ls -n metrics-server\n```\n\nNext, you can check the status of all of the Kubernetes resources deployed to the metrics-server namespace:\n\n```shell\nkubectl get all -n metrics-server\n```\n\nFinally, check if the `kubectl top` command works (similar to Linux top command - prints current resource usage, such as CPU and memory). Below command displays current resource usage for all Pods in the kube-system namespace:\n\n```shell\nkubectl top pods -n kube-system\n```\n\nYou have now deployed metrics-server into your Kubernetes cluster. In the next step, you'll review some of the parameters of a HorizontalPodAutoscaler Custom Resource Definition.\n\n## Introducing HPAs: The Key to Dynamic Scaling\n\nSo far, you've been manually configuring the number of pods in your Kubernetes deployments. This is fine for simple cases, but it can be a pain to keep up with as your application grows and traffic fluctuates.\n\nThat's where **HorizontalPodAutoscalers (HPAs)** come in. HPAs are a Kubernetes feature that automatically scales your deployments up or down based on metrics like CPU usage and memory utilization. This means that you can focus on building great applications, and let HPAs take care of the scaling for you.\n\n### How an HPA Works\n\n1. **Metric Monitoring**: The HPA watches a metric, such as CPU usage.\n2. **Threshold Exceeded**: When the metric exceeds a threshold, the HPA triggers a scale operation.\n3. **Scaling**: The scale operation either increases or decreases the number of pods in the deployment.\n\n### HPA CRD (Custom Resource Definition)\n\nThe HPA CRD is a YAML file that specifies the configuration of the HPA. The CRD includes the following fields:\n\n- **Target**: The name of the Kubernetes object that the HPA is monitoring.\n- **Metrics**: The metrics that the HPA is watching.\n- **Scaling Policies**: The rules for how the HPA scales the deployment.\n\nHere's an example of an HPA CRD:\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n    name: demo-hpa\nspec:\n    scaleTargetRef:\n        apiVersion: apps/v1\n        kind: Deployment\n        name: my-deployment\n    minReplicas: 1\n    maxReplicas: 3\n    metrics:\n        - type: Resource\n            resource:\n                name: cpu\n                target:\n                    type: Utilization\n                    averageUtilization: 80\n```\n\n### Explanation of the Configuration\n\n- **spec.scaleTargetRef**: Specifies the Kubernetes object that the HPA is monitoring. In this case, it is the `my-deployment` deployment.\n- **spec.minReplicas**: Specifies the lower limit for the number of replicas in the deployment. The HPA will never scale the deployment down below 1 pod.\n- **spec.maxReplicas**: Specifies the upper limit for the number of replicas in the deployment. The HPA will never scale the deployment up above 3 pods.\n- **spec.metrics.type**: Specifies the type of metric that the HPA is using to calculate the desired replica count. Here, it is using the `Resource` type, which means it is scaling the deployment based on the average CPU utilization.\n- **spec.metrics.resource.name**: Specifies the name of the resource that the HPA is monitoring. In this case, it is the `cpu` resource.\n- **spec.metrics.resource.averageUtilization**: Specifies the threshold value for the metric. The HPA will scale the deployment up if the average CPU utilization exceeds 80%.\n\n### Creating an HPA\n\nThere are two ways to create an HPA for your application deployment. You can use the `kubectl autoscale` command on an existing deployment, or you can create an HPA YAML manifest.\n\n#### Using `kubectl autoscale` Command\n\nTo create an HPA using the `kubectl autoscale` command, you need to specify the name of the deployment and the target CPU utilization. For example, the following command would create an HPA for the `myapp-test` deployment with a target CPU utilization of 80%:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: myapp-test\nspec:\n    selector:\n        matchLabels:\n            run: myapp-test\n    replicas: 1\n    template:\n        metadata:\n            labels:\n                run: myapp-test\n        spec:\n            containers:\n                - name: busybox\n                    image: busybox\n                    resources:\n                        limits:\n                            cpu: 50m\n                        requests:\n                            cpu: 20m\n                    command: [\"sh\", \"-c\"]\n                    args:\n                        - while [ 1 ]; do\n                            echo \"Test\";\n                            sleep 0.01;\n                            done\n```\n\nNote the last few lines of this file. They contain some shell syntax to repeatedly print \"Test\" a hundred times a second, to simulate load. Once you are done reviewing the file, you can deploy it into your cluster using `kubectl`:\n\n```shell\nkubectl apply -f assets/manifests/hpa/metrics-server/myapp-test.yaml\n```\n\nFinally, create a HorizontalPodAutoscaler targeting the `myapp-test` deployment:\n\n```shell\nkubectl autoscale deployment hpa-test --cpu-percent=80 --min=1 --max=3\n```\n\nYou can check if the HPA resource was created by running:\n\n```shell\nkubectl get hpa\n```\n\nThe `TARGETS` column of the output will eventually show a figure of current usage%/target usage%.\n\nYou can also observe the logged events that an HPA generates by using:\n\n```shell\nkubectl describe hpa hpa-test\n```\n\nIn a real-world scenario, you will want to use a dedicated YAML manifest to define each HPA. This way, you can track the changes by having the manifest committed in a Git repository, as well as come back to it later and perform changes.\n\n### Scaling Applications Automatically with Metrics Server\n\nIn this last step, you will experiment with two different ways of generating server load and scaling your applications automatically via a YAML manifest.\n\n#### Application Deployment\n\nOne way to generate server load is to create an application deployment that performs some CPU-intensive computations. This will cause the pods in the deployment to use more CPU resources, which will trigger the HorizontalPodAutoscaler (HPA) to scale the deployment up.\n\n#### Shell Script\n\nAnother way to generate server load is to use a shell script. This script can be configured to perform fast successive HTTP calls to a web application. This will cause the web application to use more CPU resources, which will also trigger the HPA to scale the deployment up.\n\n#### Constant Load Test\n\nIn this scenario, you will create a sample application that performs some CPU-intensive computations. This application is implemented in Python and is included in one of the example manifests from the starter kit. You can open the manifest, called `constant-load.yaml`, using nano or your favorite text editor.\n\nThe manifest defines a deployment that creates a pod that runs the Python application. The application will continuously perform CPU-intensive computations, which will cause the pod to use more CPU resources. This will trigger the HorizontalPodAutoscaler (HPA) to scale the deployment up.\n\nOnce you have opened the manifest, you can edit it to change the number of replicas in the deployment. The default number of replicas is 1, but you can increase this number to create more pods. The more pods you create, the more CPU resources will be used, and the more the HPA will scale the deployment up.\n\nOnce you have edited the manifest, you can save it and apply it to your cluster using the `kubectl apply` command:\n\n```shell\nkubectl apply -f assets/manifests/hpa/metrics-server/constant-load.yaml\n```\n\nOnce the manifest has been applied, the HPA will start monitoring the CPU usage of the deployment. If the CPU usage exceeds the target utilization, the HPA will scale the deployment up to add more pods.\n\nVerify that the deployment was created successfully, and that it's up and running:\n\n```shell\nkubectl get deployments\n```\n\nNext, you will need to deploy another HorizontalPodAutoscaler (HPA) to this cluster. There is an example HPA that is matched to this scenario in the file `constant-load-hpa.yaml`.\n\nThe `constant-load-hpa.yaml` file defines an HPA that monitors the CPU usage of the constant-load deployment. The HPA will scale the deployment up if the CPU usage exceeds the target utilization, which is set to 80% by default.\n\n```shell\ncat assets/manifests/hpa/metrics-server/constant-load-hpa.yaml\n```\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n    name: constant-load\nspec:\n    scaleTargetRef:\n        apiVersion: apps/v1\n        kind: Deployment\n        name: constant-load-deployment\n    minReplicas: 1\n    maxReplicas: 3\n    metrics:\n        - type: Resource\n            resource:\n                name: cpu\n                target:\n                    type: Utilization\n                    averageUtilization: 80\n```\n\n```shell\nkubectl apply -f constant-load-hpa.yaml\n```\n\nOnce the HPA has been applied, it will start monitoring the CPU usage of the constant-load deployment. If the CPU usage exceeds the target utilization, the HPA will scale the deployment up to add more pods.\n\n## Running Vertical Pod Autoscaler\n\nFor the VPA to work, it relies on the Kubernetes Metrics Server to get a Pod's CPU/Memory values. However, it can also use Prometheus. See [How can I use Prometheus for the VPA recommender](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#using-prometheus).\n\n### üõ†Ô∏è Installing Vertical Pod Autoscaler\n\nLet's use the Helm chart `cowboysysop/vertical-pod-autoscaler`:\n\n```shell\nhelm repo add cowboysysop https://cowboysysop.github.io/charts/\nhelm -n kube-system upgrade --install vertical-pod-autoscaler cowboysysop/vertical-pod-autoscaler\n```\n\nThen, you can check VPA's Pods:\n\n```shell\nkubectl -n kube-system get pod -l app.kubernetes.io/name=vertical-pod-autoscaler\n```\n\nAnd its CustomResourceDefinitions:\n\n```shell\nkubectl get crds\nNAME                                                   CREATED AT\nverticalpodautoscalercheckpoints.autoscaling.k8s.io  2023-08-26T10:45:46Z\nverticalpodautoscalers.autoscaling.k8s.io            2023-08-26T10:45:46Z\n```\n\nNow everything is ready to start using it.\n\n### üìÑ Examples of Work with Vertical Pod Autoscaler\n\nIn the [VPA repository](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler), there is a directory named `vpa`, which contains examples of manifests. In the `hamster.yaml` file, there is an example of a configured VPA and a test Deployment. But let's create our manifests and deploy resources separately.\n\n#### üêπ First, Describe a Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: hamster\nspec:\n    selector:\n        matchLabels:\n            app: hamster\n    replicas: 2\n    template:\n        metadata:\n            labels:\n                app: hamster\n        spec:\n            securityContext:\n                runAsNonRoot: true\n                runAsUser: 65534 # nobody\n            containers:\n                - name: hamster\n                    image: registry.k8s.io/ubuntu-slim:0.1\n                    resources:\n                        requests:\n                            cpu: 100m\n                            memory: 50Mi\n                    command: [\"/bin/sh\"]\n                    args:\n                        - \"-c\"\n                        - \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"\n```\n\nHere we have to create two Pods with requests at 100 Milli CPU and 50 Megabyte memory. Deploy it:\n\n```shell\nkubectl apply -f assets/manifests/vpa/hamster.yaml\ndeployment.apps/hamster created\n```\n\nA few minutes later, check the resources that are actually consumed by the Pods:\n\n```shell\nkubectl top pod\nNAME                       CPU(cores)   MEMORY(bytes)\nhamster-65cd4dd797-fq9lq   498m         0Mi\nhamster-65cd4dd797-lnpks   499m         0Mi\n```\n\n#### Now, Add a VPA\n\n```yaml\napiVersion: \"autoscaling.k8s.io/v1\"\nkind: VerticalPodAutoscaler\nmetadata:\n    name: hamster-vpa\nspec:\n    targetRef:\n        apiVersion: \"apps/v1\"\n        kind: Deployment\n        name: hamster\n    resourcePolicy:\n        containerPolicies:\n            - containerName: '*'\n                minAllowed:\n                    cpu: 100m\n                    memory: 50Mi\n                maxAllowed:\n                    cpu: 1\n                    memory: 500Mi\n                controlledResources: [\"cpu\", \"memory\"]\n```\n\nDeploy it:\n\n```shell\nkubectl apply -f assets/manifests/vpa/vpa.yaml\nverticalpodautoscaler.autoscaling.k8s.io/hamster-vpa created\n```\n\nCheck the VPA object:\n\n```shell\nkubectl get vpa\nNAME         MODE CPU MEM PROVIDED AGE\nhamster-vpa   Auto                 14s\n```\n\nAnd in a minute or two, the Recommender starts working:\n\n```shell\nkubectl get vpa\nNAME          MODE  CPU   MEM     PROVIDED AGE\nhamster-vpa   Auto  587m  262144k  True    43s\n```\n\nAnd in another minute, check the Updater work‚Ää‚Äî‚Ääit kills old Pods to apply new recommended values for the requests:\n\n```shell\nkubectl get pod\nNAME                       READY   STATUS        RESTARTS   AGE\nhamster-65cd4dd797-fq9lq   1/1     Terminating   0          3m43s\nhamster-65cd4dd797-hc9cn   1/1     Running       0          13s\nhamster-65cd4dd797-lnpks   1/1     Running       0          3m43s\n```\n\nCheck the value requests of the new Pod:\n\n```shell\nkubectl get pod hamster-65cd4dd797-hc9cn -o yaml | yq '.spec.containers[].resources'\n{\n    \"requests\": {\n        \"cpu\": \"587m\",\n        \"memory\": \"262144k\"\n    }\n}\n```\n\nThat's all for now. üéâ\n\n## üìï Closing Notes\n\nKubernetes autoscaling is a pivotal strategy for achieving seamless scalability and operational efficiency in your containerized applications. By mastering the art of vertical and horizontal pod autoscaling, you unlock the ability to adapt to varying workloads while optimizing resource utilization. Whether it's boosting performance through vertical scaling or accommodating increased traffic with horizontal scaling, Kubernetes provides the tools to strike a balance between performance and cost-effectiveness. Harness the power of autoscaling to elevate your applications, ensuring they effortlessly meet user demands and resource constraints in today's dynamic computing landscape.\n\n<br>\n\n**_Until next time, „Å§„Å•„Åè üéâ_**\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìÖ Stay updated**\n\n**Subscribe to our newsletter for more insights on AWS cloud computing and containers.**\n","wordCount":{"words":1989},"frontmatter":{"id":"58af1812eaa1a9302e171699","path":"/blog/kubernetes-autoscaling-pods-hpa-vpa/","humanDate":"Oct 27, 2024","fullDate":"2024-10-27","title":"Kubernetes Autoscaling: Achieving Scalability and Efficiency","keywords":["Kubernetes","Autoscaling","HPA & VPA","SRE"],"excerpt":"Learn how to effectively scale your Kubernetes pods both vertically and horizontally using Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) for optimal resource utilization.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAADOUlEQVR42qVVa0hTcRS/pT006AWBlEr1rT72oIIeXyIsFAoaZR8yCLX5qCQo+lC3DJs687Es3XI+shksnzG3+ajNOTUqk+ixpndGOm3mlptOZ5O7X/c/p02CSjtwOX/O4/c/5/zPOZei/kAAFhGep+xJFalM6f6yBZNIydwufzkCWacDd9W92RRFL/ap5gHsi4KXLl+Vp+q9K20dREn7EEQNn8Vn76jXLihSnlweQLhQYdwl0ZpR2jEMQeWH/f66eRENeFNLqzHte9BiRhkHeKvSeNBfNy+KE2MJ4eIG465CjRnFbcPIb5iOcEb3h/QQ4L2V1IV8tC+C7Viynx66vOfad+yhHdh7zXKdivAs86Uwx342alJcmhPwaPnSWUOONvKth8ITbB+DTo/iqHDUc1hg9wTHjCIswda9iT8c5W8bkaxcRnBmHypXyZzMrDEkk/MpgWINQAWEJk00R+YCN5843eU6Fx62uJBW7XRHcbINiS7dJaFwBekC4pNR9S4hq84Y7QXLqPsYWdz2zVHcZvle8LzPINX194gazUwY3zpxRGBnVV0/PGUtk3jU6kLj2x+ITLezIfE2l6hxgCnibAuff/kk1VusHIZTWGs4ThVqLY8rP0xB9mZ0qtoI1DNAdosHIfEjrKDGCUmzC60GN150uyFuckH4dByh/BH2dpMH9Sag+hNQ0TXmrjKwkOgslRR1gA4s0PSrxdpBTWZd98ViLXPlRM7AvU3nnWycZMxD0n3cNokK/SRkrZPgPxjzbL4wjmNZg5KHOuayUNGdUqgdbCp41tcUQ2uWe9O+I+/aQJd17vDW2Vfo8MSR+ytjp3C1Yswt07sg1UzXcFXcFMIT7SX+tsSXruhcP2fUiFJO2odGIHmtrbz3S8OSnM3bbgCh52xsSKyV3ZnKnRMd+t0pfUE+wECfz2Lqt1H0F9DTCyA4xhmyJcVSyssdQnS+FVsufpUtP2ML82+ZX75/mW1yM+FSblIedZgh7xxGkWJ6UmZ0C1sOtb7Ra/+P5TDT8TF0zWqRujdfqhtgS/RfkacyiZPTlOv+a9HmKJm08ld2yF47wG3tHD/V/AFnfwFq0608FZP1L5H9BK/gH2DI+9VTAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/ac5fc33b531d775f51f3a52118a89a2d/d0bbc/hpa-vpa-cover.png","srcSet":"/static/ac5fc33b531d775f51f3a52118a89a2d/38e6a/hpa-vpa-cover.png 750w,\n/static/ac5fc33b531d775f51f3a52118a89a2d/d0bbc/hpa-vpa-cover.png 910w","sizes":"100vw"},"sources":[{"srcSet":"/static/ac5fc33b531d775f51f3a52118a89a2d/b9e89/hpa-vpa-cover.webp 750w,\n/static/ac5fc33b531d775f51f3a52118a89a2d/55619/hpa-vpa-cover.webp 910w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.0021978021978022}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/cve-vulnerability-database-devsecops/","title":"CVE Vulnerability Database: What It Is and How to Use It in DevSecOps","date":"2024-10-27 20:22:00"},"excerpt":"Shifting Security Left\nüîÑ Shifting Security Left üìå Introduction Common Vulnerabilities and Exposures (CVE) is a standardized dictionary of‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/pack-dockerfile-less-deployment-kubernetes/","title":"Pack: Dockerfile-less Deployment to Kubernetes with Cloud Native Buildpacks","date":"2024-10-27 19:34:00"},"excerpt":"Build your OCI images using Buildpacks ‚ÑπÔ∏è Overview In the ecosystem of containerization, Docker has long been the de facto standard for‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}