{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/data-on-kubernetes-ai-ml-5/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>Using Kubernetes to deploy and scale AI/ML models efficiently üìà</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üìö Introduction</h2>\n<p>Welcome back to our 'Data on Kubernetes' series. In this fifth part, we're focusing on AI and machine learning (ML). We'll discuss how Kubernetes is transforming the use of <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a> software.</p>\n<p>The more we rely on <a href=\"https://ai.engineering.columbia.edu/ai-vs-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">AI/ML</a>, the more complex it becomes to manage these systems. Kubernetes offers a helping hand with features that allow AI/ML software to automatically adjust their size, update without stopping, and repair themselves. This keeps AI/ML systems always on and working smoothly.</p>\n<p>In this blog, we'll discover the tools <a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, and their integration with Kubernetes to simplify the process, all within the AWS cloud ecosystem.</p>\n<h2 id=\"-kubernetes-making-aiml-easier\" style=\"position:relative;\"><a href=\"#-kubernetes-making-aiml-easier\" aria-label=\" kubernetes making aiml easier permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üöÄ Kubernetes: Making AI/ML Easier</h2>\n<p>When we use AI and ML, things can get pretty tricky. It's exciting but can also be a bit overwhelming with all the pieces you need to keep track of and there's a lot to manage. That's exactly why Kubernetes comes in‚Ää‚Äî‚Ääit's like having a smart assistant that helps keep everything in order.</p>\n<p>Kubernetes is great because it can fix itself. If something goes wrong with an AI program, Kubernetes doesn't wait around; it jumps right in to fix the problem.</p>\n<p>This means if an AI program stops working, Kubernetes doesn't just stand by. It jumps into action, fixing things up or starting fresh so that there's no downtime. This means you can rely on your AI and ML to work non-stop.</p>\n<p>But that's not all. Kubernetes is also smart about using resources. Sometimes AI needs more resources to work on big tasks, and sometimes it needs less.</p>\n<p>Kubernetes is smart enough to adjust on the fly, giving them more resources when they're busy and scaling back when they're not. This way, you're not wasting any energy or money, and everything runs super efficiently.</p>\n<h3 id=\"Ô∏è-ray-and-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-ray-and-kubernetes\" aria-label=\"Ô∏è ray and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Ray and Kubernetes</h3>\n<p>When working with AI and ML tasks, we require tools that can manage extensive workloads efficiently.</p>\n<p><a href=\"https://www.ray.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Ray</a> is such a tool, designed for <a href=\"https://www.python.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Python</a>, a widely-used programming language. It simplifies the execution of large-scale projects on multiple computers simultaneously.</p>\n<p>Ray serves as a comprehensive toolkit for deep learning, an AI branch that equips computers with cognitive abilities.</p>\n<p><a href=\"https://ray-project.github.io/q4-2021-docs-hackathon/0.4/ray-overview/what-and-why-ray/\" target=\"_blank\" rel=\"noopener noreferrer\">Developed by experts at UC Berkeley</a>, Ray's objective is to streamline large computing projects. It consists of Ray Core, the primary component for distributing tasks across computers, and Ray Ecosystem, a suite of specialized tools for various purposes, including project optimization and decision-making algorithms.</p>\n<p>While similar to Dask, a tool for executing Python code concurrently in different locations and processing multiple data-related tasks simultaneously, Ray stands out.</p>\n<p>It doesn't merely replicate existing tools like NumPy and Pandas; it's engineered to be versatile, catering to a wide array of projects beyond data science. Its adaptability has led major projects such as spaCy, Hugging Face, and XGBoost to incorporate Ray, enhancing their efficiency and intelligence.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.23529411764706%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAACiElEQVR42jWNa0hTcRjG/0Rk60ZgWrpVNAXXEiqOSK3LiEqjDGJS9CGx9sFKmjQlqFGNxgwNUUaSac7Qpc7j5mWZ7qpubmM3d8s8m1vokTPmTAzTrTT1dFb5wI/39jy8AACwm82+eZzDKaMTPYkggWArk8mkMCBGWnY6mQJRqQfoSUk7iP0Wgu3xDKCcILwUklDIIwt5D1Po/24JAF9dLR73OGQ2j89qnp4mcV04GQD2zkk5tSGoO8UCd3Ey5WVLetozGEodxxNPJydcID7ermV9OFi5n52q0qhNGq2hY/DrEs8686sUYFgoUuFcYDAbbZlQO5rGEkqv5tziHjXKS3het+J5LBrT+GbnB8XukL7Iht85n3P5EiNlzxlWXj2Ne6WOPOoNcFDLo67eNq5UUNNcDv4qW7QLHLueBAr79hHTZhqNlih4VVeuUqp7IohzyhWYCrfYglNdI2OFxp5GgVfxrmTvxbJkCIK2xeMYDERNgizkWv69t0DtCMBqO9I+6PC3Km1+WO0ISrWjwQ6dHZG2Gv1wtcorVwzZ5Z+sPrnK7JWp9Hat0uTS9Bs9HR8NLqlixN3ZZ/JKldYJucLgrgG+uXX8c3h5fWxmBf8SWcE9odiaA12KIdjSmiG48EPinotpXOhvJ7qwaguEoyYEi1qC84uW4NyyNfANj2OZmF0LfMdxJ7roB2/ggYw49TIlrbq1jypVas9ZzOaqim59bqYKG8gaCj9BqwsKawXiG6VNkSp+E/JULBbr2jq7X0j6LZTmLvVhSY8u4z2Rb4B7D4EN8fn8TfGqNRjoky6zoMCE5ZGGoxKS/ucDWfOR3LOPh08WvV6svC8KcRrrRHIYhovjfvx/bkN/AMWvUIIkJ3LDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ray\" title=\"\" src=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png\" srcset=\"/static/4abe3f459ce22fb22fa13e41f0905d5f/04472/ray.png 170w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/9f933/ray.png 340w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/c5bb3/ray.png 680w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b12f7/ray.png 1020w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/b5a09/ray.png 1360w,\n/static/4abe3f459ce22fb22fa13e41f0905d5f/2cefc/ray.png 1400w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead\">Source</a></div>\n<p>The diagram above shows that at a high level, the Ray ecosystem consists of:</p>\n<ul>\n<li>The core Ray system</li>\n<li>Scalable libraries for machine learning (both native and third party)</li>\n<li>Tools for <a href=\"https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208\" target=\"_blank\" rel=\"noopener noreferrer\">launching clusters on any cluster or cloud provider</a></li>\n</ul>\n<p>KubeRay amplifies Ray's capabilities by integrating it with Kubernetes, a platform that orchestrates numerous computers working in unison. This integration combines Ray's user-friendly Python interface with Kubernetes' robust and dependable framework.</p>\n<p>The result is a great system capable of developing, operating, and maintaining large-scale projects easily. It's akin to an advanced system that scales with your requirements, self-repairs, and autonomously performs maintenance and updates.</p>\n<p>The use of Kubernetes is advantageous for Ray. It acts as a command center for computers, ensuring smooth operation and task management. Kubernetes excels with both tools, adeptly managing numerous tasks without confusion.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfElEQVR42lVR2VLcMBD0/39RXvLKoykCbAxsFvbyWpLlS7Zl6+qMZHKgqi71HN010mTOOTjn8ef23mNdV3DGIIRIYJxj0QvVwmfvP2itUV6vqOs6abNARsQ+sfGYY+0AwRlu1wu6tkm1kOpfMc0aH6cLJBmGEJCxyaEcDW6jxW3yqChms8dp9BQ7yhtwTXnKsdlB6ACeeITF6ijuNBZ6VTzZmUSv+194ez+BLcDhdMaxrFAqCyFbiKZDLSXemwln0aBkAiWvcRMSh97A0rP5oDGYAMQJi87hWS7YNSseG4vnWqc4ZzPu9iXhisdK4UEa5HzBPdcJOeGpMTRTwMN+BG83nr31Dk8tmSaQYefxc6AmpvD9xyu+5QXuiRcKKKj3RXkUVN/1HlFrLH1VxaHUuP1h3G7cTgTFsEpCDxJVH5AfDYLROO5KTKrHy2XF4dKjbQQGpZJB1MUTefTKIvkf3hk4a7AYj27a+NjP6Va0iHE2MGaFpcn+atIwG/8NhIhlZU3+KBUAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay\" title=\"\" src=\"/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png\" srcset=\"/static/a78dd992dd6340928c4606a616df4d61/04472/kuberay.png 170w,\n/static/a78dd992dd6340928c4606a616df4d61/9f933/kuberay.png 340w,\n/static/a78dd992dd6340928c4606a616df4d61/c5bb3/kuberay.png 680w,\n/static/a78dd992dd6340928c4606a616df4d61/b12f7/kuberay.png 1020w,\n/static/a78dd992dd6340928c4606a616df4d61/b5a09/kuberay.png 1360w,\n/static/a78dd992dd6340928c4606a616df4d61/29007/kuberay.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\">Source</a></div>\n<h3 id=\"-pytorch-and-kubernetes\" style=\"position:relative;\"><a href=\"#-pytorch-and-kubernetes\" aria-label=\" pytorch and kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üî• PyTorch and Kubernetes</h3>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> is an open-source machine learning library for Python, widely used for applications such as natural language processing, computer vision, and deep learning.</p>\n<p>What makes PyTorch special is its ease of use, flexibility, and dynamic computational graph, which allows for quick prototyping and experimentation. Researchers like it because it's designed to work well with Python and makes deep learning straightforward.</p>\n<h4 id=\"pytorch-and-tensorflow\" style=\"position:relative;\"><a href=\"#pytorch-and-tensorflow\" aria-label=\"pytorch and tensorflow permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch and TensorFlow</h4>\n<p><a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> and <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a> are both strong in deep learning, but they're good at different things. TensorFlow, made by Google, is great for getting big machine learning models ready for use and for training them on many computers at once.</p>\n<p>PyTorch is more about being easy to work with and changing things as you go, which is really helpful when you're still figuring things out.</p>\n<h4 id=\"pytorch-with-ray-and-kuberay\" style=\"position:relative;\"><a href=\"#pytorch-with-ray-and-kuberay\" aria-label=\"pytorch with ray and kuberay permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch with Ray and KubeRay</h4>\n<p>When PyTorch works together with Ray or KubeRay, it gets even better at deep learning jobs. Ray and KubeRay help spread out the work over many computers, making things faster and more reliable.</p>\n<p>This integration facilitates the distribution of computational tasks across multiple nodes, enabling faster processing times and resilience against individual node failures.</p>\n<p>They also make better use of cloud services, saving money. This combination means developers can spend more time being creative with their machine learning models, while the tough tech stuff is taken care of.</p>\n<h4 id=\"pytorch-meets-kubernetes\" style=\"position:relative;\"><a href=\"#pytorch-meets-kubernetes\" aria-label=\"pytorch meets kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>PyTorch meets Kubernetes</h4>\n<p>By using PyTorch with Kubernetes (also known as k8s), you get all the perks of Kubernetes' smart way of managing containers. Kubernetes helps set up, grow, and manage containers over lots of computers.</p>\n<p>This means PyTorch applications can handle big, complicated jobs better, grow more easily, and keep running smoothly. Developers can then put more energy into making their models, without worrying too much about the tech behind it.</p>\n<h2 id=\"Ô∏è-hands-on-ray-on-kubernetes\" style=\"position:relative;\"><a href=\"#%EF%B8%8F-hands-on-ray-on-kubernetes\" aria-label=\"Ô∏è hands on ray on kubernetes permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üõ†Ô∏è Hands-on: Ray on Kubernetes</h2>\n<p>Our setup consists of a Kubernetes cluster in Amazon EKS, which hosts our AI applications. The recommended method for installing Ray onto a Kubernetes cluster is through KubeRay, a Kubernetes Operator. The KubeRay Operator allows you to create and manage Ray clusters in a Kubernetes-native way by defining clusters as a custom resource called a RayCluster.</p>\n<p>The installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob, and RayService as documented <a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n<p>This RayCluster resource describes the desired state of a Ray cluster. The KubeRay Operator manages the cluster lifecycle, autoscaling, and monitoring functions.</p>\n<p>Hence, we use the KubeRay Operator for our Ray cluster installation.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 63.52941176470588%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeUlEQVR42o2SiW7CQAxE8/9/iBQgQuIIRw4ICWkSu35ujQiiUleabuNjPDsmUdWH4esV0zQ98Z4LjOPooIbzG+8T+zPpP06e57perzXLMl2tVno4HPR0Onn8er3q44EulRmhTfNEVVVa17XebjdlOPGyLLVtW+26zuOQXS4XPZ/PfxOSoCBNUwdKIC6KwklEZKaaAeAlPiekCTXDMDhQhwrUMIj/qUENT91ut7rb7Tz/UeH9fvcnoQqypmncK4gAJPv93uuowZq+79WWEyp/CPlAGdODBKAKFRDQyE3d+5k92QomJgAmQogamgHfx+PRgQ3UhSUM4SUBo5LECqYwF1UsAKLYOhYwKIZByGEoQ/CPm/yTED/YcBDi1XK59C3TgG/c5FGFOnKLxUI3m40T0hOEo5GJkYr91sQIxbwUa3LYZDE1nuO2p4mRCj1GIqZajND7sc8V8gyeB2JjgYgHiFETi8Iq7uhNrKAwSypgwY94z8f3K4gbV/kNppPvlhafGiUAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"kuberay operator\" title=\"\" src=\"/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png\" srcset=\"/static/0fe3a262cf7937c74bc1df905770e678/04472/kuberay-op.png 170w,\n/static/0fe3a262cf7937c74bc1df905770e678/9f933/kuberay-op.png 340w,\n/static/0fe3a262cf7937c74bc1df905770e678/c5bb3/kuberay-op.png 680w,\n/static/0fe3a262cf7937c74bc1df905770e678/b12f7/kuberay-op.png 1020w,\n/static/0fe3a262cf7937c74bc1df905770e678/c61d0/kuberay-op.png 1145w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<div class=\"image-title\"><a href=\"https://medium.com/sage-ai/demystifying-the-process-of-building-a-ray-cluster-110c67914a99\">Source</a></div>\n<p>KubeRay Operator managing the Ray Cluster lifecycle. Now let us deploy the infrastructure. Start by cloning the repo and change the working directory.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">git</span> clone https://github.com/seifrajhi/data-on-eks\n<span class=\"token builtin class-name\">cd</span> data-on-eks/ai-ml/ray/terraform</code></pre></div>\n<p>Next, we can use the shell script <code class=\"language-text\">install.sh</code> to run the <code class=\"language-text\">terraform init</code> and <code class=\"language-text\">apply</code> commands.</p>\n<p>Update <code class=\"language-text\">variables.tf</code> to change the region. Also, we can update any other input variables or make any other changes to the terraform template.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">./install.sh</code></pre></div>\n<p>And now, we can run the PyTorch benchmark.</p>\n<p>We deploy a Ray Cluster with its own configuration for Karpenter workers. Different jobs can have different requirements for Ray Cluster such as a different version of Ray libraries or EC2 instance configuration such as making use of Spot market or GPU instances.</p>\n<p>To deploy the Ray cluster run below commands:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token builtin class-name\">cd</span> examples/pytorch\nterraform init\nterraform plan\nterraform apply -auto-approve</code></pre></div>\n<p>Once running, we can forward the port for the server:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl port-forward service/pytorch-kuberay-head-svc <span class=\"token parameter variable\">-n</span> pytorch <span class=\"token number\">8266</span>:8265</code></pre></div>\n<p>We can then submit the job for PyTorch benchmark workload:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">python job/pytorch_submit.py</code></pre></div>\n<p>You can open <a href=\"http://localhost:8266\" target=\"_blank\" rel=\"noopener noreferrer\">http://localhost:8266</a> to monitor the progress of the PyTorch benchmark.</p>\n<p>The <code class=\"language-text\">pytorch_submit.py</code> script is a benchmarking for evaluating the performance of training and tuning a PyTorch model on a distributed system using Ray.</p>\n<p>It measures the time taken to train a model and to find the best hyperparameters through tuning, providing insights into the efficiency of distributed machine learning workflows.</p>\n<h2 id=\"-key-takeaways\" style=\"position:relative;\"><a href=\"#-key-takeaways\" aria-label=\" key takeaways permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>üîö Key Takeaways</h2>\n<p>Kubernetes makes working with AI/ML simpler. It scales AI/ML models easily and keeps them running smoothly. With Kubernetes, Ray, and PyTorch work better in the cloud, making AI/ML systems more reliable and easier to manage.</p>\n<br>\n<br>\n<blockquote>\n<p>üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  <strong><em>Until next time üéâ</em></strong></p>\n</blockquote>\n<p>üöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>‚ôªÔ∏è LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>‚ôªÔ∏è X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ‚úåüèª</strong></p>\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n<p><strong>üìªüß° References:</strong></p>\n<ul>\n<li><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">https://docs.ray.io/en/latest/cluster/kubernetes/index.html</a></li>\n<li><a href=\"https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/\" target=\"_blank\" rel=\"noopener noreferrer\">https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/</a></li>\n<li><a href=\"https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws\" target=\"_blank\" rel=\"noopener noreferrer\">https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws</a></li>\n<li><a href=\"https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay\" target=\"_blank\" rel=\"noopener noreferrer\">https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay</a></li>\n</ul>\n<p><strong>üìÖ Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":6,"rawMarkdownBody":"\n> **Using Kubernetes to deploy and scale AI/ML models efficiently üìà**\n\n## üìö Introduction\n\nWelcome back to our 'Data on Kubernetes' series. In this fifth part, we're focusing on AI and machine learning (ML). We'll discuss how Kubernetes is transforming the use of [AI/ML](https://ai.engineering.columbia.edu/ai-vs-machine-learning/) software.\n\nThe more we rely on [AI/ML](https://ai.engineering.columbia.edu/ai-vs-machine-learning/), the more complex it becomes to manage these systems. Kubernetes offers a helping hand with features that allow AI/ML software to automatically adjust their size, update without stopping, and repair themselves. This keeps AI/ML systems always on and working smoothly.\n\nIn this blog, we'll discover the tools [Ray](https://www.ray.io/) and [PyTorch](https://pytorch.org/), and their integration with Kubernetes to simplify the process, all within the AWS cloud ecosystem.\n\n## üöÄ Kubernetes: Making AI/ML Easier\n\nWhen we use AI and ML, things can get pretty tricky. It's exciting but can also be a bit overwhelming with all the pieces you need to keep track of and there's a lot to manage. That's exactly why Kubernetes comes in‚Ää‚Äî‚Ääit's like having a smart assistant that helps keep everything in order.\n\nKubernetes is great because it can fix itself. If something goes wrong with an AI program, Kubernetes doesn't wait around; it jumps right in to fix the problem.\n\nThis means if an AI program stops working, Kubernetes doesn't just stand by. It jumps into action, fixing things up or starting fresh so that there's no downtime. This means you can rely on your AI and ML to work non-stop.\n\nBut that's not all. Kubernetes is also smart about using resources. Sometimes AI needs more resources to work on big tasks, and sometimes it needs less.\n\nKubernetes is smart enough to adjust on the fly, giving them more resources when they're busy and scaling back when they're not. This way, you're not wasting any energy or money, and everything runs super efficiently.\n\n### üõ†Ô∏è Ray and Kubernetes\n\nWhen working with AI and ML tasks, we require tools that can manage extensive workloads efficiently.\n\n[Ray](https://www.ray.io/) is such a tool, designed for [Python](https://www.python.org/), a widely-used programming language. It simplifies the execution of large-scale projects on multiple computers simultaneously.\n\nRay serves as a comprehensive toolkit for deep learning, an AI branch that equips computers with cognitive abilities.\n\n[Developed by experts at UC Berkeley](https://ray-project.github.io/q4-2021-docs-hackathon/0.4/ray-overview/what-and-why-ray/), Ray's objective is to streamline large computing projects. It consists of Ray Core, the primary component for distributing tasks across computers, and Ray Ecosystem, a suite of specialized tools for various purposes, including project optimization and decision-making algorithms.\n\nWhile similar to Dask, a tool for executing Python code concurrently in different locations and processing multiple data-related tasks simultaneously, Ray stands out.\n\nIt doesn't merely replicate existing tools like NumPy and Pandas; it's engineered to be versatile, catering to a wide array of projects beyond data science. Its adaptability has led major projects such as spaCy, Hugging Face, and XGBoost to incorporate Ray, enhancing their efficiency and intelligence.\n\n![ray](./ray.png)\n<div class=\"image-title\"><a href=\"https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead\">Source</a></div>\n\nThe diagram above shows that at a high level, the Ray ecosystem consists of:\n\n- The core Ray system\n- Scalable libraries for machine learning (both native and third party)\n- Tools for [launching clusters on any cluster or cloud provider](https://medium.com/distributed-computing-with-ray/how-to-scale-python-on-every-major-cloud-provider-12b3bde01208)\n\nKubeRay amplifies Ray's capabilities by integrating it with Kubernetes, a platform that orchestrates numerous computers working in unison. This integration combines Ray's user-friendly Python interface with Kubernetes' robust and dependable framework.\n\nThe result is a great system capable of developing, operating, and maintaining large-scale projects easily. It's akin to an advanced system that scales with your requirements, self-repairs, and autonomously performs maintenance and updates.\n\nThe use of Kubernetes is advantageous for Ray. It acts as a command center for computers, ensuring smooth operation and task management. Kubernetes excels with both tools, adeptly managing numerous tasks without confusion.\n\n![kuberay](./kuberay.png)\n<div class=\"image-title\"><a href=\"https://docs.ray.io/en/latest/cluster/kubernetes/index.html\">Source</a></div>\n\n### üî• PyTorch and Kubernetes\n\n[PyTorch](https://pytorch.org/) is an open-source machine learning library for Python, widely used for applications such as natural language processing, computer vision, and deep learning.\n\nWhat makes PyTorch special is its ease of use, flexibility, and dynamic computational graph, which allows for quick prototyping and experimentation. Researchers like it because it's designed to work well with Python and makes deep learning straightforward.\n\n#### PyTorch and TensorFlow\n\n[PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) are both strong in deep learning, but they're good at different things. TensorFlow, made by Google, is great for getting big machine learning models ready for use and for training them on many computers at once.\n\nPyTorch is more about being easy to work with and changing things as you go, which is really helpful when you're still figuring things out.\n\n#### PyTorch with Ray and KubeRay\n\nWhen PyTorch works together with Ray or KubeRay, it gets even better at deep learning jobs. Ray and KubeRay help spread out the work over many computers, making things faster and more reliable.\n\nThis integration facilitates the distribution of computational tasks across multiple nodes, enabling faster processing times and resilience against individual node failures.\n\nThey also make better use of cloud services, saving money. This combination means developers can spend more time being creative with their machine learning models, while the tough tech stuff is taken care of.\n\n#### PyTorch meets Kubernetes\n\nBy using PyTorch with Kubernetes (also known as k8s), you get all the perks of Kubernetes' smart way of managing containers. Kubernetes helps set up, grow, and manage containers over lots of computers.\n\nThis means PyTorch applications can handle big, complicated jobs better, grow more easily, and keep running smoothly. Developers can then put more energy into making their models, without worrying too much about the tech behind it.\n\n## üõ†Ô∏è Hands-on: Ray on Kubernetes\n\nOur setup consists of a Kubernetes cluster in Amazon EKS, which hosts our AI applications. The recommended method for installing Ray onto a Kubernetes cluster is through KubeRay, a Kubernetes Operator. The KubeRay Operator allows you to create and manage Ray clusters in a Kubernetes-native way by defining clusters as a custom resource called a RayCluster.\n\nThe installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob, and RayService as documented [here](https://docs.ray.io/en/latest/cluster/kubernetes/index.html).\n\nThis RayCluster resource describes the desired state of a Ray cluster. The KubeRay Operator manages the cluster lifecycle, autoscaling, and monitoring functions.\n\nHence, we use the KubeRay Operator for our Ray cluster installation.\n\n![kuberay operator](./kuberay-op.png)\n<div class=\"image-title\"><a href=\"https://medium.com/sage-ai/demystifying-the-process-of-building-a-ray-cluster-110c67914a99\">Source</a></div>\n\nKubeRay Operator managing the Ray Cluster lifecycle. Now let us deploy the infrastructure. Start by cloning the repo and change the working directory.\n\n```shell\ngit clone https://github.com/seifrajhi/data-on-eks\ncd data-on-eks/ai-ml/ray/terraform\n```\n\nNext, we can use the shell script `install.sh` to run the `terraform init` and `apply` commands.\n\nUpdate `variables.tf` to change the region. Also, we can update any other input variables or make any other changes to the terraform template.\n\n```shell\n./install.sh\n```\n\nAnd now, we can run the PyTorch benchmark.\n\nWe deploy a Ray Cluster with its own configuration for Karpenter workers. Different jobs can have different requirements for Ray Cluster such as a different version of Ray libraries or EC2 instance configuration such as making use of Spot market or GPU instances.\n\nTo deploy the Ray cluster run below commands:\n\n```shell\ncd examples/pytorch\nterraform init\nterraform plan\nterraform apply -auto-approve\n```\n\nOnce running, we can forward the port for the server:\n\n```shell\nkubectl port-forward service/pytorch-kuberay-head-svc -n pytorch 8266:8265\n```\n\nWe can then submit the job for PyTorch benchmark workload:\n\n```shell\npython job/pytorch_submit.py\n```\n\nYou can open [http://localhost:8266](http://localhost:8266) to monitor the progress of the PyTorch benchmark.\n\nThe `pytorch_submit.py` script is a benchmarking for evaluating the performance of training and tuning a PyTorch model on a distributed system using Ray.\n\nIt measures the time taken to train a model and to find the best hyperparameters through tuning, providing insights into the efficiency of distributed machine learning workflows.\n\n## üîö Key Takeaways\n\nKubernetes makes working with AI/ML simpler. It scales AI/ML models easily and keeps them running smoothly. With Kubernetes, Ray, and PyTorch work better in the cloud, making AI/ML systems more reliable and easier to manage.\n\n<br>\n<br>\n\n> üí° Thank you for Reading !! üôåüèªüòÅüìÉ, see you in the next blog.ü§ò  **_Until next time üéâ_**\n\n\nüöÄ Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**‚ôªÔ∏è LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**‚ôªÔ∏è X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ‚úåüèª**\n\n<h1 align=\"center\">üî∞ Keep Learning !! Keep Sharing !! üî∞</h1>\n\n**üìªüß° References:**\n\n- https://docs.ray.io/en/latest/cluster/kubernetes/index.html\n- https://awslabs.github.io/data-on-eks/docs/blueprints/ai-ml/\n- https://aws.amazon.com/blogs/opensource/scaling-ai-and-machine-learning-workloads-with-ray-on-aws\n- https://cloud.google.com/blog/products/containers-kubernetes/use-ray-on-kubernetes-with-kuberay\n\n**üìÖ Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":1348},"frontmatter":{"id":"2c77221c9d5636c2ea28a663","path":"/blog/data-on-kubernetes-ai-ml-5/","humanDate":"Oct 25, 2024","fullDate":"2024-10-25","title":"Data on Kubernetes: Part 5‚Ää-‚ÄäMaking AI/ML simpler","keywords":["Kubernetes","AI/ML","EKS","Model Deployment","Scalability"],"excerpt":"Explore how Kubernetes simplifies the deployment and scaling of AI/ML models on EKS.","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC5UlEQVR42g2Sa3MaBRSG+Ul+UdPxFqeaWm2bZDTtCG0DaZLlHtgCYWG5X8Ntl13uEEBImKRNqrYJCm0c/eTX/gQ/90887qcz884575z3OcekThdUxm/oX7ylPPyNeOOMROslcvOCaOtXop3XRLtXRHtXRIwq96+JDa6JH89Jj+fUL/+meP4XlVf/YB8uMHXP5wymJ2i9HjlFJVGuE8io+LN1vNkGQkzDkWxgj+l4Mg28GQ2xqBOqNZEaDcO8S+b0GHk6YPv4ElNRe8Hp7JxWZ0Sx3EbvTElme5RrQ+rtM4LxJkVtQlYbMZxdovYmVPpjSv0hzZMTEo06imHamox4pI4xFZQzqmqLTndCp39KsdYlEKtzmGlhe14ir/9CtNoztlMJl5sECwpSqY4jc0RWb1LqdBjMpjgKaTYKA0y51u9IaQ1PrMG+VGddKPDdfpY9uUpSaVPpDtEGI0L5GuGSSqnVMyJr3D+QeSLleZbOIdYqPNer2PRTTN7qGVapyKYvzheuFF/vxbj9VGJzXyZRaiKmFYObgjXWxpObIGYqPPDIfLonsuoKsh6OYEmG8WsVzNUZpp1ih5/EQx6EE3ySOeFWtMtdyx5fbuzy8T0rnz1yceuhD0/zP8KDD3z0pMiKkGLVc8TaQZUNKYU5LSGoeZ72DIYOpY+k19lOxlk12K1FSqzvurlnOeCHbR93bCG+soaxHf2Lvf6eb7wa39qNPnea7/1HbMkpHuei2Ap51nwtTMH+BeM3C5y5PGZJQgx4Mbvd3LU6uWOx8/lDJytbHlY2BW4/C7EVnPCjfcR9Ic+mI4AlFEFIpfArDcx5I3Lt4oaa8ZxCfIYzNiN2aLAKdTAH2my5qph9bczOBpYdnR3XkMf+K34Wr3HIL/Clp0TKxozy0rj4a/zqHJPcfYWoL7FnlwiRJbvBdwjJG+zxBfboEod4gzN+g8vQXPI7nOE/DP1P3NEF3sRbDhJLfPEl3pyhleb8Dz0ILAnLxu1pAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/11350ee12dbbab2a6cb82c52ec843057/c8e30/dok5.png","srcSet":"/static/11350ee12dbbab2a6cb82c52ec843057/f95e9/dok5.png 750w,\n/static/11350ee12dbbab2a6cb82c52ec843057/34087/dok5.png 1080w,\n/static/11350ee12dbbab2a6cb82c52ec843057/1c572/dok5.png 1366w,\n/static/11350ee12dbbab2a6cb82c52ec843057/c8e30/dok5.png 1600w","sizes":"100vw"},"sources":[{"srcSet":"/static/11350ee12dbbab2a6cb82c52ec843057/4e534/dok5.webp 750w,\n/static/11350ee12dbbab2a6cb82c52ec843057/e5dd4/dok5.webp 1080w,\n/static/11350ee12dbbab2a6cb82c52ec843057/1de46/dok5.webp 1366w,\n/static/11350ee12dbbab2a6cb82c52ec843057/289c2/dok5.webp 1600w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.565625}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/data-on-kubernetes-airflow-on-aws-3/","title":"Data on Kubernetes: Part 3‚Ää-‚ÄäManaging Workflows with Job Schedulers and Batch-Oriented Workflow Orchestrators","date":"2024-10-25 14:06:00"},"excerpt":"Automate and orchestrate data workflows in Kubernetes with Amazon managed Apache Airflow üî∑ Introduction Welcome back to this blog series on‚Ä¶"},"nextThought":{"frontmatter":{"path":"/blog/data-on-kubernetes-cloudnativepg-ceph-2/","title":"Data on Kubernetes: Part 2‚Ää-‚ÄäDeploying Databases in K8s with PostgreSQL, CloudNative-PG, and Ceph Rook on Amazon¬†EKS","date":"2024-10-25 13:06:00"},"excerpt":"Data persistence: running PostgreSQL on Amazon¬†EKS üéÜ Introduction PostgreSQL, the well-known open-source relational database, is a popular‚Ä¶"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}