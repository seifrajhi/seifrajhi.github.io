{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/Kubernetes-anti-patterns/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p><strong>A simple guide to avoid these Pitfalls 🕳🚶</strong></p>\n</blockquote>\n<h2 id=\"-introduction\" style=\"position:relative;\"><a href=\"#-introduction\" aria-label=\" introduction permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📚 Introduction</h2>\n<p>Are you new to Kubernetes or considering using it for your applications? While Kubernetes is an excellent set of tools for managing automatically scalable, highly available distributed cloud-native applications, there are common mistakes that many make.</p>\n<p>In this article, we will explore some of the most frequent pitfalls when using Kubernetes and provide tips on how to avoid them.</p>\n<blockquote>\n<p>📢 Announcement: I have just launched a GitHub repository dedicated to gathering resources, exercises, and labs to aid in learning Kubernetes from the ground up. This repository aims to provide practical exercises that guide users in deploying, managing, and scaling containerized applications using Kubernetes. Your feedback contributions are welcomed to enhance and improve the learning experience. 🎉 </p>\n<p>Link: <a href=\"https://github.com/seifrajhi/Kubernetes-practical-exercises-Hands-on\" target=\"_blank\" rel=\"noopener noreferrer\">https://github.com/seifrajhi/Kubernetes-practical-exercises-Hands-on</a></p>\n</blockquote>\n<h2 id=\"-not-setting-resource-requests\" style=\"position:relative;\"><a href=\"#-not-setting-resource-requests\" aria-label=\" not setting resource requests permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🙅 Not Setting Resource Requests</h2>\n<p>This definitely deserves the most attention and first place in this list.\nCPU requests are usually either not set or set very low (so that we can fit a lot of pods on each node) and nodes are thus overcommited. In times of high demand the CPUs of the node are fully utilized and our workload is getting only \"what it had requested\" and gets CPU throttled, causing increased application latency, timeouts, etc.</p>\n<p>BestEffort (please don't):</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span></code></pre></div>\n<p>very low CPU (please don't):</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\">    <span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"1m\"</span></code></pre></div>\n<p>On the other hand, having a CPU limit can unnecessarily throttle pods even if the node's CPU is not fully utilized which again can cause increased latency.\nThere is an open discussion around CPU CFS quota in Linux kernel and cpu throttling based on set CPU limits and turning off the CFS quota.</p>\n<p>CPU limits can cause more problems than they solve. See more in the link below.\nMemory overcommiting can get you in more trouble. Reaching a CPU limit results in throttling, reaching memory limit will get your pod killed.</p>\n<p>Ever seen OOMkill? Yep, that's the one we are talking about. Want to minimize how often it can happen? Don't overcommit your memory and use Guaranteed QoS (Quality of Service) setting memory request equal to limit like in the example below. Read more about the topic in <a href=\"https://www.slideshare.net/try_except_/optimizing-kubernetes-resource-requestslimits-for-costefficiency-and-latency-highload\" target=\"_blank\" rel=\"noopener noreferrer\">Henning Jacobs' (Zalando) presentation</a>.</p>\n<p>Burstable (more likely to get OOMkilled more often):</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"128Mi\"</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"500m\"</span>\n      <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"256Mi\"</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span></code></pre></div>\n<p>Guaranteed:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">resources</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">requests</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"128Mi\"</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n    <span class=\"token key atrule\">limits</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">memory</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"128Mi\"</span>\n        <span class=\"token key atrule\">cpu</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span></code></pre></div>\n<p>So what can help you when setting resources?</p>\n<p>You can see the current cpu and memory usage of pods (and containers in them) using metrics-server. Chances are, you are already running it. Simply run these:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">kubectl <span class=\"token function\">top</span> pods \nkubectl <span class=\"token function\">top</span> pods <span class=\"token parameter variable\">--containers</span> \nkubectl <span class=\"token function\">top</span> nodes</code></pre></div>\n<p>However these show just the current usage. That is great to get the rough idea about the numbers but you end up wanting to see these usage metrics in time (to answer questions like: what was the cpu usage in peak, yesterday morning, etc.). For that, you can use Prometheus, DataDog and many others. They just ingest the metrics from metrics-server and store them, then you can query &#x26; graph them.</p>\n<p><a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">VerticalPodAutoscaler</a> can help you automate away this manual process - looking at cpu/mem usage in time and setting new requests and limits based on that all over again.</p>\n<blockquote>\n<p>Using effectively your compute is not an easy task. It is like playing tetris all the time. If you find yourself paying a lot for compute while having low average utilization (say ~10%), you might want to check AWS Fargate or Virtual Kubelet based products that make use of more of a serverless/pay-per-usage billing model that might be cheaper for you.</p>\n</blockquote>\n<h2 id=\"-omit-healthchecks\" style=\"position:relative;\"><a href=\"#-omit-healthchecks\" aria-label=\" omit healthchecks permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🚫 Omit Health Checks</h2>\n<p>When deploying your services to Kubernetes, health checks play an important role in maintaining your services.</p>\n<p>Health checks are highly under-utilized 😿 in the Kubernetes environment. Through health checks, you keep an eye on the health of the pods and their containers.</p>\n<p>Kubernetes has three main tools you can use for health checks:\nLiveness Check allows Kubernetes to check whether your app is alive or not. The Kubelet agent running on each node uses liveness probes to ensure that the containers are running as expected. Readiness checks run during the entire lifecycle of the container. Kubernetes uses this probe to know when the container is ready to accept traffic. The startup probe determines when the container application has been started successfully. If the startup check fails, the pod is restarted.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 30%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA80lEQVR42lVRSW7DMBDTOxpvWm0ttuw4LdD0/+9ih0oNpAdC1ELOcKS6fkTXDbgJ+mESaGjjYKxvIB9GjVw2OB9gXYAPi/AZS8yYl4RJW9CHeqVFxEsfZmjr4OcFPGsPxGicTBOt2y6oWFJpoHHd7/h+/mCre3tPKCfVUl5R1ipmUXhBFEHKm6wJsazYjweO40StByZjBa+uacBkPkScj6+mV9chQc6IjDCMpnVKA4pZlCOgKOYMK6nYZZA9RxAYXbSKuSkgLs6H5FcMFvu49Q0syBFw5R2Lmz/eIr8+4j84t/dPocH9/Gwfw44Yn11d9+/aX4uaqh4evmdWAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Kubernetes Health Checks\" title=\"\" src=\"/static/27608c29d8c94ea4fdffcab0a2345fb7/c5bb3/Health-checks-k8s.png\" srcset=\"/static/27608c29d8c94ea4fdffcab0a2345fb7/04472/Health-checks-k8s.png 170w,\n/static/27608c29d8c94ea4fdffcab0a2345fb7/9f933/Health-checks-k8s.png 340w,\n/static/27608c29d8c94ea4fdffcab0a2345fb7/c5bb3/Health-checks-k8s.png 680w,\n/static/27608c29d8c94ea4fdffcab0a2345fb7/b12f7/Health-checks-k8s.png 1020w,\n/static/27608c29d8c94ea4fdffcab0a2345fb7/b5a09/Health-checks-k8s.png 1360w,\n/static/27608c29d8c94ea4fdffcab0a2345fb7/29007/Health-checks-k8s.png 1600w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<h2 id=\"-using-the-latesttag\" style=\"position:relative;\"><a href=\"#-using-the-latesttag\" aria-label=\" using the latesttag permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🛑 Using the Latest Tag</h2>\n<p>This one is a classic. The latest is not descriptive and hard to work with. The Kubernetes docs are very clear on using docker images:latest tags in a production environment:</p>\n<p>You should avoid using the :latest tag when deploying containers in production, as it makes it difficult to track which version of the image is running and hard to roll back.</p>\n<p>I feel like lately we don't see this very often as a lot of us got burned too many times and we stopped using <code class=\"language-text\">**:latest**</code> and everyone started to pin the versions. Yay!</p>\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ecr-now-supports-immutable-image-tags/\" target=\"_blank\" rel=\"noopener noreferrer\">ECR has a great feature of tag immutability</a>, definitely worth checking out.</p>\n<h2 id=\"-overprivileged-containers\" style=\"position:relative;\"><a href=\"#-overprivileged-containers\" aria-label=\" overprivileged containers permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📛 Overprivileged Containers</h2>\n<p>Overprivileged containers are containers that have been given too many privileges, such as access to resources that are not accessible in ordinary containers.\nThis is a common mistake that developers make while using Kubernetes, and it can create security risks.\nFor example, running a Docker daemon inside a Docker container is an example of a privileged container, which is not necessarily secure.\nTo avoid this, it is recommended to avoid giving <code class=\"language-text\">**CAP_SYS_ADMIN**</code> capability to your container, as it comprises more than 25% of all kernel vulnerabilities.\nAdditionally, it is important to avoid giving full privileges to the container and host file system privileges given to the container. This means the container can be used to compromise the whole host by replacing binaries with malicious ones.\nTo prevent overprivileged containers, it is essential to carefully configure permission settings and never run processes with higher privileges than they need.\nIt is also important to use monitoring and logging to detect and resolve issues.</p>\n<h2 id=\"-lack-of-monitoring-andlogging\" style=\"position:relative;\"><a href=\"#-lack-of-monitoring-andlogging\" aria-label=\" lack of monitoring andlogging permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>👀 Lack of Monitoring and Logging</h2>\n<p>The lack of monitoring and logging in a Kubernetes environment can be detrimental to its security and overall performance. Inadequate logging and monitoring present challenges during incident investigation and response efforts, making it difficult to detect and resolve issues effectively.</p>\n<p>One common pitfall is being unable to find failure points in the Kubernetes platform and applications due to a lack of relevant logs or metrics.\nTo address this, it is essential to set up proper monitoring and logging tools, such as Prometheus, Grafana, Fluentd, and Jaeger, to collect, analyze, and visualize metrics, logs, and traces, gaining insights into the performance and health of the Kubernetes environment.</p>\n<p>By implementing robust monitoring and logging practices, organizations can effectively correlate information, gain deeper insights, and overcome the challenges associated with the d</p>\n<h2 id=\"-default-namespace-for-allobjects\" style=\"position:relative;\"><a href=\"#-default-namespace-for-allobjects\" aria-label=\" default namespace for allobjects permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>😵 Default Namespace for all Objects</h2>\n<p>Using the default namespace for all objects in Kubernetes can lead to organizational and management challenges.</p>\n<p>The default namespace is where services and apps are created by default, and it is also the active namespace unless explicitly specified.</p>\n<p>Relying solely on the default namespace can result in a lack of isolation and organization for different components or teams within the cluster. This can lead to difficulties in resource management, access control, and visibility. To avoid this, it is recommended to create custom namespaces for different projects, teams, or applications, allowing for better organization, resource allocation, and access control within the Kubernetes cluster.</p>\n<p>By using multiple namespaces, users can effectively compartmentalize and manage their resources, enhancing the overall operational efficiency and security of the Kubernetes environment.</p>\n<h2 id=\"-missing-security-configurations\" style=\"position:relative;\"><a href=\"#-missing-security-configurations\" aria-label=\" missing security configurations permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>➖ Missing Security Configurations</h2>\n<p>When deploying your application, you should always keep security in mind. So what are some of the most important things to consider when it comes to security? For example, using an endpoint accessible outside of your cluster, not securing your secrets, not considering how to run privileged containers, etc. safely.</p>\n<p>Kubernetes security is an integral part of any Kubernetes deployment. Security challenges include:</p>\n<ul>\n<li><strong>Authorization</strong>: Authentication and authorization are essential for controlling access to resources in a Kubernetes cluster.</li>\n<li><strong>Networking</strong>: Kubernetes networking involves managing overlay networks and service endpoints to ensure that traffic between containers is routed securely within the cluster.</li>\n<li><strong>Storage</strong>: Securing storage in a cluster consists of ensuring that data cannot be accessed by unauthorized users or processes.</li>\n</ul>\n<p>The Kubernetes API server has a REST interface that provides access to all the information stored. This means that users can access any information stored in the API by simply sending HTTP requests to it. To protect this data from unauthenticated users, you need to configure authentication for the API server using supported methods like username/password or token-based authentication.</p>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 680px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 54.11764705882353%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB40lEQVR42pVSTUtVURS9P8AcS+Om0qRB0KBRs35DYpSTCMIgoYiIZk0lgiAp0ydhZUURNFKfSZo+QbAvLS+V3/ksvd5z7/larX3PewjVwHdgcfbZ5+x11v6I0MAyHqjmHtXMw/ng8zXU7Wg/RPXgwa8WV6Y0LhPPYlP4Em472uMXP8osCRN+u02H7BJoaxB7lz55LLus2x8M2kd1gQfzBtYBm1S7oRyWkvA2Svk40YCy/yoTot/8Oa0R3vtscGlS4+KExlAcAoRkTXl8SxxSUdi3YNA9Z/Bo0WJ42eJ6xeDGjMH4msPdT6Y4985bGEq+89HgTFnjNBUOfAmEOVWuK+AHFYodHR5SODSocOxFhs63Gk33UxzoTXF1WuPI0wzNtI8+VywLMLBg0fVOo2syKJSstnKpH7CRSmlIePxlhtYnCidfZ7hW0WgpKRwkbs4ayF1LKcWJVxmDfZFyx1iOs+UcD9mglATS8WoGrO764hxVfjqUVxxmNx3iHY+xVYc3xHemIHejvJvhnWXKPSzB+XGNc4SUSJYQ1hUWhPsZm/qc9VBhJxtygaV5HO8RisJ1FZob2b9G5X92rcm49d6gbSTHqeG8UOt9IJSGxNuBuCGFUooSG9NPTHAKwmihGK2tPKT8B85IMEWoSHZcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Kubernetes API Server\" title=\"\" src=\"/static/4c9f8fed9814f19dc97c8651065af00f/c5bb3/Kubernetes-API-server.png\" srcset=\"/static/4c9f8fed9814f19dc97c8651065af00f/04472/Kubernetes-API-server.png 170w,\n/static/4c9f8fed9814f19dc97c8651065af00f/9f933/Kubernetes-API-server.png 340w,\n/static/4c9f8fed9814f19dc97c8651065af00f/c5bb3/Kubernetes-API-server.png 680w,\n/static/4c9f8fed9814f19dc97c8651065af00f/b12f7/Kubernetes-API-server.png 1020w,\n/static/4c9f8fed9814f19dc97c8651065af00f/2bef9/Kubernetes-API-server.png 1024w\" sizes=\"(max-width: 680px) 100vw, 680px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n    </span>\n          </p>\n<p>It's not just about securing the cluster itself but also the secrets and configurations on it. To protect the cluster from vulnerabilities, you will need to configure a set of security controls on it.</p>\n<p>One such robust security control is securing a Kubernetes cluster with RBAC: Role-Based Access Control can be used to secure Kubernetes clusters by limiting access to resources based on roles assigned to users. These roles can be configured as \"admin\" or \"operator.\"</p>\n<p>The admin role has full access rights, while the operator role has limited rights over resources within the cluster. We can control and manage anyone getting access to the cluster by doing this.</p>\n<h2 id=\"-missing-poddisruptionbudget\" style=\"position:relative;\"><a href=\"#-missing-poddisruptionbudget\" aria-label=\" missing poddisruptionbudget permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>🙅 Missing poddisruptionbudget</h2>\n<p>You run the production workload on kubernetes. Your nodes &#x26; cluster have to be upgraded, or decommissioned, from time to time. PodDisruptionBudget (pdb) is sort of an API for service guarantees between cluster administrators and cluster-users.</p>\n<p>Make sure to create <code class=\"language-text\">pdb</code> to avoid unnecessary service outages due to draining nodes.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> policy/v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> PodDisruptionBudget\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> db<span class=\"token punctuation\">-</span>pdb\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">minAvailable</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span>\n  <span class=\"token key atrule\">selector</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">matchLabels</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> database</code></pre></div>\n<p>With this as a cluster-user you tell the cluster-administrators: \"hey, I have this database service here and no matter what you have to do, I'd like to have at least 2 replicas always available\".</p>\n<h2 id=\"-self-anti-affinities-forpods\" style=\"position:relative;\"><a href=\"#-self-anti-affinities-forpods\" aria-label=\" self anti affinities forpods permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>😠 Self anti-affinities for pods</h2>\n<p>Running e.g. 3 pod replicas of some deployment, node goes down and all the replicas with it. Huh? All the replicas were running on one node? Wasn't Kubernetes supposed to be magical and provide HA?!\nYou can't expect a kubernetes scheduler to enforce <code class=\"language-text\">anti-affinites</code> for your pods. You have to define them explicitly.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token punctuation\">...</span><span class=\"token punctuation\">...</span>\n      <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">app</span><span class=\"token punctuation\">:</span> db\n<span class=\"token punctuation\">...</span><span class=\"token punctuation\">...</span>\n      <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">podAntiAffinity</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">labelSelector</span><span class=\"token punctuation\">:</span>\n                <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n                  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"app\"</span>\n                    <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n                    <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token punctuation\">-</span> db\n              <span class=\"token key atrule\">topologyKey</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"kubernetes.io/hostname\"</span></code></pre></div>\n<p>That's it. This will make sure the pods will be scheduled to different nodes (this is being checked only at scheduling time, not at execution time, hence the <code class=\"language-text\">requiredDuringSchedulingIgnoredDuringExecution</code>).\nWe are talking about podAntiAffinity on different node names - <code class=\"language-text\">topologyKey: \"kubernetes.io/hostname\"</code> - not different availability zones. If you really need proper HA, dig a bit deeper into this topic.</p>\n<h2 id=\"️loadbalancer-for-every-httpservice\" style=\"position:relative;\"><a href=\"#%EF%B8%8Floadbalancer-for-every-httpservice\" aria-label=\"️loadbalancer for every httpservice permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>⚖️LoadBalancer for every HTTP service</h2>\n<p>Chances are you have more http services in your cluster that you'd like to expose to the outside world.</p>\n<p>If you expose the kubernetes service as a <code class=\"language-text\">type: LoadBalancer</code>, its controller (vendor specific) will provision and reconcile an external LoadBalancer and those resources might get expensive (external static IPv4 address, compute, per-second pricing…) as you create many of them.</p>\n<p>In that case, sharing one external loadbalancer might make more sense and you expose your services as <code class=\"language-text\">type: NodePort</code>. Or yet better, deploying something like <code class=\"language-text\">nginx-ingress-controller</code> (or <code class=\"language-text\">traefik</code> or <code class=\"language-text\">Istio</code>) being the single NodePort endpoint exposed to the external loadbalancer and routing the traffic in the cluster based on kubernetes ingress resources.</p>\n<p>The other in-cluster (micro)services that talk to each other can talk through ClusterIP services and out-of-box DNS service discovery.</p>\n<blockquote>\n<p>Be careful about not using their public DNS/IPs as it could affect their latency and cloud cost.</p>\n</blockquote>\n<h2 id=\"-non-kubernetes-aware-cluster-autoscaling\" style=\"position:relative;\"><a href=\"#-non-kubernetes-aware-cluster-autoscaling\" aria-label=\" non kubernetes aware cluster autoscaling permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>😱 Non-kubernetes-aware cluster autoscaling</h2>\n<p>When adding and removing nodes to/from the cluster, you shouldn't consider some simple metrics like a CPU utilization of those nodes. When scheduling pods, you decide based on a lot of scheduling constraints like pod &#x26; node affinities, taints, and tolerations, resource requests, QoS, etc.</p>\n<p>Having an external autoscaler that does not understand these constraints might be troublesome.\nImagine there is a new pod to be scheduled but all of the CPU available is requested and the pod is stuck in the Pending state. External autoscaler sees the average CPU currently used (not requested) and won't scale out (will not add another node). The pod won't be scheduled.</p>\n<p>Scaling-in (removing a node from the cluster) is always harder. Imagine you have a stateful pod (with persistent volume attached) and as persistent volumes are usually resources that belong to a specific availability zone and are not replicated in the region, your custom autoscaler removes a node with this pod on it and scheduler cannot schedule it onto a different node as it is very limited by the only availability zone with your persistent disk in it. Pod is again stuck in Pending state.</p>\n<blockquote>\n<p>The community is widely using <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\" target=\"_blank\" rel=\"noopener noreferrer\">cluster-autoscaler</a> which runs in your cluster and is integrated with most major public cloud vendors APIs, understands all these constraints and would scale-out in the mentioned cases. It will also figure out if it can gracefully scale-in without affecting any constraints we have set and saves you money on compute.</p>\n</blockquote>\n<h2 id=\"-conclusion\" style=\"position:relative;\"><a href=\"#-conclusion\" aria-label=\" conclusion permalink\" class=\"anchor before\"><svg class=\"anchor-icon\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>📌 Conclusion</h2>\n<p>In conclusion, Kubernetes is a powerful tool for managing containerized applications, but it comes with its own set of challenges. To avoid common mistakes and pitfalls, it is essential to pay close attention to your interactions with Kubernetes and understand the differences between how it interacts with your deployed services.</p>\n<p>Don't expect everything to work automagically, and invest some time in making your app cloud-native. By avoiding these mistakes, you can work efficiently with your Kubernetes deployments and enhance the stability, performance, and security of your Kubernetes environment.</p>\n<p><strong>Until next time 🎉</strong></p>\n<p><br><br></p>\n<blockquote>\n<p>💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  <em><strong>Until next time 🎉</strong></em></p>\n</blockquote>\n<p>🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:</p>\n<p><strong>♻️ LinkedIn:</strong> <a href=\"https://www.linkedin.com/in/rajhi-saif/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.linkedin.com/in/rajhi-saif/</a></p>\n<p><strong>♻️ X/Twitter:</strong> <a href=\"https://x.com/rajhisaifeddine\" target=\"_blank\" rel=\"noopener noreferrer\">https://x.com/rajhisaifeddine</a></p>\n<p><strong>The end ✌🏻</strong></p>\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n<p><strong>📅 Stay updated</strong></p>\n<p>Subscribe to our newsletter for more insights on AWS cloud computing and containers.</p>","timeToRead":9,"rawMarkdownBody":"> **A simple guide to avoid these Pitfalls 🕳🚶**\n\n## 📚 Introduction\n\nAre you new to Kubernetes or considering using it for your applications? While Kubernetes is an excellent set of tools for managing automatically scalable, highly available distributed cloud-native applications, there are common mistakes that many make.\n\nIn this article, we will explore some of the most frequent pitfalls when using Kubernetes and provide tips on how to avoid them.\n\n\n> 📢 Announcement: I have just launched a GitHub repository dedicated to gathering resources, exercises, and labs to aid in learning Kubernetes from the ground up. This repository aims to provide practical exercises that guide users in deploying, managing, and scaling containerized applications using Kubernetes. Your feedback contributions are welcomed to enhance and improve the learning experience. 🎉 \n>\n> Link: https://github.com/seifrajhi/Kubernetes-practical-exercises-Hands-on\n\n\n## 🙅 Not Setting Resource Requests\n\nThis definitely deserves the most attention and first place in this list.\nCPU requests are usually either not set or set very low (so that we can fit a lot of pods on each node) and nodes are thus overcommited. In times of high demand the CPUs of the node are fully utilized and our workload is getting only \"what it had requested\" and gets CPU throttled, causing increased application latency, timeouts, etc.\n\nBestEffort (please don't):\n\n```yaml\nresources: {}\n```\n\nvery low CPU (please don't):\n```yaml\n    resources:\n      requests:\n        cpu: \"1m\"\n```\n\nOn the other hand, having a CPU limit can unnecessarily throttle pods even if the node's CPU is not fully utilized which again can cause increased latency.\nThere is an open discussion around CPU CFS quota in Linux kernel and cpu throttling based on set CPU limits and turning off the CFS quota.\n\nCPU limits can cause more problems than they solve. See more in the link below.\nMemory overcommiting can get you in more trouble. Reaching a CPU limit results in throttling, reaching memory limit will get your pod killed.\n\nEver seen OOMkill? Yep, that's the one we are talking about. Want to minimize how often it can happen? Don't overcommit your memory and use Guaranteed QoS (Quality of Service) setting memory request equal to limit like in the example below. Read more about the topic in [Henning Jacobs' (Zalando) presentation](https://www.slideshare.net/try_except_/optimizing-kubernetes-resource-requestslimits-for-costefficiency-and-latency-highload).\n\nBurstable (more likely to get OOMkilled more often):\n\n```yaml\nresources:\n      requests:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: 2\n```\n\nGuaranteed:\n\n```yaml\nresources:\n    requests:\n        memory: \"128Mi\"\n        cpu: 2\n    limits:\n        memory: \"128Mi\"\n        cpu: 2\n```\n\nSo what can help you when setting resources?\n\nYou can see the current cpu and memory usage of pods (and containers in them) using metrics-server. Chances are, you are already running it. Simply run these:\n\n```shell\nkubectl top pods \nkubectl top pods --containers \nkubectl top nodes\n```\n\nHowever these show just the current usage. That is great to get the rough idea about the numbers but you end up wanting to see these usage metrics in time (to answer questions like: what was the cpu usage in peak, yesterday morning, etc.). For that, you can use Prometheus, DataDog and many others. They just ingest the metrics from metrics-server and store them, then you can query & graph them.\n\n[VerticalPodAutoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler) can help you automate away this manual process - looking at cpu/mem usage in time and setting new requests and limits based on that all over again.\n\n>Using effectively your compute is not an easy task. It is like playing tetris all the time. If you find yourself paying a lot for compute while having low average utilization (say ~10%), you might want to check AWS Fargate or Virtual Kubelet based products that make use of more of a serverless/pay-per-usage billing model that might be cheaper for you.\n\n## 🚫 Omit Health Checks\nWhen deploying your services to Kubernetes, health checks play an important role in maintaining your services.\n\nHealth checks are highly under-utilized 😿 in the Kubernetes environment. Through health checks, you keep an eye on the health of the pods and their containers.\n\nKubernetes has three main tools you can use for health checks:\nLiveness Check allows Kubernetes to check whether your app is alive or not. The Kubelet agent running on each node uses liveness probes to ensure that the containers are running as expected. Readiness checks run during the entire lifecycle of the container. Kubernetes uses this probe to know when the container is ready to accept traffic. The startup probe determines when the container application has been started successfully. If the startup check fails, the pod is restarted.\n\n![Kubernetes Health Checks](./Health-checks-k8s.png)\n\n## 🛑 Using the Latest Tag\n\nThis one is a classic. The latest is not descriptive and hard to work with. The Kubernetes docs are very clear on using docker images:latest tags in a production environment:\n\nYou should avoid using the :latest tag when deploying containers in production, as it makes it difficult to track which version of the image is running and hard to roll back.\n\nI feel like lately we don't see this very often as a lot of us got burned too many times and we stopped using `**:latest**` and everyone started to pin the versions. Yay!\n\n[ECR has a great feature of tag immutability](https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ecr-now-supports-immutable-image-tags/), definitely worth checking out.\n\n## 📛 Overprivileged Containers\nOverprivileged containers are containers that have been given too many privileges, such as access to resources that are not accessible in ordinary containers.\nThis is a common mistake that developers make while using Kubernetes, and it can create security risks.\nFor example, running a Docker daemon inside a Docker container is an example of a privileged container, which is not necessarily secure.\nTo avoid this, it is recommended to avoid giving `**CAP_SYS_ADMIN**` capability to your container, as it comprises more than 25% of all kernel vulnerabilities.\nAdditionally, it is important to avoid giving full privileges to the container and host file system privileges given to the container. This means the container can be used to compromise the whole host by replacing binaries with malicious ones.\nTo prevent overprivileged containers, it is essential to carefully configure permission settings and never run processes with higher privileges than they need.\nIt is also important to use monitoring and logging to detect and resolve issues.\n\n\n## 👀 Lack of Monitoring and Logging\n\nThe lack of monitoring and logging in a Kubernetes environment can be detrimental to its security and overall performance. Inadequate logging and monitoring present challenges during incident investigation and response efforts, making it difficult to detect and resolve issues effectively.\n\nOne common pitfall is being unable to find failure points in the Kubernetes platform and applications due to a lack of relevant logs or metrics.\nTo address this, it is essential to set up proper monitoring and logging tools, such as Prometheus, Grafana, Fluentd, and Jaeger, to collect, analyze, and visualize metrics, logs, and traces, gaining insights into the performance and health of the Kubernetes environment.\n\n\nBy implementing robust monitoring and logging practices, organizations can effectively correlate information, gain deeper insights, and overcome the challenges associated with the d\n\n## 😵 Default Namespace for all Objects\n\nUsing the default namespace for all objects in Kubernetes can lead to organizational and management challenges.\n\nThe default namespace is where services and apps are created by default, and it is also the active namespace unless explicitly specified.\n\nRelying solely on the default namespace can result in a lack of isolation and organization for different components or teams within the cluster. This can lead to difficulties in resource management, access control, and visibility. To avoid this, it is recommended to create custom namespaces for different projects, teams, or applications, allowing for better organization, resource allocation, and access control within the Kubernetes cluster.\n\nBy using multiple namespaces, users can effectively compartmentalize and manage their resources, enhancing the overall operational efficiency and security of the Kubernetes environment.\n\n\n## ➖ Missing Security Configurations\n\nWhen deploying your application, you should always keep security in mind. So what are some of the most important things to consider when it comes to security? For example, using an endpoint accessible outside of your cluster, not securing your secrets, not considering how to run privileged containers, etc. safely.\n\nKubernetes security is an integral part of any Kubernetes deployment. Security challenges include:\n\n- **Authorization**: Authentication and authorization are essential for controlling access to resources in a Kubernetes cluster.\n- **Networking**: Kubernetes networking involves managing overlay networks and service endpoints to ensure that traffic between containers is routed securely within the cluster.\n- **Storage**: Securing storage in a cluster consists of ensuring that data cannot be accessed by unauthorized users or processes.\n\nThe Kubernetes API server has a REST interface that provides access to all the information stored. This means that users can access any information stored in the API by simply sending HTTP requests to it. To protect this data from unauthenticated users, you need to configure authentication for the API server using supported methods like username/password or token-based authentication.\n\n![Kubernetes API Server](./Kubernetes-API-server.png)\n\nIt's not just about securing the cluster itself but also the secrets and configurations on it. To protect the cluster from vulnerabilities, you will need to configure a set of security controls on it.\n\nOne such robust security control is securing a Kubernetes cluster with RBAC: Role-Based Access Control can be used to secure Kubernetes clusters by limiting access to resources based on roles assigned to users. These roles can be configured as \"admin\" or \"operator.\"\n\nThe admin role has full access rights, while the operator role has limited rights over resources within the cluster. We can control and manage anyone getting access to the cluster by doing this.\n\n## 🙅 Missing poddisruptionbudget\n\nYou run the production workload on kubernetes. Your nodes & cluster have to be upgraded, or decommissioned, from time to time. PodDisruptionBudget (pdb) is sort of an API for service guarantees between cluster administrators and cluster-users.\n\nMake sure to create `pdb` to avoid unnecessary service outages due to draining nodes.\n\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: db-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: database\n```\n\nWith this as a cluster-user you tell the cluster-administrators: \"hey, I have this database service here and no matter what you have to do, I'd like to have at least 2 replicas always available\".\n\n## 😠 Self anti-affinities for pods\n\nRunning e.g. 3 pod replicas of some deployment, node goes down and all the replicas with it. Huh? All the replicas were running on one node? Wasn't Kubernetes supposed to be magical and provide HA?!\nYou can't expect a kubernetes scheduler to enforce `anti-affinites` for your pods. You have to define them explicitly.\n\n```yaml\n......\n      labels:\n        app: db\n......\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                  - key: \"app\"\n                    operator: In\n                    values:\n                    - db\n              topologyKey: \"kubernetes.io/hostname\"\n```\n\nThat's it. This will make sure the pods will be scheduled to different nodes (this is being checked only at scheduling time, not at execution time, hence the `requiredDuringSchedulingIgnoredDuringExecution`).\nWe are talking about podAntiAffinity on different node names - `topologyKey: \"kubernetes.io/hostname\"` - not different availability zones. If you really need proper HA, dig a bit deeper into this topic.\n\n## ⚖️LoadBalancer for every HTTP service\n\nChances are you have more http services in your cluster that you'd like to expose to the outside world.\n\nIf you expose the kubernetes service as a `type: LoadBalancer`, its controller (vendor specific) will provision and reconcile an external LoadBalancer and those resources might get expensive (external static IPv4 address, compute, per-second pricing…) as you create many of them.\n\nIn that case, sharing one external loadbalancer might make more sense and you expose your services as `type: NodePort`. Or yet better, deploying something like `nginx-ingress-controller` (or `traefik` or `Istio`) being the single NodePort endpoint exposed to the external loadbalancer and routing the traffic in the cluster based on kubernetes ingress resources.\n\nThe other in-cluster (micro)services that talk to each other can talk through ClusterIP services and out-of-box DNS service discovery.\n\n> Be careful about not using their public DNS/IPs as it could affect their latency and cloud cost.\n\n\n## 😱 Non-kubernetes-aware cluster autoscaling\n\nWhen adding and removing nodes to/from the cluster, you shouldn't consider some simple metrics like a CPU utilization of those nodes. When scheduling pods, you decide based on a lot of scheduling constraints like pod & node affinities, taints, and tolerations, resource requests, QoS, etc. \n\nHaving an external autoscaler that does not understand these constraints might be troublesome.\nImagine there is a new pod to be scheduled but all of the CPU available is requested and the pod is stuck in the Pending state. External autoscaler sees the average CPU currently used (not requested) and won't scale out (will not add another node). The pod won't be scheduled.\n\nScaling-in (removing a node from the cluster) is always harder. Imagine you have a stateful pod (with persistent volume attached) and as persistent volumes are usually resources that belong to a specific availability zone and are not replicated in the region, your custom autoscaler removes a node with this pod on it and scheduler cannot schedule it onto a different node as it is very limited by the only availability zone with your persistent disk in it. Pod is again stuck in Pending state.\n\n> The community is widely using [cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) which runs in your cluster and is integrated with most major public cloud vendors APIs, understands all these constraints and would scale-out in the mentioned cases. It will also figure out if it can gracefully scale-in without affecting any constraints we have set and saves you money on compute.\n\n## 📌 Conclusion\n\nIn conclusion, Kubernetes is a powerful tool for managing containerized applications, but it comes with its own set of challenges. To avoid common mistakes and pitfalls, it is essential to pay close attention to your interactions with Kubernetes and understand the differences between how it interacts with your deployed services.\n\nDon't expect everything to work automagically, and invest some time in making your app cloud-native. By avoiding these mistakes, you can work efficiently with your Kubernetes deployments and enhance the stability, performance, and security of your Kubernetes environment.\n\n**Until next time 🎉**\n\n<br><br>\n\n> 💡 Thank you for Reading !! 🙌🏻😁📃, see you in the next blog.🤘  _**Until next time 🎉**_\n\n🚀 Thank you for sticking up till the end. If you have any questions/feedback regarding this blog feel free to connect with me:\n\n**♻️ LinkedIn:** https://www.linkedin.com/in/rajhi-saif/\n\n**♻️ X/Twitter:** https://x.com/rajhisaifeddine\n\n**The end ✌🏻**\n\n<h1 align=\"center\">🔰 Keep Learning !! Keep Sharing !! 🔰</h1>\n\n**📅 Stay updated**\n\nSubscribe to our newsletter for more insights on AWS cloud computing and containers.\n","wordCount":{"words":2270},"frontmatter":{"id":"54e37b96a737ae61cbad4abe","path":"/blog/Kubernetes-anti-patterns/","humanDate":"Sep 29, 2024","fullDate":"2024-09-29","title":"Most common mistakes to avoid when using Kubernetes: Anti-Patterns ☸️","keywords":["Open source","Kubernetes","Best practices","AWS EKS","Cloud Native","Containers"],"excerpt":"Kubernetes, Anti-Patterns, Cloud Native, Best practices","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACs0lEQVR42lWS20+ScRjH+VdcHMVA48w6XDQvWidFAZEEhfKQOWkd7MIUFBA84pEID2nmygtLs+aFXbSZra3Z5tZqbQnyCkptKdG6/fbwgptdfPf83r17Pvt+n+fhCPQ9OFbWC77OA37VOPgV45AYfSis7IayJkgKQWwaAs80DIFpkP4HcMz6EAJjP4ocb6B2M1B3RkhRVpw8axh5ldRU9RLKxm/QNjMQV6/j9M1FCGxhSK9NQlY1gnxSQeMsBJYxyB2zkN9dg9obZ2GqQ3VEwOFWDKKwbg0lrjScjw8QXE6hpi+FM7d2oGpcgLJ2HEXmAE42TUNePwluwwzkbZvQegjWkXWmYh3mgAWmAfCtmyxs9VMaKx//YHE9heKWn1DUvYLU0g+xeQga2zBk1lHIWj5knVGzJhfzqDhSSxjq5gQmVlJYep/Gu89/Mf82BUcoDaF5FUK9FzydHwU0P03rBrRdCQIRzB39X4dAETmU2jdg9O5jYe2AlEJg4QDKpj1ILM9otgGI7VOQ3tuEonMHJ9q3IGuPQOY8IvpWuLLxORJTN8RXglDd+A695zf07l8417qPfDPFNXlReHUKWtdXVE8mcWFgG8bgDi4NxnAxEMNlqmUjDKvinm2aIQHFhl6I9D6K9AA8wzy4JKHxCYQ6L0FDUNz/glM+Bref7uLO/C48y0m4l5K4PpOA80US7c/30P36B+oexVHUllmKoQ+8Uj/NyQd+SQcEBBeVeyCxTUPlpDPyMlBSnIwL4xiD8tEYWy3hHZhDDPRj5DbjdJjBWf921qGwzA+RoQcCgnNLu3DcPktD3oLGw7DD1rqzUWvJRUY6arZNxOGYS6BhJo4aetdPJ3B+IAaOUE+RdX3gZ2AZsH2OvSuNO8Zu8/Ac5K4oLSDKLkHuiuTe0dxiqLZHKUkU/wB11Ce1ctI6KwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/e70e4/Kubernetes-anti-patterns.png","srcSet":"/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/24579/Kubernetes-anti-patterns.png 750w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/f8e01/Kubernetes-anti-patterns.png 1080w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/22914/Kubernetes-anti-patterns.png 1366w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/e70e4/Kubernetes-anti-patterns.png 1472w","sizes":"100vw"},"sources":[{"srcSet":"/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/ee3fa/Kubernetes-anti-patterns.webp 750w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/3309f/Kubernetes-anti-patterns.webp 1080w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/6ed74/Kubernetes-anti-patterns.webp 1366w,\n/static/b97081f7c9e73c2d5a0ac1d1a61f76b2/a7633/Kubernetes-anti-patterns.webp 1472w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5516304347826086}}},"coverCredits":"Photo by Saifeddine Rajhi"}}},"pageContext":{"prevThought":{"frontmatter":{"path":"/blog/ai-and-kubernetes/","title":"AI and Kubernetes: Open Source Tools powered by AI/OpenAI for Kubernetes","date":"2024-10-01 13:34:00"},"excerpt":"Kubernetes & AI team-up 📚 Introduction AI is generating a lot of buzz these days, and the Kubernetes-powered DevOps world is no exception…"},"nextThought":{"frontmatter":{"path":"/blog/data-on-kubernetes/","title":"Managing Data on Kubernetes","date":"2024-09-27 20:06:00"},"excerpt":"From databases to Big data: how k8s handles stateful workloads 📗\nThis is a blog series about Data on Kubernetes DoK 🔥 I have been inspired…"}}},"staticQueryHashes":["1271460761","1321585977"],"slicesMap":{}}